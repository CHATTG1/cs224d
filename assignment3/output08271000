I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.7.5 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
NVIDIA: no NVIDIA devices found
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_UNKNOWN
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:145] kernel driver does not appear to be running on this host (ip-172-30-1-62): /proc/driver/nvidia/version does not exist
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcupti.so.7.5 locally
Loading train trees..
Loading dev trees..
Loading test trees..
1852 1510
5058.0 total words with 1568 uniques
epoch 0
0 / 700 :    loss = 1.7083340883310 / 700 :    loss = 1.2365310192120 / 700 :    loss = 1.0206022262630 / 700 :    loss = 0.94411593675640 / 700 :    loss = 0.89931392669750 / 700 :    loss = 0.86989390850160 / 700 :    loss = 0.84928673505870 / 700 :    loss = 0.83211809396780 / 700 :    loss = 0.81870168447590 / 700 :    loss = 0.808579564095100 / 700 :    loss = 0.810078144073110 / 700 :    loss = 0.870913505554120 / 700 :    loss = 0.881151914597130 / 700 :    loss = 0.888177514076140 / 700 :    loss = 0.89533239603150 / 700 :    loss = 0.895379066467160 / 700 :    loss = 0.892138361931170 / 700 :    loss = 0.887326776981180 / 700 :    loss = 0.881672739983190 / 700 :    loss = 0.876725971699200 / 700 :    loss = 0.877317667007210 / 700 :    loss = 0.926516711712220 / 700 :    loss = 0.925807893276230 / 700 :    loss = 0.92182213068240 / 700 :    loss = 0.916087508202250 / 700 :    loss = 0.911182999611260 / 700 :    loss = 0.905213296413270 / 700 :    loss = 0.899200975895280 / 700 :    loss = 0.893045127392290 / 700 :    loss = 0.887030422688300 / 700 :    loss = 0.892330527306310 / 700 :    loss = 0.90959751606320 / 700 :    loss = 0.909501791330 / 700 :    loss = 0.905859172344340 / 700 :    loss = 0.905116736889350 / 700 :    loss = 0.904488503933360 / 700 :    loss = 0.901709377766370 / 700 :    loss = 0.898340940475380 / 700 :    loss = 0.894941270351390 / 700 :    loss = 0.890759706497400 / 700 :    loss = 0.914484858513410 / 700 :    loss = 0.952412307262420 / 700 :    loss = 0.959798157215430 / 700 :    loss = 0.963038742542440 / 700 :    loss = 0.962559103966450 / 700 :    loss = 0.960383832455460 / 700 :    loss = 0.957739055157470 / 700 :    loss = 0.9541670084480 / 700 :    loss = 0.949823498726490 / 700 :    loss = 0.945954918861500 / 700 :    loss = 0.948062956333510 / 700 :    loss = 0.968540251255520 / 700 :    loss = 0.969322681427530 / 700 :    loss = 0.967111349106540 / 700 :    loss = 0.964476048946550 / 700 :    loss = 0.961463093758560 / 700 :    loss = 0.958441793919570 / 700 :    loss = 0.954859018326580 / 700 :    loss = 0.950978398323590 / 700 :    loss = 0.947397112846600 / 700 :    loss = 0.95517706871610 / 700 :    loss = 0.96466755867620 / 700 :    loss = 0.965345203876630 / 700 :    loss = 0.965991914272640 / 700 :    loss = 0.964000701904650 / 700 :    loss = 0.962044775486660 / 700 :    loss = 0.959592223167670 / 700 :    loss = 0.956991493702680 / 700 :    loss = 0.953997433186690 / 700 :    loss = 0.951014995575()
Training acc (only root node): 0.518571428571
Valiation acc (only root node): 0.45
[[ 152.  198.]
 [ 139.  211.]]
[[ 20.  30.]
 [ 25.  25.]]
epoch 1
0 / 700 :    loss = 0.74872428178810 / 700 :    loss = 0.73690563440320 / 700 :    loss = 0.72657662630130 / 700 :    loss = 0.72388041019440 / 700 :    loss = 0.72603040933650 / 700 :    loss = 0.72595155239160 / 700 :    loss = 0.72352248430370 / 700 :    loss = 0.72182452678780 / 700 :    loss = 0.72163009643690 / 700 :    loss = 0.720603466034100 / 700 :    loss = 0.719255328178110 / 700 :    loss = 0.718351125717120 / 700 :    loss = 0.717349052429130 / 700 :    loss = 0.716880440712140 / 700 :    loss = 0.716273784637150 / 700 :    loss = 0.715469777584160 / 700 :    loss = 0.714381992817170 / 700 :    loss = 0.714189648628180 / 700 :    loss = 0.7134360075190 / 700 :    loss = 0.712795555592200 / 700 :    loss = 0.712211489677210 / 700 :    loss = 0.711701869965220 / 700 :    loss = 0.710900902748230 / 700 :    loss = 0.710391640663240 / 700 :    loss = 0.710207462311250 / 700 :    loss = 0.709795296192260 / 700 :    loss = 0.709388136864270 / 700 :    loss = 0.709336519241280 / 700 :    loss = 0.709114134312290 / 700 :    loss = 0.70883333683300 / 700 :    loss = 0.70867395401310 / 700 :    loss = 0.70834082365320 / 700 :    loss = 0.708194851875330 / 700 :    loss = 0.70692306757340 / 700 :    loss = 0.707026481628350 / 700 :    loss = 0.707040131092360 / 700 :    loss = 0.706547021866370 / 700 :    loss = 0.706542611122380 / 700 :    loss = 0.706361413002390 / 700 :    loss = 0.706283032894400 / 700 :    loss = 0.706025481224410 / 700 :    loss = 0.705939769745420 / 700 :    loss = 0.70594394207430 / 700 :    loss = 0.705905258656440 / 700 :    loss = 0.705430924892450 / 700 :    loss = 0.70543140173460 / 700 :    loss = 0.705279767513470 / 700 :    loss = 0.705008327961480 / 700 :    loss = 0.705016374588490 / 700 :    loss = 0.704957723618500 / 700 :    loss = 0.704821228981510 / 700 :    loss = 0.704782545567520 / 700 :    loss = 0.704747438431530 / 700 :    loss = 0.704469442368540 / 700 :    loss = 0.704328775406550 / 700 :    loss = 0.704223573208560 / 700 :    loss = 0.704248547554570 / 700 :    loss = 0.70424747467580 / 700 :    loss = 0.704297065735590 / 700 :    loss = 0.704148113728600 / 700 :    loss = 0.704066038132610 / 700 :    loss = 0.703770816326620 / 700 :    loss = 0.703789234161630 / 700 :    loss = 0.703748524189640 / 700 :    loss = 0.70385825634650 / 700 :    loss = 0.7039244771660 / 700 :    loss = 0.703857958317670 / 700 :    loss = 0.703796684742680 / 700 :    loss = 0.70362329483690 / 700 :    loss = 0.703583955765()
Training acc (only root node): 0.511428571429
Valiation acc (only root node): 0.53
[[ 268.   82.]
 [ 260.   90.]]
[[ 44.   6.]
 [ 41.   9.]]
epoch 2
0 / 700 :    loss = 0.71550607681310 / 700 :    loss = 0.70153152942720 / 700 :    loss = 0.69905614852930 / 700 :    loss = 0.69810879230540 / 700 :    loss = 0.69995874166550 / 700 :    loss = 0.70031279325560 / 700 :    loss = 0.69949322938970 / 700 :    loss = 0.69966280460480 / 700 :    loss = 0.70009684562790 / 700 :    loss = 0.70014834404100 / 700 :    loss = 0.700023233891110 / 700 :    loss = 0.700026571751120 / 700 :    loss = 0.70007699728130 / 700 :    loss = 0.700059711933140 / 700 :    loss = 0.700113296509150 / 700 :    loss = 0.700019359589160 / 700 :    loss = 0.699513196945170 / 700 :    loss = 0.699993491173180 / 700 :    loss = 0.699688673019190 / 700 :    loss = 0.699431717396200 / 700 :    loss = 0.699359118938210 / 700 :    loss = 0.699258506298220 / 700 :    loss = 0.699015319347230 / 700 :    loss = 0.698874592781240 / 700 :    loss = 0.699000179768250 / 700 :    loss = 0.698894023895260 / 700 :    loss = 0.698738694191270 / 700 :    loss = 0.698885440826280 / 700 :    loss = 0.698886394501290 / 700 :    loss = 0.698839306831300 / 700 :    loss = 0.698904573917310 / 700 :    loss = 0.698794662952320 / 700 :    loss = 0.698832631111330 / 700 :    loss = 0.697786808014340 / 700 :    loss = 0.698029577732350 / 700 :    loss = 0.698167443275360 / 700 :    loss = 0.697846353054370 / 700 :    loss = 0.697984457016380 / 700 :    loss = 0.697952091694390 / 700 :    loss = 0.698042273521400 / 700 :    loss = 0.697902202606410 / 700 :    loss = 0.697967648506420 / 700 :    loss = 0.698082685471430 / 700 :    loss = 0.698170721531440 / 700 :    loss = 0.697833061218450 / 700 :    loss = 0.697938144207460 / 700 :    loss = 0.69788736105470 / 700 :    loss = 0.697750985622480 / 700 :    loss = 0.697835326195490 / 700 :    loss = 0.697852432728500 / 700 :    loss = 0.697823703289510 / 700 :    loss = 0.697852909565520 / 700 :    loss = 0.697897732258530 / 700 :    loss = 0.697706282139540 / 700 :    loss = 0.697633326054550 / 700 :    loss = 0.697592616081560 / 700 :    loss = 0.697681605816570 / 700 :    loss = 0.697745263577580 / 700 :    loss = 0.697844207287590 / 700 :    loss = 0.697792649269600 / 700 :    loss = 0.69779509306610 / 700 :    loss = 0.697585999966620 / 700 :    loss = 0.697666347027630 / 700 :    loss = 0.697669446468640 / 700 :    loss = 0.697798728943650 / 700 :    loss = 0.697922945023660 / 700 :    loss = 0.697916686535670 / 700 :    loss = 0.697914004326680 / 700 :    loss = 0.697804927826690 / 700 :    loss = 0.697816371918()
Training acc (only root node): 0.521428571429
Valiation acc (only root node): 0.51
[[ 285.   65.]
 [ 270.   80.]]
[[ 45.   5.]
 [ 44.   6.]]
annealed lr to 0.006667
epoch 3
0 / 700 :    loss = 0.7128697037710 / 700 :    loss = 0.69939589500420 / 700 :    loss = 0.69685602188130 / 700 :    loss = 0.69587343931240 / 700 :    loss = 0.69721060991350 / 700 :    loss = 0.69746857881560 / 700 :    loss = 0.69682163000170 / 700 :    loss = 0.69695413112680 / 700 :    loss = 0.69741147756690 / 700 :    loss = 0.697303593159100 / 700 :    loss = 0.697280287743110 / 700 :    loss = 0.697307944298120 / 700 :    loss = 0.697317183018130 / 700 :    loss = 0.697287082672140 / 700 :    loss = 0.697275340557150 / 700 :    loss = 0.697267353535160 / 700 :    loss = 0.696893036366170 / 700 :    loss = 0.697322309017180 / 700 :    loss = 0.696957707405190 / 700 :    loss = 0.696725428104200 / 700 :    loss = 0.696667551994210 / 700 :    loss = 0.696586847305220 / 700 :    loss = 0.696477115154230 / 700 :    loss = 0.696382284164240 / 700 :    loss = 0.696523845196250 / 700 :    loss = 0.696407258511260 / 700 :    loss = 0.69628494978270 / 700 :    loss = 0.696398079395280 / 700 :    loss = 0.696459174156290 / 700 :    loss = 0.696399927139300 / 700 :    loss = 0.696458518505310 / 700 :    loss = 0.696380734444320 / 700 :    loss = 0.696403980255330 / 700 :    loss = 0.695517897606340 / 700 :    loss = 0.695694804192350 / 700 :    loss = 0.695845961571360 / 700 :    loss = 0.695649981499370 / 700 :    loss = 0.695760309696380 / 700 :    loss = 0.695715248585390 / 700 :    loss = 0.695837795734400 / 700 :    loss = 0.695682764053410 / 700 :    loss = 0.695752322674420 / 700 :    loss = 0.695842027664430 / 700 :    loss = 0.695923566818440 / 700 :    loss = 0.695656299591450 / 700 :    loss = 0.695760846138460 / 700 :    loss = 0.69570428133470 / 700 :    loss = 0.695620775223480 / 700 :    loss = 0.695693314075490 / 700 :    loss = 0.695692121983Traceback (most recent call last):
  File "rnn.py