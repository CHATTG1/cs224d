Loading train trees..
Loading dev trees..
Loading test trees..
1852 1510
5058.0 total words with 1568 uniques
epoch 0
0 / 700 :    loss = 4.4434604644810 / 700 :    loss = 2.7853951454220 / 700 :    loss = 1.9130154848130 / 700 :    loss = 1.5995334386840 / 700 :    loss = 1.4208532571850 / 700 :    loss = 1.3073859214860 / 700 :    loss = 1.2214233875370 / 700 :    loss = 1.1583170890880 / 700 :    loss = 1.1082837581690 / 700 :    loss = 1.07033336163100 / 700 :    loss = 1.06364309788110 / 700 :    loss = 1.18498194218120 / 700 :    loss = 1.17552936077130 / 700 :    loss = 1.15929853916140 / 700 :    loss = 1.14502716064150 / 700 :    loss = 1.12786030769160 / 700 :    loss = 1.10923278332170 / 700 :    loss = 1.0927169323180 / 700 :    loss = 1.07455337048190 / 700 :    loss = 1.05855989456200 / 700 :    loss = 1.04529237747210 / 700 :    loss = 1.07397472858220 / 700 :    loss = 1.0672968626230 / 700 :    loss = 1.05729627609240 / 700 :    loss = 1.04674386978250 / 700 :    loss = 1.03741872311260 / 700 :    loss = 1.02752280235270 / 700 :    loss = 1.01724267006280 / 700 :    loss = 1.00723445415290 / 700 :    loss = 0.998417675495300 / 700 :    loss = 0.991769969463310 / 700 :    loss = 0.990347504616320 / 700 :    loss = 0.984070777893330 / 700 :    loss = 0.975926816463340 / 700 :    loss = 0.970132887363350 / 700 :    loss = 0.965077638626360 / 700 :    loss = 0.958780109882370 / 700 :    loss = 0.95267021656380 / 700 :    loss = 0.946815788746390 / 700 :    loss = 0.940898835659400 / 700 :    loss = 0.947781383991410 / 700 :    loss = 0.983944356441420 / 700 :    loss = 0.994460344315430 / 700 :    loss = 1.00224709511440 / 700 :    loss = 1.00566792488450 / 700 :    loss = 1.00630927086460 / 700 :    loss = 1.00477659702470 / 700 :    loss = 1.00454580784480 / 700 :    loss = 1.0003169775490 / 700 :    loss = 0.996666550636500 / 700 :    loss = 0.993082404137510 / 700 :    loss = 0.992499768734520 / 700 :    loss = 0.988942205906530 / 700 :    loss = 0.984179139137540 / 700 :    loss = 0.979796588421550 / 700 :    loss = 0.975555121899560 / 700 :    loss = 0.97139531374570 / 700 :    loss = 0.967207252979580 / 700 :    loss = 0.962866485119590 / 700 :    loss = 0.958870589733600 / 700 :    loss = 0.963358581066610 / 700 :    loss = 0.973607420921620 / 700 :    loss = 0.97553563118630 / 700 :    loss = 0.976153254509640 / 700 :    loss = 0.975568711758650 / 700 :    loss = 0.973454475403660 / 700 :    loss = 0.970553934574670 / 700 :    loss = 0.967458546162680 / 700 :    loss = 0.964102268219690 / 700 :    loss = 0.961420118809()
Training acc (only root node): 0.482857142857
Valiation acc (only root node): 0.47
[[ 304.   46.]
 [ 316.   34.]]
[[ 41.   9.]
 [ 44.   6.]]
epoch 1
0 / 700 :    loss = 0.70606756210310 / 700 :    loss = 0.72478598356220 / 700 :    loss = 0.7213315367730 / 700 :    loss = 0.72219794988640 / 700 :    loss = 0.72187161445650 / 700 :    loss = 0.72022187709860 / 700 :    loss = 0.7192600965570 / 700 :    loss = 0.71773564815580 / 700 :    loss = 0.71786755323490 / 700 :    loss = 0.717422008514100 / 700 :    loss = 0.717098176479110 / 700 :    loss = 0.716592073441120 / 700 :    loss = 0.716219425201130 / 700 :    loss = 0.715473771095140 / 700 :    loss = 0.715502917767150 / 700 :    loss = 0.714470982552160 / 700 :    loss = 0.713481128216170 / 700 :    loss = 0.713449060917180 / 700 :    loss = 0.713302969933190 / 700 :    loss = 0.712424576283200 / 700 :    loss = 0.712128281593210 / 700 :    loss = 0.711883842945220 / 700 :    loss = 0.711364030838230 / 700 :    loss = 0.711104393005240 / 700 :    loss = 0.710740864277250 / 700 :    loss = 0.710696280003260 / 700 :    loss = 0.710536122322270 / 700 :    loss = 0.710218727589280 / 700 :    loss = 0.709880709648290 / 700 :    loss = 0.709528684616300 / 700 :    loss = 0.709359824657310 / 700 :    loss = 0.709087908268320 / 700 :    loss = 0.709070622921330 / 700 :    loss = 0.707820057869340 / 700 :    loss = 0.707926571369350 / 700 :    loss = 0.70803886652360 / 700 :    loss = 0.70758587122370 / 700 :    loss = 0.707481622696380 / 700 :    loss = 0.707502365112390 / 700 :    loss = 0.707304298878400 / 700 :    loss = 0.707344174385410 / 700 :    loss = 0.707319915295420 / 700 :    loss = 0.707295238972430 / 700 :    loss = 0.707374513149440 / 700 :    loss = 0.70683401823450 / 700 :    loss = 0.706869542599460 / 700 :    loss = 0.706823766232470 / 700 :    loss = 0.706870317459480 / 700 :    loss = 0.706981360912490 / 700 :    loss = 0.706875920296500 / 700 :    loss = 0.706665754318510 / 700 :    loss = 0.706839740276520 / 700 :    loss = 0.706775844097530 / 700 :    loss = 0.70660161972540 / 700 :    loss = 0.706457912922550 / 700 :    loss = 0.706313669682560 / 700 :    loss = 0.70625680685570 / 700 :    loss = 0.706365168095580 / 700 :    loss = 0.706372559071590 / 700 :    loss = 0.706206321716600 / 700 :    loss = 0.70613348484610 / 700 :    loss = 0.705955088139620 / 700 :    loss = 0.705932736397630 / 700 :    loss = 0.705865263939640 / 700 :    loss = 0.705799758434650 / 700 :    loss = 0.705810368061660 / 700 :    loss = 0.705722570419670 / 700 :    loss = 0.70564442873680 / 700 :    loss = 0.705506086349690 / 700 :    loss = 0.705522835255()
Training acc (only root node): 0.537142857143
Valiation acc (only root node): 0.5
[[ 275.   75.]
 [ 249.  101.]]
[[ 38.  12.]
 [ 38.  12.]]
epoch 2
0 / 700 :    loss = 0.69905084371610 / 700 :    loss = 0.69331747293520 / 700 :    loss = 0.69621676206630 / 700 :    loss = 0.69803833961540 / 700 :    loss = 0.70003390312250 / 700 :    loss = 0.7001533508360 / 700 :    loss = 0.70004755258670 / 700 :    loss = 0.6996968984680 / 700 :    loss = 0.70069187879690 / 700 :    loss = 0.700792968273100 / 700 :    loss = 0.70096552372110 / 700 :    loss = 0.701351344585120 / 700 :    loss = 0.701413869858130 / 700 :    loss = 0.701136410236140 / 700 :    loss = 0.701517641544150 / 700 :    loss = 0.70107460022160 / 700 :    loss = 0.70076996088170 / 700 :    loss = 0.700970470905180 / 700 :    loss = 0.700993597507190 / 700 :    loss = 0.700527429581200 / 700 :    loss = 0.700654268265210 / 700 :    loss = 0.700668036938220 / 700 :    loss = 0.700405180454230 / 700 :    loss = 0.700388252735240 / 700 :    loss = 0.700248003006250 / 700 :    loss = 0.700570166111260 / 700 :    loss = 0.700589179993270 / 700 :    loss = 0.700483679771280 / 700 :    loss = 0.700320005417290 / 700 :    loss = 0.700193464756300 / 700 :    loss = 0.700190901756310 / 700 :    loss = 0.700059115887320 / 700 :    loss = 0.700188457966330 / 700 :    loss = 0.699138820171340 / 700 :    loss = 0.699376225471350 / 700 :    loss = 0.699583768845360 / 700 :    loss = 0.699236214161370 / 700 :    loss = 0.69926828146380 / 700 :    loss = 0.699332237244390 / 700 :    loss = 0.699311137199400 / 700 :    loss = 0.699439883232410 / 700 :    loss = 0.699511885643420 / 700 :    loss = 0.699565887451430 / 700 :    loss = 0.69969189167440 / 700 :    loss = 0.699284911156450 / 700 :    loss = 0.699396848679460 / 700 :    loss = 0.699427247047470 / 700 :    loss = 0.699535608292480 / 700 :    loss = 0.699721515179490 / 700 :    loss = 0.699688255787500 / 700 :    loss = 0.699573159218510 / 700 :    loss = 0.699794828892520 / 700 :    loss = 0.69978672266530 / 700 :    loss = 0.699667036533540 / 700 :    loss = 0.699594557285550 / 700 :    loss = 0.699492931366560 / 700 :    loss = 0.699477910995570 / 700 :    loss = 0.699632287025580 / 700 :    loss = 0.699662923813590 / 700 :    loss = 0.699574530125600 / 700 :    loss = 0.699549138546610 / 700 :    loss = 0.699424862862620 / 700 :    loss = 0.699462711811630 / 700 :    loss = 0.699459075928640 / 700 :    loss = 0.699434459209650 / 700 :    loss = 0.699485182762660 / 700 :    loss = 0.699438393116670 / 700 :    loss = 0.699423670769680 / 700 :    loss = 0.699323177338690 / 700 :    loss = 0.699374854565()
Training acc (only root node): 0.544285714286
Valiation acc (only root node): 0.51
[[ 294.   56.]
 [ 263.   87.]]
[[ 39.  11.]
 [ 38.  12.]]
annealed lr to 0.006667
epoch 3
0 / 700 :    loss = 0.69814723730110 / 700 :    loss = 0.69094115495720 / 700 :    loss = 0.69348281621930 / 700 :    loss = 0.69482970237740 / 700 :    loss = 0.69624906778350 / 700 :    loss = 0.69644576311160 / 700 :    loss = 0.69641596078970 / 700 :    loss = 0.69612038135580 / 700 :    loss = 0.69707649946290 / 700 :    loss = 0.696995437145100 / 700 :    loss = 0.697267115116110 / 700 :    loss = 0.697645783424120 / 700 :    loss = 0.697710335255130 / 700 :    loss = 0.697459936142140 / 700 :    loss = 0.697752892971150 / 700 :    loss = 0.697441160679160 / 700 :    loss = 0.697232842445170 / 700 :    loss = 0.697452366352180 / 700 :    loss = 0.697399020195190 / 700 :    loss = 0.697003006935200 / 700 :    loss = 0.697158873081210 / 700 :    loss = 0.697177231312220 / 700 :    loss = 0.696998000145230 / 700 :    loss = 0.696987688541240 / 700 :    loss = 0.696918547153250 / 700 :    loss = 0.697190344334260 / 700 :    loss = 0.697215557098270 / 700 :    loss = 0.697125375271280 / 700 :    loss = 0.697046160698290 / 700 :    loss = 0.696902155876300 / 700 :    loss = 0.696904063225310 / 700 :    loss = 0.696806967258320 / 700 :    loss = 0.696912646294330 / 700 :    loss = 0.696013271809340 / 700 :    loss = 0.696220755577350 / 700 :    loss = 0.696435928345360 / 700 :    loss = 0.696164727211370 / 700 :    loss = 0.696218311787380 / 700 :    loss = 0.696250081062390 / 700 :    loss = 0.696301102638400 / 700 :    loss = 0.696409583092410 / 700 :    loss = 0.696488976479420 / 700 :    loss = 0.69651722908430 / 700 :    loss = 0.696626126766440 / 700 :    loss = 0.696300923824450 / 700 :    loss = 0.696435451508460 / 700 :    loss = 0.696450829506470 / 700 :    loss = 0.696585059166480 / 700 :    loss = 0.696759343147490 / 700 :    loss = 0.696705877781500 / 700 :    loss = 0.696637988091510 / 700 :    loss = 0.696823775768520 / 700 :    loss = 0.696815192699530 / 700 :    loss = 0.696710944176540 / 700 :    loss = 0.69663476944550 / 700 :    loss = 0.696534693241560 / 700 :    loss = 0.696545541286570 / 700 :    loss = 0.696691811085580 / 700 :    loss = 0.696704745293590 / 700 :    loss = 0.696635663509600 / 700 :    loss = 0.696602344513610 / 700 :    loss = 0.69652068615620 / 700 :    loss = 0.696536302567630 / 700 :    loss = 0.696563959122640 / 700 :    loss = 0.696520626545650 / 700 :    loss = 0.696573972702660 / 700 :    loss = 0.696523249149670 / 700 :    loss = 0.696527242661680 / 700 :    loss = 0.696439683437690 / 700 :    loss = 0.69647949934()
Training acc (only root node): 0.542857142857
Valiation acc (only root node): 0.49
[[ 280.   70.]
 [ 250.  100.]]
[[ 38.  12.]
 [ 39.  11.]]
annealed lr to 0.004444
epoch 4
0 / 700 :    loss = 0.69502109289210 / 700 :    loss = 0.68965095281620 / 700 :    loss = 0.6919825673130 / 700 :    loss = 0.69311380386440 / 700 :    loss = 0.69425475597450 / 700 :    loss = 0.69449675083260 / 700 :    loss = 0.69458884000870 / 700 :    loss = 0.69433605670980 / 700 :    loss = 0.69525337219290 / 700 :    loss = 0.695086956024100 / 700 :    loss = 0.695440471172110 / 700 :    loss = 0.695791363716120 / 700 :    loss = 0.695819914341130 / 700 :    loss = 0.695604324341140 / 700 :    loss = 0.695848941803150 / 700 :    loss = 0.695593237877160 / 700 :    loss = 0.695471584797170 / 700 :    loss = 0.695681869984180 / 700 :    loss = 0.695583939552190 / 700 :    loss = 0.695217907429200 / 700 :    loss = 0.695370137691210 / 700 :    loss = 0.695386707783220 / 700 :    loss = 0.695292472839230 / 700 :    loss = 0.695292949677240 / 700 :    loss = 0.695248544216250 / 700 :    loss = 0.695481836796260 / 700 :    loss = 0.695522487164270 / 700 :    loss = 0.695417702198280 / 700 :    loss = 0.695394814014290 / 700 :    loss = 0.695243835449300 / 700 :    loss = 0.695227324963310 / 700 :    loss = 0.69515401125320 / 700 :    loss = 0.695237398148330 / 700 :    loss = 0.694455265999340 / 700 :    loss = 0.694611132145350 / 700 :    loss = 0.694845318794360 / 700 :    loss = 0.694687902927370 / 700 :    loss = 0.694711863995380 / 700 :    loss = 0.694729089737390 / 700 :    loss = 0.694802820683400 / 700 :    loss = 0.694889962673410 / 700 :    loss = 0.6949685812420 / 700 :    loss = 0.694987177849430 / 700 :    loss = 0.695077419281440 / 700 :    loss = 0.69480907917450 / 700 :    loss = 0.694936811924460 / 700 :    loss = 0.694941282272470 / 700 :    loss = 0.695102095604480 / 700 :    loss = 0.695261716843490 / 700 :    loss = 0.695193648338500 / 700 :    loss = 0.695145785809510 / 700 :    loss = 0.695313692093520 / 700 :    loss = 0.695300281048530 / 700 :    loss = 0.695206880569540 / 700 :    loss = 0.695131063461550 / 700 :    loss = 0.695024490356560 / 700 :    loss = 0.695054590702570 / 700 :    loss = 0.695202171803580 / 700 :    loss = 0.695202231407590 / 700 :    loss = 0.695136189461600 / 700 :    loss = 0.695096075535610 / 700 :    loss = 0.69504660368620 / 700 :    loss = 0.695041835308630 / 700 :    loss = 0.695082068443640 / 700 :    loss = 0.695024013519650 / 700 :    loss = 0.69507753849660 / 700 :    loss = 0.695024847984670 / 700 :    loss = 0.695032060146680 / 700 :    loss = 0.694957733154690 / 700 :    loss = 0.694986760616()
Training acc (only root node): 0.55
Valiation acc (only root node): 0.48
[[ 288.   62.]
 [ 253.   97.]]
[[ 37.  13.]
 [ 39.  11.]]
annealed lr to 0.002963
epoch 5
0 / 700 :    loss = 0.69426816701910 / 700 :    loss = 0.68912136554720 / 700 :    loss = 0.69116139411930 / 700 :    loss = 0.69218581914940 / 700 :    loss = 0.69316655397450 / 700 :    loss = 0.69341742992460 / 700 :    loss = 0.69360214471870 / 700 :    loss = 0.69338244199880 / 700 :    loss = 0.69422203302490 / 700 :    loss = 0.694003283978100 / 700 :    loss = 0.694404959679110 / 700 :    loss = 0.694738149643120 / 700 :    loss = 0.694724142551130 / 700 :    loss = 0.694550216198140 / 700 :    loss = 0.694766879082150 / 700 :    loss = 0.694524526596160 / 700 :    loss = 0.694464921951170 / 700 :    loss = 0.694655418396180 / 700 :    loss = 0.694535732269190 / 700 :    loss = 0.694194078445200 / 700 :    loss = 0.694327533245210 / 700 :    loss = 0.694343328476220 / 700 :    loss = 0.694321990013230 / 700 :    loss = 0.694344222546240 / 700 :    loss = 0.694302916527250 / 700 :    loss = 0.694512188435260 / 700 :    loss = 0.694555103779270 / 700 :    loss = 0.694434642792280 / 700 :    loss = 0.694450378418290 / 700 :    loss = 0.694304406643300 / 700 :    loss = 0.694267094135310 / 700 :    loss = 0.694212794304320 / 700 :    loss = 0.694274902344330 / 700 :    loss = 0.693596303463340 / 700 :    loss = 0.693696260452350 / 700 :    loss = 0.693937301636360 / 700 :    loss = 0.693897366524370 / 700 :    loss = 0.693875432014380 / 700 :    loss = 0.693898200989390 / 700 :    loss = 0.69396007061400 / 700 :    loss = 0.694030940533410 / 700 :    loss = 0.694105625153420 / 700 :    loss = 0.694122254848430 / 700 :    loss = 0.694196045399440 / 700 :    loss = 0.693974018097450 / 700 :    loss = 0.694081366062460 / 700 :    loss = 0.694078564644470 / 700 :    loss = 0.69425958395480 / 700 :    loss = 0.694410681725490 / 700 :    loss = 0.694332182407500 / 700 :    loss = 0.694290280342510 / 700 :    loss = 0.694449543953520 / 700 :    loss = 0.694430828094530 / 700 :    loss = 0.694349050522540 / 700 :    loss = 0.694280445576550 / 700 :    loss = 0.694164395332560 / 700 :    loss = 0.694198429585570 / 700 :    loss = 0.694353282452580 / 700 :    loss = 0.694345831871590 / 700 :    loss = 0.694272100925600 / 700 :    loss = 0.694227516651610 / 700 :    loss = 0.694205164909620 / 700 :    loss = 0.69418823719630 / 700 :    loss = 0.694227993488640 / 700 :    loss = 0.694162547588650 / 700 :    loss = 0.69421505928660 / 700 :    loss = 0.694163560867670 / 700 :    loss = 0.694167613983680 / 700 :    loss = 0.694102227688690 / 700 :    loss = 0.694124519825()
Training acc (only root node): 0.548571428571
Valiation acc (only root node): 0.47
[[ 290.   60.]
 [ 256.   94.]]
[[ 37.  13.]
 [ 40.  10.]]
annealed lr to 0.001975
epoch 6
0 / 700 :    loss = 0.6938233971610 / 700 :    loss = 0.68884479999520 / 700 :    loss = 0.69068002700830 / 700 :    loss = 0.69165194034640 / 700 :    loss = 0.69253444671650 / 700 :    loss = 0.69280558824560 / 700 :    loss = 0.693083703518