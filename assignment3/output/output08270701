I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.7.5 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
NVIDIA: no NVIDIA devices found
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_UNKNOWN
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:145] kernel driver does not appear to be running on this host (ip-172-30-1-62): /proc/driver/nvidia/version does not exist
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcupti.so.7.5 locally
Loading train trees..
Loading dev trees..
Loading test trees..
1852 1510
5058.0 total words with 1568 uniques
epoch 0
0 / 700 :    loss = 3.2377598285710 / 700 :    loss = 2.3881106376620 / 700 :    loss = 1.6956992149430 / 700 :    loss = 1.4193031787940 / 700 :    loss = 1.2755198478750 / 700 :    loss = 1.1787589788460 / 700 :    loss = 1.1116588115770 / 700 :    loss = 1.060850977980 / 700 :    loss = 1.0227118730590 / 700 :    loss = 0.992827951908100 / 700 :    loss = 1.05086481571110 / 700 :    loss = 1.34402549267120 / 700 :    loss = 1.33551955223130 / 700 :    loss = 1.31596016884140 / 700 :    loss = 1.29571557045150 / 700 :    loss = 1.2731025219160 / 700 :    loss = 1.24695563316170 / 700 :    loss = 1.22667002678180 / 700 :    loss = 1.20302581787190 / 700 :    loss = 1.18092048168200 / 700 :    loss = 1.18487930298210 / 700 :    loss = 1.3030936718220 / 700 :    loss = 1.31282150745230 / 700 :    loss = 1.31311118603240 / 700 :    loss = 1.30574226379250 / 700 :    loss = 1.29910111427260 / 700 :    loss = 1.28707695007270 / 700 :    loss = 1.27137696743280 / 700 :    loss = 1.25769519806290 / 700 :    loss = 1.24235880375300 / 700 :    loss = 1.24834954739310 / 700 :    loss = 1.29322171211320 / 700 :    loss = 1.28220903873330 / 700 :    loss = 1.26808130741340 / 700 :    loss = 1.25603175163350 / 700 :    loss = 1.24584329128360 / 700 :    loss = 1.23349535465370 / 700 :    loss = 1.22146165371380 / 700 :    loss = 1.20912253857390 / 700 :    loss = 1.19742083549400 / 700 :    loss = 1.21099901199410 / 700 :    loss = 1.2487860918420 / 700 :    loss = 1.24921214581430 / 700 :    loss = 1.24823403358440 / 700 :    loss = 1.24363267422450 / 700 :    loss = 1.23758113384460 / 700 :    loss = 1.229144454470 / 700 :    loss = 1.2229269743480 / 700 :    loss = 1.21430516243490 / 700 :    loss = 1.20577228069500 / 700 :    loss = 1.20424056053510 / 700 :    loss = 1.22413706779520 / 700 :    loss = 1.22154045105530 / 700 :    loss = 1.2166762352540 / 700 :    loss = 1.21052491665550 / 700 :    loss = 1.20414531231560 / 700 :    loss = 1.19692289829570 / 700 :    loss = 1.18949389458580 / 700 :    loss = 1.1818614006590 / 700 :    loss = 1.17438590527600 / 700 :    loss = 1.19285750389610 / 700 :    loss = 1.21843886375620 / 700 :    loss = 1.21963655949630 / 700 :    loss = 1.21916246414640 / 700 :    loss = 1.21430695057650 / 700 :    loss = 1.20932090282660 / 700 :    loss = 1.20399320126670 / 700 :    loss = 1.19832873344680 / 700 :    loss = 1.19225358963690 / 700 :    loss = 1.1863476038()
Training acc (only root node): 0.477142857143
Valiation acc (only root node): 0.57
[[ 101.  249.]
 [ 117.  233.]]
[[ 25.  25.]
 [ 18.  32.]]
epoch 1
0 / 700 :    loss = 0.74047476053210 / 700 :    loss = 0.7554851770420 / 700 :    loss = 0.74023056030330 / 700 :    loss = 0.73861193656940 / 700 :    loss = 0.7387527227450 / 700 :    loss = 0.7355445623460 / 700 :    loss = 0.73257404565870 / 700 :    loss = 0.72970694303580 / 700 :    loss = 0.7288830280390 / 700 :    loss = 0.728106498718100 / 700 :    loss = 0.726958990097110 / 700 :    loss = 0.725930035114120 / 700 :    loss = 0.724329710007130 / 700 :    loss = 0.723632216454140 / 700 :    loss = 0.723391473293150 / 700 :    loss = 0.722360312939160 / 700 :    loss = 0.720956504345170 / 700 :    loss = 0.720268845558180 / 700 :    loss = 0.719339132309190 / 700 :    loss = 0.718675732613200 / 700 :    loss = 0.717739105225210 / 700 :    loss = 0.717024326324220 / 700 :    loss = 0.716352343559230 / 700 :    loss = 0.715313911438240 / 700 :    loss = 0.714964807034250 / 700 :    loss = 0.714508891106260 / 700 :    loss = 0.714107453823270 / 700 :    loss = 0.713724732399280 / 700 :    loss = 0.713060319424290 / 700 :    loss = 0.712657928467300 / 700 :    loss = 0.712158024311310 / 700 :    loss = 0.711834609509320 / 700 :    loss = 0.711612999439330 / 700 :    loss = 0.710250616074340 / 700 :    loss = 0.710064470768350 / 700 :    loss = 0.709853589535360 / 700 :    loss = 0.709254384041370 / 700 :    loss = 0.709090054035380 / 700 :    loss = 0.708770275116390 / 700 :    loss = 0.708515584469400 / 700 :    loss = 0.708473205566410 / 700 :    loss = 0.708337366581420 / 700 :    loss = 0.708050608635430 / 700 :    loss = 0.707868158817440 / 700 :    loss = 0.707457304001450 / 700 :    loss = 0.707518815994460 / 700 :    loss = 0.707429111004470 / 700 :    loss = 0.707345604897480 / 700 :    loss = 0.707205832005490 / 700 :    loss = 0.707169234753500 / 700 :    loss = 0.707017362118510 / 700 :    loss = 0.706873476505520 / 700 :    loss = 0.706812143326530 / 700 :    loss = 0.706508815289540 / 700 :    loss = 0.706275463104550 / 700 :    loss = 0.70619982481560 / 700 :    loss = 0.706081569195570 / 700 :    loss = 0.705871224403580 / 700 :    loss = 0.705702006817590 / 700 :    loss = 0.70556807518600 / 700 :    loss = 0.705554306507610 / 700 :    loss = 0.705362021923620 / 700 :    loss = 0.705303072929630 / 700 :    loss = 0.705212235451640 / 700 :    loss = 0.705176651478650 / 700 :    loss = 0.705133616924660 / 700 :    loss = 0.705076158047670 / 700 :    loss = 0.704978346825680 / 700 :    loss = 0.704820692539690 / 700 :    loss = 0.704756438732()
Training acc (only root node): 0.517142857143
Valiation acc (only root node): 0.54
[[ 288.   62.]
 [ 276.   74.]]
[[ 44.   6.]
 [ 40.  10.]]
epoch 2
0 / 700 :    loss = 0.69878673553510 / 700 :    loss = 0.698940277120 / 700 :    loss = 0.69661015272130 / 700 :    loss = 0.69791394472140 / 700 :    loss = 0.6985929012350 / 700 :    loss = 0.69800245761960 / 700 :    loss = 0.69667094945970 / 700 :    loss = 0.69651103019780 / 700 :    loss = 0.69729328155590 / 700 :    loss = 0.697532474995100 / 700 :    loss = 0.698327481747110 / 700 :    loss = 0.698856949806120 / 700 :    loss = 0.698807239532130 / 700 :    loss = 0.698971092701140 / 700 :    loss = 0.699594080448150 / 700 :    loss = 0.699496328831160 / 700 :    loss = 0.699047029018170 / 700 :    loss = 0.699369132519180 / 700 :    loss = 0.699250638485190 / 700 :    loss = 0.699224829674200 / 700 :    loss = 0.699208855629210 / 700 :    loss = 0.699148356915220 / 700 :    loss = 0.699164509773230 / 700 :    loss = 0.698835790157240 / 700 :    loss = 0.69889652729250 / 700 :    loss = 0.698906123638260 / 700 :    loss = 0.698991894722270 / 700 :    loss = 0.698999524117280 / 700 :    loss = 0.69868683815290 / 700 :    loss = 0.698647677898300 / 700 :    loss = 0.698544025421310 / 700 :    loss = 0.698484122753320 / 700 :    loss = 0.698584914207330 / 700 :    loss = 0.697588741779340 / 700 :    loss = 0.697747886181350 / 700 :    loss = 0.69779509306360 / 700 :    loss = 0.697465658188370 / 700 :    loss = 0.697541177273380 / 700 :    loss = 0.69743591547390 / 700 :    loss = 0.697458565235400 / 700 :    loss = 0.697594642639410 / 700 :    loss = 0.697644531727420 / 700 :    loss = 0.69754999876430 / 700 :    loss = 0.69755423069440 / 700 :    loss = 0.697317481041450 / 700 :    loss = 0.697510361671460 / 700 :    loss = 0.69753909111470 / 700 :    loss = 0.697606801987480 / 700 :    loss = 0.697636306286490 / 700 :    loss = 0.697726249695500 / 700 :    loss = 0.697725355625510 / 700 :    loss = 0.69772785902520 / 700 :    loss = 0.697776556015530 / 700 :    loss = 0.697598040104540 / 700 :    loss = 0.697486817837550 / 700 :    loss = 0.697495281696560 / 700 :    loss = 0.697502017021570 / 700 :    loss = 0.697405517101580 / 700 :    loss = 0.697338759899590 / 700 :    loss = 0.697300553322600 / 700 :    loss = 0.697369217873610 / 700 :    loss = 0.697271227837620 / 700 :    loss = 0.697299659252630 / 700 :    loss = 0.697307825089640 / 700 :    loss = 0.69734287262650 / 700 :    loss = 0.697379648685660 / 700 :    loss = 0.697395563126670 / 700 :    loss = 0.697387218475680 / 700 :    loss = 0.697329103947690 / 700 :    loss = 0.697325706482()
Training acc (only root node): 0.518571428571
Valiation acc (only root node): 0.58
[[ 292.   58.]
 [ 279.   71.]]
[[ 46.   4.]
 [ 38.  12.]]
epoch 3
0 / 700 :    loss = 0.7003613710410 / 700 :    loss = 0.69653093814820 / 700 :    loss = 0.69508391618730 / 700 :    loss = 0.69620651006740 / 700 :    loss = 0.69677150249550 / 700 :    loss = 0.69627845287360 / 700 :    loss = 0.6950423717570 / 700 :    loss = 0.69490897655580 / 700 :    loss = 0.69557076692690 / 700 :    loss = 0.695762634277100 / 700 :    loss = 0.696458518505110 / 700 :    loss = 0.697018742561120 / 700 :    loss = 0.696844041348130 / 700 :    loss = 0.696949481964140 / 700 :    loss = 0.697515904903150 / 700 :    loss = 0.697425186634160 / 700 :    loss = 0.696989834309170 / 700 :    loss = 0.697353124619180 / 700 :    loss = 0.697199523449190 / 700 :    loss = 0.697183310986200 / 700 :    loss = 0.697194874287210 / 700 :    loss = 0.697151243687220 / 700 :    loss = 0.697176754475230 / 700 :    loss = 0.696906268597240 / 700 :    loss = 0.696958839893250 / 700 :    loss = 0.69696944952260 / 700 :    loss = 0.697084069252270 / 700 :    loss = 0.697091042995280 / 700 :    loss = 0.696773350239290 / 700 :    loss = 0.696759223938300 / 700 :    loss = 0.696669459343310 / 700 :    loss = 0.696617901325320 / 700 :    loss = 0.696732640266330 / 700 :    loss = 0.695771038532340 / 700 :    loss = 0.695941090584350 / 700 :    loss = 0.695990681648360 / 700 :    loss = 0.695672512054370 / 700 :    loss = 0.695763587952380 / 700 :    loss = 0.695668756962390 / 700 :    loss = 0.695714831352400 / 700 :    loss = 0.695823669434410 / 700 :    loss = 0.69588893652420 / 700 :    loss = 0.695807218552430 / 700 :    loss = 0.695816218853440 / 700 :    loss = 0.695588588715450 / 700 :    loss = 0.695776343346460 / 700 :    loss = 0.695799171925470 / 700 :    loss = 0.695876717567480 / 700 :    loss = 0.695922613144490 / 700 :    loss = 0.696005165577500 / 700 :    loss = 0.696016192436510 / 700 :    loss = 0.696034789085520 / 700 :    loss = 0.69608014822530 / 700 :    loss = 0.695900678635540 / 700 :    loss = 0.695801258087550 / 700 :    loss = 0.695802807808560 / 700 :    loss = 0.69581001997570 / 700 :    loss = 0.695721447468580 / 700 :    loss = 0.69564884901590 / 700 :    loss = 0.695601582527600 / 700 :    loss = 0.695674717426610 / 700 :    loss = 0.695576131344620 / 700 :    loss = 0.695612430573630 / 700 :    loss = 0.695629477501640 / 700 :    loss = 0.695654988289650 / 700 :    loss = 0.695693314075660 / 700 :    loss = 0.695711731911670 / 700 :    loss = 0.695709884167680 / 700 :    loss = 0.695659577847690 / 700 :    loss = 0.695656120777()
Training acc (only root node): 0.53
Valiation acc (only root node): 0.55
[[ 296.   54.]
 [ 275.   75.]]
[[ 45.   5.]
 [ 40.  10.]]
annealed lr to 0.006667
epoch 4
0 / 700 :    loss = 0.69958955049510 / 700 :    loss = 0.69434124231320 / 700 :    loss = 0.69306629896230 / 700 :    loss = 0.69412934780140 / 700 :    loss = 0.69457161426550 / 700 :    loss = 0.69412028789560 / 700 :    loss = 0.69304281473270 / 700 :    loss = 0.69293451309280 / 700 :    loss = 0.69357222318690 / 700 :    loss = 0.693700969219100 / 700 :    loss = 0.694373726845110 / 700 :    loss = 0.694913506508120 / 700 :    loss = 0.694702208042130 / 700 :    loss = 0.694806754589140 / 700 :    loss = 0.695278406143150 / 700 :    loss = 0.695201158524160 / 700 :    loss = 0.694878220558170 / 700 :    loss = 0.695246219635180 / 700 :    loss = 0.695039510727190 / 700 :    loss = 0.69504070282200 / 700 :    loss = 0.695042550564210 / 700 :    loss = 0.695010960102220 / 700 :    loss = 0.695129275322230 / 700 :    loss = 0.694915115833240 / 700 :    loss = 0.694959104061250 / 700 :    loss = 0.69496101141260 / 700 :    loss = 0.695127785206270 / 700 :    loss = 0.695100605488280 / 700 :    loss = 0.694844603539290 / 700 :    loss = 0.694818854332300 / 700 :    loss = 0.694726586342310 / 700 :    loss = 0.694688439369320 / 700 :    loss = 0.694790184498330 / 700 :    loss = 0.693960607052340 / 700 :    loss = 0.694076359272350 / 700 :    loss = 0.69415384531360 / 700 :    loss = 0.693975567818370 / 700 :    loss = 0.694031000137380 / 700 :    loss = 0.693930625916390 / 700 :    loss = 0.694003999233400 / 700 :    loss = 0.694074451923410 / 700 :    loss = 0.694143712521420 / 700 :    loss = 0.694051206112430 / 700 :    loss = 0.694052875042440 / 700 :    loss = 0.693878769875450 / 700 :    loss = 0.694061458111460 / 700 :    loss = 0.694068908691470 / 700 :    loss = 0.694183170795480 / 700 :    loss = 0.694215118885490 / 700 :    loss = 0.694277405739500 / 700 :    loss = 0.694311439991510 / 700 :    loss = 0.694330334663520 / 700 :    loss = 0.694365561008530 / 700 :    loss = 0.694200634956540 / 700 :    loss = 0.694106221199550 / 700 :    loss = 0.694098174572560 / 700 :    loss = 0.69412535429570 / 700 :    loss = 0.694046497345580 / 700 :    loss = 0.69396173954590 / 700 :    loss = 0.693911135197600 / 700 :    loss = 0.693972051144610 / 700 :    loss = 0.693912804127620 / 700 :    loss = 0.693930029869630 / 700 :    loss = 0.693960547447640 / 700 :    loss = 0.693968176842650 / 700 :    loss = 0.694013237953660 / 700 :    loss = 0.69403231144670 / 700 :    loss = 0.694033920765680 / 700 :    loss = 0.693996727467690 / 700 :    loss = 0.693989098072()
Training acc (only root node): 0.535714285714
Valiation acc (only root node): 0.55
[[ 301.   49.]
 [ 276.   74.]]
[[ 45.   5.]
 [ 40.  10.]]
annealed lr to 0.004444
epoch 5
0 / 700 :    loss = 0.69865447282810 / 700 :    loss = 0.69311887025820 / 700 :    loss = 0.69184106588430 / 700 :    loss = 0.69288676977240 / 700 :    loss = 0.69327795505550 / 700 :    loss = 0.69285196065960 / 700 :    loss = 0.69190549850570 / 700 :    loss = 0.69182068109580 / 700 :    loss = 0.6924113035290 / 700 :    loss = 0.692496597767100 / 700 :    loss = 0.693178892136110 / 700 :    loss = 0.693699896336120 / 700 :    loss = 0.693462133408130 / 700 :    loss = 0.693578958511140 / 700 :    loss = 0.693992674351150 / 700 :    loss = 0.69390553236160 / 700 :    loss = 0.69365811348170 / 700 :    loss = 0.694006145180 / 700 :    loss = 0.693770945072190 / 700 :    loss = 0.693796038628200 / 700 :    loss = 0.693780779839210 / 700 :    loss = 0.693757593632220 / 700 :    loss = 0.693958044052230 / 700 :    loss = 0.693790555240 / 700 :    loss = 0.693818926811250 / 700 :    loss = 0.693816244602260 / 700 :    loss = 0.694011569023270 / 700 :    loss = 0.693954706192280 / 700 :    loss = 0.693754136562290 / 700 :    loss = 0.693723022938300 / 700 :    loss = 0.693612217903310 / 700 :    loss = 0.693582415581320 / 700 :    loss = 0.693669617176330 / 700 :    loss = 0.692947030067340 / 700 :    loss = 0.693008065224350 / 700 :    loss = 0.693101644516360 / 700 :    loss = 0.693061232567370 / 700 :    loss = 0.693058490753380 / 700 :    loss = 0.692960619926390 / 700 :    loss = 0.693031430244400 / 700 :    loss = 0.693080663681410 / 700 :    loss = 0.693147480488420 / 700 :    loss = 0.693050146103430 / 700 :    loss = 0.693042159081440 / 700 :    loss = 0.692909359932450 / 700 :    loss = 0.693073272705460 / 700 :    loss = 0.693070590496470 / 700 :    loss = 0.693209588528480 / 700 :    loss = 0.693230211735490 / 700 :    loss = 0.693279087543500 / 700 :    loss = 0.693321108818510 / 700 :    loss = 0.693343818188520 / 700 :    loss = 0.693370878696530 / 700 :    loss = 0.693222105503540 / 700 :    loss = 0.693139851093550 / 700 :    loss = 0.69312030077560 / 700 :    loss = 0.693155050278570 / 700 :    loss = 0.693088948727580 / 700 :    loss = 0.692997336388590 / 700 :    loss = 0.692934513092600 / 700 :    loss = 0.692988634109610 / 700 :    loss = 0.692961871624620 / 700 :    loss = 0.692967712879630 / 700 :    loss = 0.692997992039640 / 700 :    loss = 0.69299519062650 / 700 :    loss = 0.693044185638660 / 700 :    loss = 0.693068742752670 / 700 :    loss = 0.693066895008680 / 700 :    loss = 0.693036139011690 / 700 :    loss = 0.6930295825()
Training acc (only root node): 0.544285714286
Valiation acc (only root node): 0.55
[[ 302.   48.]
 [ 271.   79.]]
[[ 45.   5.]
 [ 40.  10.]]
annealed lr to 0.002963
epoch 6
0 / 700 :    loss = 0.69778227806110 / 700 :    loss = 0.69240051507920 / 700 :    loss = 0.69104111194630 / 700 :    loss = 0.69210994243640 / 700 :    loss = 0.69248843193150 / 700 :    loss = 0.69210541248360 / 700 :    loss = 0.69128531217670 / 700 :    loss = 0.69122302532280 / 700 :    loss = 0.69170528650390 / 700 :    loss = 0.691779315472100 / 700 :    loss = 0.692443907261110 / 700 :    loss = 0.692954182625120 / 700 :    loss = 0.692698299885130 / 700 :    loss = 0.69284081459140 / 700 :    loss = 0.693223655224150 / 700 :    loss = 0.693112313747160 / 700 :    loss = 0.692907214165170 / 700 :    loss = 0.693241298199180 / 700 :    loss = 0.692993760109190 / 700 :    loss = 0.693044066429200 / 700 :    loss = 0.693004727364210 / 700 :    loss = 0.692982971668220 / 700 :    loss = 0.693239033222230 / 700 :    loss = 0.693119108677240 / 700 :    loss = 0.693140923977250 / 700 :    loss = 0.693140864372260 / 700 :    loss = 0.693331062794270 / 700 :    loss = 0.693260729313280 / 700 :    loss = 0.693095684052290 / 700 :    loss = 0.693071246147300 / 700 :    loss = 0.692941248417310 / 700 :    loss = 0.692922770977320 / 700 :    loss = 0.692994713783330 / 700 :    loss = 0.692372500896340 / 700 :    loss = 0.692380011082350 / 700 :    loss = 0.692467331886360 / 700 :    loss = 0.69252705574370 / 700 :    loss = 0.69247508049380 / 700 :    loss = 0.692395448685390 / 700 :    loss = 0.692442774773400 / 700 :    loss = 0.692476391792410 / 700 :    loss = 0.692539095879420 / 700 :    loss = 0.692436218262430 / 700 :    loss = 0.692421019077440 / 700 :    loss = 0.692320108414450 / 700 :    loss = 0.69246506691460 / 700 :    loss = 0.692458689213470 / 700 :    loss = 0.692608773708480 / 700 :    loss = 0.692622482777490 / 700 :    loss = 0.692663967609500 / 700 :    loss = 0.692707121372510 / 700 :    loss = 0.692731320858520 / 700 :    loss = 0.692753076553530 / 700 :    loss = 0.692618012428540 / 700 :    loss = 0.692552268505550 / 700 :    loss = 0.69252371788560 / 700 :    loss = 0.692552387714570 / 700 :    loss = 0.692496657372580 / 700 :    loss = 0.692402482033590 / 700 :    loss = 0.692327260971600 / 700 :    loss = 0.692378342152610 / 700 :    loss = 0.69238114357620 / 700 :    loss = 0.692382395267630 / 700 :    loss = 0.692404270172640 / 700 :    loss = 0.692400336266650 / 700 :    loss = 0.69244748354660 / 700 :    loss = 0.692475318909670 / 700 :    loss = 0.692470610142680 / 700 :    loss = 0.692440807819690 / 700 :    loss = 0.692437529564()
Training acc (only root node): 0.548571428571
Valiation acc (only root node): 0.55
[[ 297.   53.]
 [ 263.   87.]]
[[ 44.   6.]
 [ 39.  11.]]
annealed lr to 0.001975
epoch 7
0 / 700 :    loss = 0.69642782211310 / 700 :    loss = 0.69184547662720 / 700 :    loss = 0.69043862819730 / 700 :    loss = 0.69156283140240 / 700 :    loss = 0.69197469949750 / 700 :    loss = 0.69165509939260 / 700 :    loss = 0.69097566604670 / 700 :    loss = 0.69094228744580 / 700 :    loss = 0.69129079580390 / 700 :    loss = 0.691388785839100 / 700 :    loss = 0.69199937582110 / 700 :    loss = 0.692504823208120 / 700 :    loss = 0.692239224911130 / 700 :    loss = 0.69239449501140 / 700 :    loss = 0.692762613297150 / 700 :    loss = 0.692627251148160 / 700 :    loss = 0.692441642284170 / 700 :    loss = 0.692766308784180 / 700 :    loss = 0.692512392998190 / 700 :    loss = 0.692585170269200 / 700 :    loss = 0.692522466183210 / 700 :    loss = 0.692499697208220 / 700 :    loss = 0.692781627178230 / 700 :    loss = 0.692700386047240 / 700 :    loss = 0.692722678185250 / 700 :    loss = 0.692733466625260 / 700 :    loss = 0.69289880991270 / 700 :    loss = 0.692829787731280 / 700 :    loss = 0.69267386198290 / 700 :    loss = 0.692655920982300 / 700 :    loss = 0.69251459837310 / 700 :    loss = 0.69250702858320 / 700 :    loss = 0.692566990852330 / 700 :    loss = 0.692034006119340 / 700 :    loss = 0.692002534866350 / 700 :    loss = 0.692069292068360 / 700 :    loss = 0.69217646122370 / 700 :    loss = 0.692094922066380 / 700 :    loss = 0.692034542561390 / 700 :    loss = 0.692056894302400 / 700 :    loss = 0.692081689835410 / 700 :    loss = 0.692143619061420 / 700 :    loss = 0.692033827305430 / 700 :    loss = 0.692014813423440 / 700 :    loss = 0.691935479641450 / 700 :    loss = 0.692065596581460 / 700 :    loss = 0.692058265209470 / 700 :    loss = 0.692209541798480 / 700 :    loss = 0.692220211029490 / 700 :    loss = 0.692258000374500 / 700 :    loss = 0.692300081253510 / 700 :    loss = 0.692324280739520 / 700 :    loss = 0.692344307899530 / 700 :    loss = 0.692218065262540 / 700 :    loss = 0.692168831825550 / 700 :    loss = 0.692136108875560 / 700 :    loss = 0.692153155804570 / 700 :    loss = 0.69210267067580 / 700 :    loss = 0.692007958889590 / 700 :    loss = 0.691923975945600 / 700 :    loss = 0.691973626614610 / 700 :    loss = 0.692001342773620 / 700 :    loss = 0.692003667355630 / 700 :    loss = 0.692017376423640 / 700 :    loss = 0.692017197609650 / 700 :    loss = 0.692057132721660 / 700 :    loss = 0.692082643509670 / 700 :    loss = 0.692076742649680 / 700 :    loss = 0.69205147028690 / 700 :    loss = 0.692051410675()
Training acc (only root node): 0.551428571429
Valiation acc (only root node): 0.56
[[ 291.   59.]
 [ 255.   95.]]
[[ 44.   6.]
 [ 38.  12.]]
annealed lr to 0.001317
epoch 8
0 / 700 :    loss = 0.69490331411410 / 700 :    loss = 0.6913073062920 / 700 :    loss = 0.68995660543430 / 700 :    loss = 0.69113755226140 / 700 :    loss = 0.69158720970250 / 700 :    loss = 0.69133609533360 / 700 :    loss = 0.69078677892770 / 700 :    loss = 0.69080322980980 / 700 :    loss = 0.69103437662190 / 700 :    loss = 0.691177725792100 / 700 :    loss = 0.691719353199110 / 700 :    loss = 0.692222714424120 / 700 :    loss = 0.691952586174130 / 700 :    loss = 0.692094922066140 / 700 :    loss = 0.692453801632150 / 700 :    loss = 0.69230723381160 / 700 :    loss = 0.692140758038170 / 700 :    loss = 0.692457199097180 / 700 :    loss = 0.692203104496190 / 700 :    loss = 0.692299067974200 / 700 :    loss = 0.692217767239210 / 700 :    loss = 0.692192494869220 / 700 :    loss = 0.692475795746230 / 700 :    loss = 0.692419290543240 / 700 :    loss = 0.692446649075250 / 700 :    loss = 0.692470908165260 / 700 :    loss = 0.692611336708270 / 700 :    loss = 0.692550778389280 / 700 :    loss = 0.692387878895290 / 700 :    loss = 0.692373216152300 / 700 :    loss = 0.692226588726310 / 700 :    loss = 0.692228376865320 / 700 :    loss = 0.692280590534330 / 700 :    loss = 0.691820323467340 / 700 :    loss = 0.691765189171350 / 700 :    loss = 0.691810548306360 / 700 :    loss = 0.691926002502370 / 700 :    loss = 0.691831171513380 / 700 :    loss = 0.691784799099390 / 700 :    loss = 0.691790282726400 / 700 :    loss = 0.691809594631410 / 700 :    loss = 0.691871881485420 / 700 :    loss = 0.691756248474430 / 700 :    loss = 0.691735386848440 / 700 :    loss = 0.691669762135450 / 700 :    loss = 0.691790163517460 / 700 :    loss = 0.691783845425470 / 700 :    loss = 0.691932559013480 / 700 :    loss = 0.691941618919490 / 700 :    loss = 0.691977798939500 / 700 :    loss = 0.692019045353510 / 700 :    loss = 0.692042887211520 / 700 :    loss = 0.692062973976530 / 700 :    loss = 0.69194149971540 / 700 :    loss = 0.691906034946550 / 700 :    loss = 0.691872596741560 / 700 :    loss = 0.691877245903570 / 700 :    loss = 0.691827952862580 / 700 :    loss = 0.691732943058590 / 700 :    loss = 0.691644191742600 / 700 :    loss = 0.691693246365610 / 700 :    loss = 0.691739618778620 / 700 :    loss = 0.691746711731630 / 700 :    loss = 0.691754281521640 / 700 :    loss = 0.691762447357650 / 700 :    loss = 0.691794812679660 / 700 :    loss = 0.691815435886670 / 700 :    loss = 0.69181060791680 / 700 :    loss = 0.691792547703690 / 700 :    loss = 0.691797733307()
Training acc (only root node): 0.564285714286
Valiation acc (only root node): 0.57
[[ 280.   70.]
 [ 235.  115.]]
[[ 41.   9.]
 [ 34.  16.]]
annealed lr to 0.000878
epoch 9
0 / 700 :    loss = 0.69280552864110 / 700 :    loss = 0.69071656465520 / 700 :    loss = 0.68944942951230 / 700 :    loss = 0.69070446491240 / 700 :    loss = 0.69119983911550 / 700 :    loss = 0.69099366664960 / 700 :    loss = 0.69053941965170 / 700 :    loss = 0.69061118364380 / 700 :    loss = 0.69077891111490 / 700 :    loss = 0.690975129604100 / 700 :    loss = 0.691470861435110 / 700 :    loss = 0.691981613636120 / 700 :    loss = 0.691717982292130 / 700 :    loss = 0.691842556140 / 700 :    loss = 0.692196547985150 / 700 :    loss = 0.692050278187160 / 700 :    loss = 0.691908657551170 / 700 :    loss = 0.692220211029180 / 700 :    loss = 0.691972672939190 / 700 :    loss = 0.692094564438200 / 700 :    loss = 0.691999018192210 / 700 :    loss = 0.691973507404220 / 700 :    loss = 0.692242145538230 / 700 :    loss = 0.692197740078240 / 700 :    loss = 0.692229807377250 / 700 :    loss = 0.692265748978260 / 700 :    loss = 0.692390918732270 / 700 :    loss = 0.692339241505280 / 700 :    loss = 0.692168712616290 / 700 :    loss = 0.692156732082300 / 700 :    loss = 0.692008197308310 / 700 :    loss = 0.692017495632320 / 700 :    loss = 0.69206571579330 / 700 :    loss = 0.691661417484340 / 700 :    loss = 0.69159412384350 / 700 :    loss = 0.691622376442360 / 700 :    loss = 0.691726922989370 / 700 :    loss = 0.691627860069380 / 700 :    loss = 0.691588461399390 / 700 :    loss = 0.691585898399400 / 700 :    loss = 0.691602170467410 / 700 :    loss = 0.691665232182420 / 700 :    loss = 0.691547334194430 / 700 :    loss = 0.691525399685440 / 700 :    loss = 0.69147080183450 / 700 :    loss = 0.69158488512460 / 700 :    loss = 0.691580891609470 / 700 :    loss = 0.69172501564480 / 700 :    loss = 0.691733717918490 / 700 :    loss = 0.691769182682500 / 700 :    loss = 0.691810190678510 / 700 :    loss = 0.691834211349520 / 700 :    loss = 0.691855072975530 / 700 :    loss = 0.691736936569540 / 700 :    loss = 0.69171243906550 / 700 :    loss = 0.691680192947560 / 700 :    loss = 0.691674292088570 / 700 :    loss = 0.691623985767580 / 700 :    loss = 0.691528439522590 / 700 :    loss = 0.69143807888600 / 700 :    loss = 0.691486537457610 / 700 :    loss = 0.691544532776620 / 700 :    loss = 0.691556930542630 / 700 :    loss = 0.69156140089640 / 700 :    loss = 0.691578388214650 / 700 :    loss = 0.691605210304660 / 700 :    loss = 0.69162094593670 / 700 :    loss = 0.691618561745680 / 700 :    loss = 0.691609740257690 / 700 :    loss = 0.691621661186()
Training acc (only root node): 0.568571428571
Valiation acc (only root node): 0.59
[[ 256.   94.]
 [ 208.  142.]]
[[ 38.  12.]
 [ 29.  21.]]
annealed lr to 0.000585
epoch 10
0 / 700 :    loss = 0.69052141904810 / 700 :    loss = 0.69014263153120 / 700 :    loss = 0.68897694349330 / 700 :    loss = 0.69030404090940 / 700 :    loss = 0.6908373832750 / 700 :    loss = 0.69064176082660 / 700 :    loss = 0.69022876024270 / 700 :    loss = 0.69034278392880 / 700 :    loss = 0.69049781560990 / 700 :    loss = 0.690732777119100 / 700 :    loss = 0.691215515137110 / 700 :    loss = 0.691740036011120 / 700 :    loss = 0.691487789154130 / 700 :    loss = 0.691606938839140 / 700 :    loss = 0.691962301731150 / 700 :    loss = 0.691820263863160 / 700 :    loss = 0.691699981689170 / 700 :    loss = 0.692012429237180 / 700 :    loss = 0.691773355007190 / 700 :    loss = 0.691919147968200 / 700 :    loss = 0.691816210747210 / 700 :    loss = 0.691793739796220 / 700 :    loss = 0.692045807838230 / 700 :    loss = 0.692007184029240 / 700 :    loss = 0.692043423653250 / 700 :    loss = 0.692088186741260 / 700 :    loss = 0.692206561565270 / 700 :    loss = 0.692161858082280 / 700 :    loss = 0.69198769331290 / 700 :    loss = 0.691978812218300 / 700 :    loss = 0.691830277443310 / 700 :    loss = 0.691846370697320 / 700 :    loss = 0.69189286232330 / 700 :    loss = 0.691531479359340 / 700 :    loss = 0.691458761692350 / 700 :    loss = 0.691475391388360 / 700 :    loss = 0.691562414169370 / 700 :    loss = 0.691463887691380 / 700 :    loss = 0.691426753998390 / 700 :    loss = 0.691422700882400 / 700 :    loss = 0.69143730402410 / 700 :    loss = 0.691500544548420 / 700 :    loss = 0.691383779049430 / 700 :    loss = 0.691360771656440 / 700 :    loss = 0.691317558289450 / 700 :    loss = 0.691427111626460 / 700 :    loss = 0.691426157951470 / 700 :    loss = 0.691564559937480 / 700 :    loss = 0.691573023796490 / 700 :    loss = 0.69160836935500 / 700 :    loss = 0.691649675369510 / 700 :    loss = 0.691674768925520 / 700 :    loss = 0.69169574976530 / 700 :    loss = 0.691582083702540 / 700 :    loss = 0.691566884518550 / 700 :    loss = 0.691536784172560 / 700 :    loss = 0.691522181034570 / 700 :    loss = 0.691469728947580 / 700 :    loss = 0.691373288631590 / 700 :    loss = 0.691283583641600 / 700 :    loss = 0.691330969334610 / 700 :    loss = 0.69139444828620 / 700 :    loss = 0.691410958767630 / 700 :    loss = 0.691414356232640 / 700 :    loss = 0.691438615322650 / 700 :    loss = 0.691461920738660 / 700 :    loss = 0.691474080086670 / 700 :    loss = 0.691474437714680 / 700 :    loss = 0.691474616528690 / 700 :    loss = 0.691493570805()
Training acc (only root node): 0.584285714286
Valiation acc (only root node): 0.57
[[ 230.  120.]
 [ 171.  179.]]
[[ 34.  16.]
 [ 27.  23.]]
annealed lr to 0.000390
epoch 11
0 / 700 :    loss = 0.68850767612510 / 700 :    loss = 0.68967652320920 / 700 :    loss = 0.68860715627730 / 700 :    loss = 0.68999058008240 / 700 :    loss = 0.69054669141850 / 700 :    loss = 0.69033783674260 / 700 :    loss = 0.68992698192670 / 700 :    loss = 0.6900656223380 / 700 :    loss = 0.69023746252190 / 700 :    loss = 0.690490663052100 / 700 :    loss = 0.690982401371110 / 700 :    loss = 0.691518723965120 / 700 :    loss = 0.691276848316130 / 700 :    loss = 0.691403567791140 / 700 :    loss = 0.691765189171150 / 700 :    loss = 0.691625714302160 / 700 :    loss = 0.691515922546170 / 700 :    loss = 0.691832244396180 / 700 :    loss = 0.691599726677190 / 700 :    loss = 0.69176197052200 / 700 :    loss = 0.691657543182210 / 700 :    loss = 0.69163954258220 / 700 :    loss = 0.691882610321230 / 700 :    loss = 0.69184923172240 / 700 :    loss = 0.691889703274250 / 700 :    loss = 0.69194227457260 / 700 :    loss = 0.692056357861270 / 700 :    loss = 0.692018508911280 / 700 :    loss = 0.69184076786290 / 700 :    loss = 0.691834330559300 / 700 :    loss = 0.691686928272310 / 700 :    loss = 0.691708087921320 / 700 :    loss = 0.691754341125330 / 700 :    loss = 0.691422820091340 / 700 :    loss = 0.691348731518350 / 700 :    loss = 0.69135838747360 / 700 :    loss = 0.69142973423370 / 700 :    loss = 0.691333532333380 / 700 :    loss = 0.691296696663390 / 700 :    loss = 0.69129383564400 / 700 :    loss = 0.691307783127410 / 700 :    loss = 0.691371142864420 / 700 :    loss = 0.69125688076430 / 700 :    loss = 0.691233277321440 / 700 :    loss = 0.691200613976450 / 700 :    loss = 0.691306889057460 / 700 :    loss = 0.691309452057470 / 700 :    loss = 0.691441774368480 / 700 :    loss = 0.691450417042490 / 700 :    loss = 0.691485702991500 / 700 :    loss = 0.691527485847510 / 700 :    loss = 0.691554188728520 / 700 :    loss = 0.691574692726530 / 700 :    loss = 0.691466510296540 / 700 :    loss = 0.691459953785550 / 700 :    loss = 0.69143217802560 / 700 :    loss = 0.691410303116570 / 700 :    loss = 0.691355228424580 / 700 :    loss = 0.691257476807590 / 700 :    loss = 0.691169857979600 / 700 :    loss = 0.69121581316610 / 700 :    loss = 0.691280186176620 / 700 :    loss = 0.691299021244630 / 700 :    loss = 0.691302359104640 / 700 :    loss = 0.691331088543650 / 700 :    loss = 0.691352844238660 / 700 :    loss = 0.691362917423670 / 700 :    loss = 0.691365420818680 / 700 :    loss = 0.691372275352690 / 700 :    loss = 0.691396951675()
Training acc (only root node): 0.578571428571
Valiation acc (only root node): 0.55
[[ 199.  151.]
 [ 144.  206.]]
[[ 30.  20.]
 [ 25.  25.]]
annealed lr to 0.000260
epoch 12
0 / 700 :    loss = 0.68700802326210 / 700 :    loss = 0.68935567140620 / 700 :    loss = 0.68835771083830 / 700 :    loss = 0.68977963924440 / 700 :    loss = 0.69035053253250 / 700 :    loss = 0.6901218295160 / 700 :    loss = 0.68969666957970 / 700 :    loss = 0.68984675407480 / 700 :    loss = 0.69004011154290 / 700 :    loss = 0.6902987957100 / 700 :    loss = 0.690805017948110 / 700 :    loss = 0.691349327564120 / 700 :    loss = 0.69111430645130 / 700 :    loss = 0.691253244877140 / 700 :    loss = 0.691621661186150 / 700 :    loss = 0.691482067108160 / 700 :    loss = 0.691374480724170 / 700 :    loss = 0.691695451736180 / 700 :    loss = 0.691466450691190 / 700 :    loss = 0.691637456417200 / 700 :    loss = 0.691533923149210 / 700 :    loss = 0.691519618034220 / 700 :    loss = 0.69176030159230 / 700 :    loss = 0.691732525826240 / 700 :    loss = 0.691776990891250 / 700 :    loss = 0.691836416721260 / 700 :    loss = 0.691946446896270 / 700 :    loss = 0.691915273666280 / 700 :    loss = 0.691732466221290 / 700 :    loss = 0.691727101803300 / 700 :    loss = 0.691581130028310 / 700 :    loss = 0.691604852676320 / 700 :    loss = 0.691651701927330 / 700 :    loss = 0.691337704659340 / 700 :    loss = 0.691263794899350 / 700 :    loss = 0.691269814968360 / 700 :    loss = 0.691331267357370 / 700 :    loss = 0.691236793995380 / 700 :    loss = 0.691200375557390 / 700 :    loss = 0.691198647022400 / 700 :    loss = 0.691212356091410 / 700 :    loss = 0.691275954247420 / 700 :    loss = 0.691163778305430 / 700 :    loss = 0.691139817238440 / 700 :    loss = 0.69111508131450 / 700 :    loss = 0.691219329834460 / 700 :    loss = 0.691224873066470 / 700 :    loss = 0.691352427006480 / 700 :    loss = 0.691361308098490 / 700 :    loss = 0.691396653652500 / 700 :    loss = 0.69143897295510 / 700 :    loss = 0.691467106342520 / 700 :    loss = 0.691487073898530 / 700 :    loss = 0.691384077072540 / 700 :    loss = 0.691384375095550 / 700 :    loss = 0.691358923912560 / 700 :    loss = 0.691331446171570 / 700 :    loss = 0.691273808479580 / 700 :    loss = 0.691174983978590 / 700 :    loss = 0.691089689732600 / 700 :    loss = 0.691134274006610 / 700 :    loss = 0.691197156906620 / 700 :    loss = 0.691217064857630 / 700 :    loss = 0.691220641136640 / 700 :    loss = 0.691251575947650 / 700 :    loss = 0.691272735596660 / 700 :    loss = 0.69128203392670 / 700 :    loss = 0.691285789013680 / 700 :    loss = 0.691296815872690 / 700 :    loss = 0.691325545311()
Training acc (only root node): 0.575714285714
Valiation acc (only root node): 0.52
[[ 181.  169.]
 [ 128.  222.]]
[[ 27.  23.]
 [ 25.  25.]]
annealed lr to 0.000173
epoch 13
0 / 700 :    loss = 0.68601322174110 / 700 :    loss = 0.68915420770620 / 700 :    loss = 0.6882029771830 / 700 :    loss = 0.6896502375640 / 700 :    loss = 0.69022971391750 / 700 :    loss = 0.6899833679260 / 700 :    loss = 0.689542949270 / 700 :    loss = 0.68969762325380 / 700 :    loss = 0.68990832567290 / 700 :    loss = 0.690167427063100 / 700 :    loss = 0.690685987473110 / 700 :    loss = 0.691235125065120 / 700 :    loss = 0.691004097462130 / 700 :    loss = 0.69115370512140 / 700 :    loss = 0.691527485847150 / 700 :    loss = 0.691386640072160 / 700 :    loss = 0.691277801991170 / 700 :    loss = 0.691602408886180 / 700 :    loss = 0.691374778748190 / 700 :    loss = 0.691550374031200 / 700 :    loss = 0.691447854042210 / 700 :    loss = 0.691436171532220 / 700 :    loss = 0.69167727232230 / 700 :    loss = 0.691654086113240 / 700 :    loss = 0.691701948643250 / 700 :    loss = 0.691766738892260 / 700 :    loss = 0.691872894764270 / 700 :    loss = 0.69184756279280 / 700 :    loss = 0.691659212112290 / 700 :    loss = 0.691654145718300 / 700 :    loss = 0.69150942564310 / 700 :    loss = 0.691533923149320 / 700 :    loss = 0.691581249237330 / 700 :    loss = 0.691276788712340 / 700 :    loss = 0.691203296185350 / 700 :    loss = 0.69120746851360 / 700 :    loss = 0.69126355648370 / 700 :    loss = 0.691170156002380 / 700 :    loss = 0.691134333611390 / 700 :    loss = 0.691133081913400 / 700 :    loss = 0.691146731377410 / 700 :    loss = 0.691210567951420 / 700 :    loss = 0.691099643707430 / 700 :    loss = 0.691075563431440 / 700 :    loss = 0.691056072712450 / 700 :    loss = 0.691159069538460 / 700 :    loss = 0.691166639328470 / 700 :    loss = 0.691291034222480 / 700 :    loss = 0.691300213337490 / 700 :    loss = 0.691335618496500 / 700 :    loss = 0.691378235817510 / 700 :    loss = 0.691407561302520 / 700 :    loss = 0.691426992416530 / 700 :    loss = 0.691327929497540 / 700 :    loss = 0.691333413124550 / 700 :    loss = 0.691309869289560 / 700 :    loss = 0.691278278828570 / 700 :    loss = 0.691218733788580 / 700 :    loss = 0.691118896008590 / 700 :    loss = 0.691035568714600 / 700 :    loss = 0.691079139709610 / 700 :    loss = 0.691139936447620 / 700 :    loss = 0.691160261631630 / 700 :    loss = 0.691164016724640 / 700 :    loss = 0.691196084023650 / 700 :    loss = 0.691217064857660 / 700 :    loss = 0.691226005554670 / 700 :    loss = 0.691230535507680 / 700 :    loss = 0.691243946552690 / 700 :    loss = 0.691275238991()
Training acc (only root node): 0.581428571429
Valiation acc (only root node): 0.51
[[ 171.  179.]
 [ 114.  236.]]
[[ 26.  24.]
 [ 25.  25.]]
annealed lr to 0.000116
epoch 14
0 / 700 :    loss = 0.6853790283210 / 700 :    loss = 0.68902969360420 / 700 :    loss = 0.6881074905430 / 700 :    loss = 0.68957036733640 / 700 :    loss = 0.6901553273250 / 700 :    loss = 0.68989646434860 / 700 :    loss = 0.68944424390870 / 700 :    loss = 0.68960112333380 / 700 :    loss = 0.68982362747290 / 700 :    loss = 0.690081894398100 / 700 :    loss = 0.690609037876110 / 700 :    loss = 0.691160917282120 / 700 :    loss = 0.69093221426130 / 700 :    loss = 0.691089749336140 / 700 :    loss = 0.691467285156150 / 700 :    loss = 0.691325008869160 / 700 :    loss = 0.691214323044170 / 700 :    loss = 0.691541552544180 / 700 :    loss = 0.691314458847190 / 700 :    loss = 0.691492199898200 / 700 :    loss = 0.691390693188210 / 700 :    loss = 0.691380560398220 / 700 :    loss = 0.691622614861230 / 700 :    loss = 0.691603124142240 / 700 :    loss = 0.691653311253250 / 700 :    loss = 0.691721975803260 / 700 :    loss = 0.691825151443270 / 700 :    loss = 0.691804111004280 / 700 :    loss = 0.691611170769290 / 700 :    loss = 0.691605985165300 / 700 :    loss = 0.691462099552310 / 700 :    loss = 0.69148683548320 / 700 :    loss = 0.691534638405330 / 700 :    loss = 0.691235125065340 / 700 :    loss = 0.691162109375350 / 700 :    loss = 0.691165268421360 / 700 :    loss = 0.691218495369370 / 700 :    loss = 0.691125512123380 / 700 :    loss = 0.691090524197390 / 700 :    loss = 0.691089212894400 / 700 :    loss = 0.691102743149410 / 700 :    loss = 0.691166996956420 / 700 :    loss = 0.691056728363430 / 700 :    loss = 0.691032528877440 / 700 :    loss = 0.691016316414450 / 700 :    loss = 0.691118597984460 / 700 :    loss = 0.691127538681470 / 700 :    loss = 0.691249907017480 / 700 :    loss = 0.691259205341490 / 700 :    loss = 0.69129472971W tensorflow/core/framework/op_kernel.cc:993] Invalid argument: Unsuccessful TensorSliceReader constructor: Failed to get matching files on ./weights/rnn_embed=35_l2=0.020000_lr=0.010000.weights.temp: Resource exhausted: ./weights
Traceback (most recent call last):
  File "rnn.py", line 450, in <module>
    test_RNN()
  File "rnn.py", line 432, in test_RNN
    stats = model.train(verbose=True)
  File "rnn.py", line 379, in train
    train_acc, val_acc, loss_history, val_loss = self.run_epoch()
  File "rnn.py", line 313, in run_epoch
    saver.restore(sess, './weights/%s.temp'%self.config.model_name)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 1439, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 767, in run
    run_metadata_ptr)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 965, in _run
    feed_dict_string, options, run_metadata)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1015, in _do_run
    target_list, options, run_metadata)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on ./weights/rnn_embed=35_l2=0.020000_lr=0.010000.weights.temp: Resource exhausted: ./weights
	 [[Node: save/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device="/job:localhost/replica:0/task:0/cpu:0"](_recv_save/Const_0, save/RestoreV2_2/tensor_names, save/RestoreV2_2/shape_and_slices)]]

Caused by op u'save/RestoreV2_2', defined at:
  File "rnn.py", line 450, in <module>
    test_RNN()
  File "rnn.py", line 432, in test_RNN
    stats = model.train(verbose=True)
  File "rnn.py", line 379, in train
    train_acc, val_acc, loss_history, val_loss = self.run_epoch()
  File "rnn.py", line 312, in run_epoch
    saver = tf.train.Saver()
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 1051, in __init__
    self.build()
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 1081, in build
    restore_sequentially=self._restore_sequentially)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 675, in build
    restore_sequentially, reshape)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 402, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 242, in restore_op
    [spec.tensor.dtype])[0])
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py", line 668, in restore_v2
    dtypes=dtypes, name=name)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 763, in apply_op
    op_def=op_def)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on ./weights/rnn_embed=35_l2=0.020000_lr=0.010000.weights.temp: Resource exhausted: ./weights
	 [[Node: save/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device="/job:localhost/replica:0/task:0/cpu:0"](_recv_save/Const_0, save/RestoreV2_2/tensor_names, save/RestoreV2_2/shape_and_slices)]]

