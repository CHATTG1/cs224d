I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.7.5 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
NVIDIA: no NVIDIA devices found
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_UNKNOWN
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:145] kernel driver does not appear to be running on this host (ip-172-30-1-62): /proc/driver/nvidia/version does not exist
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcupti.so.7.5 locally
Loading train trees..
Loading dev trees..
Loading test trees..
1852 1510
5058.0 total words with 1568 uniques
epoch 0
0 / 700 :    loss = 0.68407070636710 / 700 :    loss = 0.69098573923120 / 700 :    loss = 0.69025230407730 / 700 :    loss = 0.6917686462440 / 700 :    loss = 0.69258058071150 / 700 :    loss = 0.69216310977960 / 700 :    loss = 0.69106304645570 / 700 :    loss = 0.69103097915680 / 700 :    loss = 0.69159215688790 / 700 :    loss = 0.691789627075100 / 700 :    loss = 0.692345023155110 / 700 :    loss = 0.693033158779120 / 700 :    loss = 0.692836463451130 / 700 :    loss = 0.693029284477140 / 700 :    loss = 0.693509042263150 / 700 :    loss = 0.693408071995160 / 700 :    loss = 0.69302302599170 / 700 :    loss = 0.693431317806180 / 700 :    loss = 0.693260669708190 / 700 :    loss = 0.693267822266200 / 700 :    loss = 0.693271458149210 / 700 :    loss = 0.693281292915220 / 700 :    loss = 0.693313121796230 / 700 :    loss = 0.693145096302240 / 700 :    loss = 0.693190336227250 / 700 :    loss = 0.693208873272260 / 700 :    loss = 0.693368852139270 / 700 :    loss = 0.693402171135280 / 700 :    loss = 0.693063557148290 / 700 :    loss = 0.693081378937300 / 700 :    loss = 0.693018436432310 / 700 :    loss = 0.692991614342320 / 700 :    loss = 0.693133890629330 / 700 :    loss = 0.6922057271340 / 700 :    loss = 0.692381024361350 / 700 :    loss = 0.692419171333360 / 700 :    loss = 0.692120313644370 / 700 :    loss = 0.692200541496380 / 700 :    loss = 0.692124307156390 / 700 :    loss = 0.692224085331400 / 700 :    loss = 0.692287325859410 / 700 :    loss = 0.692399203777420 / 700 :    loss = 0.692321836948430 / 700 :    loss = 0.692341446877440 / 700 :    loss = 0.692130506039450 / 700 :    loss = 0.692321658134460 / 700 :    loss = 0.692335963249470 / 700 :    loss = 0.69245249033480 / 700 :    loss = 0.692499577999490 / 700 :    loss = 0.692576706409500 / 700 :    loss = 0.692610621452510 / 700 :    loss = 0.692652881145520 / 700 :    loss = 0.692703127861530 / 700 :    loss = 0.692524313927540 / 700 :    loss = 0.692449808121550 / 700 :    loss = 0.692450225353560 / 700 :    loss = 0.692463099957570 / 700 :    loss = 0.692395567894580 / 700 :    loss = 0.692301213741590 / 700 :    loss = 0.692236125469600 / 700 :    loss = 0.692314267159610 / 700 :    loss = 0.692213892937620 / 700 :    loss = 0.692253887653630 / 700 :    loss = 0.692291736603640 / 700 :    loss = 0.692305207253650 / 700 :    loss = 0.692342877388660 / 700 :    loss = 0.69236189127670 / 700 :    loss = 0.69237947464680 / 700 :    loss = 0.692327320576690 / 700 :    loss = 0.692329823971()
Training acc (only root node): 0.55
Valiation acc (only root node): 0.53
[[ 302.   48.]
 [ 267.   83.]]
[[ 45.   5.]
 [ 42.   8.]]
epoch 1
0 / 700 :    loss = 0.69648081064210 / 700 :    loss = 0.69121849536920 / 700 :    loss = 0.69015479087830 / 700 :    loss = 0.69152295589440 / 700 :    loss = 0.69208157062550 / 700 :    loss = 0.69162279367460 / 700 :    loss = 0.69052034616570 / 700 :    loss = 0.6904799938280 / 700 :    loss = 0.69099640846390 / 700 :    loss = 0.691157639027100 / 700 :    loss = 0.691667854786110 / 700 :    loss = 0.692333340645120 / 700 :    loss = 0.692080140114130 / 700 :    loss = 0.692246973515140 / 700 :    loss = 0.692693829536150 / 700 :    loss = 0.69258928299160 / 700 :    loss = 0.692198932171170 / 700 :    loss = 0.692599356174180 / 700 :    loss = 0.692410469055190 / 700 :    loss = 0.692419707775200 / 700 :    loss = 0.692410349846210 / 700 :    loss = 0.692430078983220 / 700 :    loss = 0.692455232143230 / 700 :    loss = 0.692306578159240 / 700 :    loss = 0.692347943783250 / 700 :    loss = 0.692366182804260 / 700 :    loss = 0.692534863949270 / 700 :    loss = 0.692561507225280 / 700 :    loss = 0.692221641541290 / 700 :    loss = 0.692235469818300 / 700 :    loss = 0.692174017429310 / 700 :    loss = 0.692148447037320 / 700 :    loss = 0.692288696766330 / 700 :    loss = 0.691365897655340 / 700 :    loss = 0.691540062428350 / 700 :    loss = 0.691579461098360 / 700 :    loss = 0.691278874874370 / 700 :    loss = 0.691363811493380 / 700 :    loss = 0.691290199757390 / 700 :    loss = 0.691395640373400 / 700 :    loss = 0.691440939903410 / 700 :    loss = 0.691561162472420 / 700 :    loss = 0.691480636597430 / 700 :    loss = 0.691498577595440 / 700 :    loss = 0.691291213036450 / 700 :    loss = 0.69148182869460 / 700 :    loss = 0.69149106741470 / 700 :    loss = 0.69160926342480 / 700 :    loss = 0.69165545702490 / 700 :    loss = 0.691729485989500 / 700 :    loss = 0.691768944263510 / 700 :    loss = 0.691814661026520 / 700 :    loss = 0.691866517067530 / 700 :    loss = 0.691685557365540 / 700 :    loss = 0.691617131233550 / 700 :    loss = 0.691617488861560 / 700 :    loss = 0.691632449627570 / 700 :    loss = 0.691568493843580 / 700 :    loss = 0.691462218761590 / 700 :    loss = 0.691392123699600 / 700 :    loss = 0.691466152668610 / 700 :    loss = 0.691364467144620 / 700 :    loss = 0.691404879093630 / 700 :    loss = 0.691447198391640 / 700 :    loss = 0.691452920437650 / 700 :    loss = 0.691490054131660 / 700 :    loss = 0.691507160664670 / 700 :    loss = 0.691529154778680 / 700 :    loss = 0.691475868225690 / 700 :    loss = 0.691477835178()
Training acc (only root node): 0.561428571429
Valiation acc (only root node): 0.54
[[ 304.   46.]
 [ 261.   89.]]
[[ 45.   5.]
 [ 41.   9.]]
annealed lr to 0.006667
epoch 2
0 / 700 :    loss = 0.69544190168410 / 700 :    loss = 0.68989044427920 / 700 :    loss = 0.68861299753230 / 700 :    loss = 0.68999052047740 / 700 :    loss = 0.69043976068550 / 700 :    loss = 0.69000148773260 / 700 :    loss = 0.68903589248770 / 700 :    loss = 0.68901085853680 / 700 :    loss = 0.68953514099190 / 700 :    loss = 0.689636707306100 / 700 :    loss = 0.690161526203110 / 700 :    loss = 0.69079720974120 / 700 :    loss = 0.690528929234130 / 700 :    loss = 0.690709888935140 / 700 :    loss = 0.691088378429150 / 700 :    loss = 0.690999031067160 / 700 :    loss = 0.69070482254170 / 700 :    loss = 0.691098988056180 / 700 :    loss = 0.69085919857190 / 700 :    loss = 0.690881848335200 / 700 :    loss = 0.690852999687210 / 700 :    loss = 0.69087690115220 / 700 :    loss = 0.690990209579230 / 700 :    loss = 0.690877258778240 / 700 :    loss = 0.690917730331250 / 700 :    loss = 0.690917670727260 / 700 :    loss = 0.691137194633270 / 700 :    loss = 0.691123127937280 / 700 :    loss = 0.690845370293290 / 700 :    loss = 0.690830826759300 / 700 :    loss = 0.690760552883310 / 700 :    loss = 0.690743029118320 / 700 :    loss = 0.690864264965330 / 700 :    loss = 0.690050005913340 / 700 :    loss = 0.690173208714350 / 700 :    loss = 0.690246343613360 / 700 :    loss = 0.690068483353370 / 700 :    loss = 0.690119028091380 / 700 :    loss = 0.690038979053390 / 700 :    loss = 0.690161824226400 / 700 :    loss = 0.690175950527410 / 700 :    loss = 0.690294027328420 / 700 :    loss = 0.690198719501430 / 700 :    loss = 0.690205574036440 / 700 :    loss = 0.690045893192450 / 700 :    loss = 0.690235197544460 / 700 :    loss = 0.690230667591470 / 700 :    loss = 0.690376281738480 / 700 :    loss = 0.69040620327490 / 700 :    loss = 0.690461575985500 / 700 :    loss = 0.690519690514510 / 700 :    loss = 0.690563797951520 / 700 :    loss = 0.69061011076530 / 700 :    loss = 0.690441668034540 / 700 :    loss = 0.690376460552550 / 700 :    loss = 0.690371096134560 / 700 :    loss = 0.690408587456570 / 700 :    loss = 0.69034910202580 / 700 :    loss = 0.690227150917590 / 700 :    loss = 0.690153896809600 / 700 :    loss = 0.690213382244610 / 700 :    loss = 0.690145075321620 / 700 :    loss = 0.690166473389630 / 700 :    loss = 0.690218746662640 / 700 :    loss = 0.690205991268650 / 700 :    loss = 0.690247774124660 / 700 :    loss = 0.690262854099670 / 700 :    loss = 0.690286397934680 / 700 :    loss = 0.690243721008690 / 700 :    loss = 0.690240502357()
Training acc (only root node): 0.568571428571
Valiation acc (only root node): 0.53
[[ 304.   46.]
 [ 256.   94.]]
[[ 44.   6.]
 [ 41.   9.]]
annealed lr to 0.004444
epoch 3
0 / 700 :    loss = 0.69461768865610 / 700 :    loss = 0.68913573026720 / 700 :    loss = 0.68763250112530 / 700 :    loss = 0.68902689218540 / 700 :    loss = 0.68942081928350 / 700 :    loss = 0.68897944688860 / 700 :    loss = 0.68811774253870 / 700 :    loss = 0.688103139480 / 700 :    loss = 0.68858748674490 / 700 :    loss = 0.688650071621100 / 700 :    loss = 0.689189314842110 / 700 :    loss = 0.689803302288120 / 700 :    loss = 0.689518630505130 / 700 :    loss = 0.689727187157140 / 700 :    loss = 0.690064787865150 / 700 :    loss = 0.689969182014160 / 700 :    loss = 0.689742565155170 / 700 :    loss = 0.690121591091180 / 700 :    loss = 0.689858198166190 / 700 :    loss = 0.689899325371200 / 700 :    loss = 0.689854562283210 / 700 :    loss = 0.689881086349220 / 700 :    loss = 0.690072059631230 / 700 :    loss = 0.68999671936240 / 700 :    loss = 0.690026521683250 / 700 :    loss = 0.690019190311260 / 700 :    loss = 0.690267264843270 / 700 :    loss = 0.690219759941280 / 700 :    loss = 0.689996600151290 / 700 :    loss = 0.689969956875300 / 700 :    loss = 0.689880371094310 / 700 :    loss = 0.689868986607320 / 700 :    loss = 0.689973711967330 / 700 :    loss = 0.689251184464340 / 700 :    loss = 0.689322173595350 / 700 :    loss = 0.689415216446360 / 700 :    loss = 0.689363479614370 / 700 :    loss = 0.689359366894380 / 700 :    loss = 0.689284741879390 / 700 :    loss = 0.689402520657400 / 700 :    loss = 0.689396262169410 / 700 :    loss = 0.689505279064420 / 700 :    loss = 0.689402699471430 / 700 :    loss = 0.689401209354440 / 700 :    loss = 0.689279139042450 / 700 :    loss = 0.689452171326460 / 700 :    loss = 0.689439237118470 / 700 :    loss = 0.689603567123480 / 700 :    loss = 0.689621627331490 / 700 :    loss = 0.689664661884500 / 700 :    loss = 0.689729690552510 / 700 :    loss = 0.689776360989520 / 700 :    loss = 0.68981641531530 / 700 :    loss = 0.689661979675540 / 700 :    loss = 0.689608156681550 / 700 :    loss = 0.689592897892560 / 700 :    loss = 0.689640879631570 / 700 :    loss = 0.689590811729580 / 700 :    loss = 0.68945902586590 / 700 :    loss = 0.689376175404600 / 700 :    loss = 0.689425885677610 / 700 :    loss = 0.689385890961620 / 700 :    loss = 0.689394652843630 / 700 :    loss = 0.689446151257640 / 700 :    loss = 0.689422667027650 / 700 :    loss = 0.689466774464660 / 700 :    loss = 0.689483523369670 / 700 :    loss = 0.689504146576680 / 700 :    loss = 0.689468502998690 / 700 :    loss = 0.689464449883()
Training acc (only root node): 0.574285714286
Valiation acc (only root node): 0.53
[[ 302.   48.]
 [ 250.  100.]]
[[ 44.   6.]
 [ 41.   9.]]
annealed lr to 0.002963
epoch 4
0 / 700 :    loss = 0.69369935989410 / 700 :    loss = 0.68866658210820 / 700 :    loss = 0.68699872493730 / 700 :    loss = 0.68839538097440 / 700 :    loss = 0.68878376483950 / 700 :    loss = 0.68837320804660 / 700 :    loss = 0.68761682510470 / 700 :    loss = 0.68761020898880 / 700 :    loss = 0.68799376487790 / 700 :    loss = 0.688045859337100 / 700 :    loss = 0.688565850258110 / 700 :    loss = 0.689165830612120 / 700 :    loss = 0.688864350319130 / 700 :    loss = 0.689101159573140 / 700 :    loss = 0.689419150352150 / 700 :    loss = 0.689304351807160 / 700 :    loss = 0.689113438129170 / 700 :    loss = 0.689481317997180 / 700 :    loss = 0.68920558691190 / 700 :    loss = 0.689265727997200 / 700 :    loss = 0.68920147419210 / 700 :    loss = 0.68922740221220 / 700 :    loss = 0.689470887184230 / 700 :    loss = 0.689435839653240 / 700 :    loss = 0.689460456371250 / 700 :    loss = 0.689456284046260 / 700 :    loss = 0.689701378345270 / 700 :    loss = 0.689637601376280 / 700 :    loss = 0.689451158047290 / 700 :    loss = 0.689423084259300 / 700 :    loss = 0.689315676689310 / 700 :    loss = 0.689314126968320 / 700 :    loss = 0.689402997494330 / 700 :    loss = 0.688768625259340 / 700 :    loss = 0.688790380955350 / 700 :    loss = 0.688881218433360 / 700 :    loss = 0.688924252987370 / 700 :    loss = 0.688874363899380 / 700 :    loss = 0.688816666603390 / 700 :    loss = 0.688911437988400 / 700 :    loss = 0.688890635967410 / 700 :    loss = 0.688993573189420 / 700 :    loss = 0.688884913921430 / 700 :    loss = 0.688876450062440 / 700 :    loss = 0.688782930374450 / 700 :    loss = 0.688938438892460 / 700 :    loss = 0.688922464848470 / 700 :    loss = 0.689095199108480 / 700 :    loss = 0.689106523991490 / 700 :    loss = 0.689142763615500 / 700 :    loss = 0.689208388329510 / 700 :    loss = 0.689256608486520 / 700 :    loss = 0.689292728901530 / 700 :    loss = 0.689150750637540 / 700 :    loss = 0.689112842083550 / 700 :    loss = 0.689090073109560 / 700 :    loss = 0.689134776592570 / 700 :    loss = 0.689093232155580 / 700 :    loss = 0.688957512379590 / 700 :    loss = 0.688863396645600 / 700 :    loss = 0.688908457756610 / 700 :    loss = 0.688894689083620 / 700 :    loss = 0.68889850378630 / 700 :    loss = 0.688942432404640 / 700 :    loss = 0.688916683197650 / 700 :    loss = 0.688957631588660 / 700 :    loss = 0.688974916935670 / 700 :    loss = 0.688992857933680 / 700 :    loss = 0.68895983696690 / 700 :    loss = 0.688956797123()
Training acc (only root node): 0.578571428571
Valiation acc (only root node): 0.53
[[ 298.   52.]
 [ 243.  107.]]
[[ 44.   6.]
 [ 41.   9.]]
annealed lr to 0.001975
epoch 5
0 / 700 :    loss = 0.69253402948410 / 700 :    loss = 0.68825972080220 / 700 :    loss = 0.68650180101430 / 700 :    loss = 0.68792766332640 / 700 :    loss = 0.68833345174850 / 700 :    loss = 0.68797546625160 / 700 :    loss = 0.68734133243670 / 700 :    loss = 0.68736350536380 / 700 :    loss = 0.68762391805690 / 700 :    loss = 0.687698960304100 / 700 :    loss = 0.688169896603110 / 700 :    loss = 0.688760340214120 / 700 :    loss = 0.688449680805130 / 700 :    loss = 0.688699364662140 / 700 :    loss = 0.689009428024150 / 700 :    loss = 0.688876152039160 / 700 :    loss = 0.688703835011170 / 700 :    loss = 0.689064621925180 / 700 :    loss = 0.688783109188190 / 700 :    loss = 0.688861310482200 / 700 :    loss = 0.688780903816210 / 700 :    loss = 0.688804149628220 / 700 :    loss = 0.689072489738230 / 700 :    loss = 0.689072489738240 / 700 :    loss = 0.689100086689250 / 700 :    loss = 0.689106106758260 / 700 :    loss = 0.689327597618270 / 700 :    loss = 0.6892619133280 / 700 :    loss = 0.689088881016290 / 700 :    loss = 0.689061522484300 / 700 :    loss = 0.688943266869310 / 700 :    loss = 0.688952863216320 / 700 :    loss = 0.689028918743330 / 700 :    loss = 0.688476383686340 / 700 :    loss = 0.688462197781350 / 700 :    loss = 0.688537299633360 / 700 :    loss = 0.688626348972370 / 700 :    loss = 0.688550651073380 / 700 :    loss = 0.688512504101390 / 700 :    loss = 0.688582539558400 / 700 :    loss = 0.688552498817410 / 700 :    loss = 0.688652515411420 / 700 :    loss = 0.688537180424430 / 700 :    loss = 0.68852519989440 / 700 :    loss = 0.688450753689450 / 700 :    loss = 0.688592672348460 / 700 :    loss = 0.688575983047470 / 700 :    loss = 0.688749790192480 / 700 :    loss = 0.688757538795490 / 700 :    loss = 0.688790380955500 / 700 :    loss = 0.688855588436510 / 700 :    loss = 0.688903689384520 / 700 :    loss = 0.688938558102530 / 700 :    loss = 0.688804745674540 / 700 :    loss = 0.688782930374550 / 700 :    loss = 0.688756465912560 / 700 :    loss = 0.688790500164570 / 700 :    loss = 0.688752949238580 / 700 :    loss = 0.688615560532590 / 700 :    loss = 0.688513278961600 / 700 :    loss = 0.688556075096610 / 700 :    loss = 0.68856459856620 / 700 :    loss = 0.688570201397630 / 700 :    loss = 0.688605606556640 / 700 :    loss = 0.688583314419650 / 700 :    loss = 0.688617706299660 / 700 :    loss = 0.688632726669670 / 700 :    loss = 0.688649833202680 / 700 :    loss = 0.688620209694690 / 700 :    loss = 0.688620209694()
Training acc (only root node): 0.581428571429
Valiation acc (only root node): 0.54
[[ 288.   62.]
 [ 231.  119.]]
[[ 44.   6.]
 [ 40.  10.]]
annealed lr to 0.001317
epoch 6
0 / 700 :    loss = 0.69100749492610 / 700 :    loss = 0.68781459331520 / 700 :    loss = 0.68605262041130 / 700 :    loss = 0.68753290176440 / 700 :    loss = 0.68797588348450 / 700 :    loss = 0.6876678466860 / 700 :    loss = 0.68715006113170 / 700 :    loss = 0.68721592426380 / 700 :    loss = 0.68736732006190 / 700 :    loss = 0.687491118908100 / 700 :    loss = 0.687900900841110 / 700 :    loss = 0.688492894173120 / 700 :    loss = 0.688183069229130 / 700 :    loss = 0.688425898552140 / 700 :    loss = 0.688729941845150 / 700 :    loss = 0.688586831093160 / 700 :    loss = 0.688435554504170 / 700 :    loss = 0.688793003559180 / 700 :    loss = 0.688509702682190 / 700 :    loss = 0.688605427742200 / 700 :    loss = 0.688508450985210 / 700 :    loss = 0.688528358936220 / 700 :    loss = 0.688798189163230 / 700 :    loss = 0.688820838928240 / 700 :    loss = 0.68885332346250 / 700 :    loss = 0.688873052597260 / 700 :    loss = 0.689070105553270 / 700 :    loss = 0.689009487629280 / 700 :    loss = 0.68883472681290 / 700 :    loss = 0.688808143139300 / 700 :    loss = 0.68868458271310 / 700 :    loss = 0.688703417778320 / 700 :    loss = 0.688771545887330 / 700 :    loss = 0.688284695148340 / 700 :    loss = 0.688248157501350 / 700 :    loss = 0.68830370903360 / 700 :    loss = 0.688402235508370 / 700 :    loss = 0.68831384182380 / 700 :    loss = 0.68828946352390 / 700 :    loss = 0.68834233284400 / 700 :    loss = 0.688306331635410 / 700 :    loss = 0.688406467438420 / 700 :    loss = 0.688285112381430 / 700 :    loss = 0.688271224499440 / 700 :    loss = 0.688209176064450 / 700 :    loss = 0.688342571259460 / 700 :    loss = 0.688326537609470 / 700 :    loss = 0.688498735428480 / 700 :    loss = 0.688504934311490 / 700 :    loss = 0.688535988331500 / 700 :    loss = 0.688601076603510 / 700 :    loss = 0.688648700714520 / 700 :    loss = 0.688684165478530 / 700 :    loss = 0.688554644585540 / 700 :    loss = 0.688546299934550 / 700 :    loss = 0.688519597054560 / 700 :    loss = 0.688542068005570 / 700 :    loss = 0.688505291939580 / 700 :    loss = 0.688366770744590 / 700 :    loss = 0.688260197639600 / 700 :    loss = 0.688302099705610 / 700 :    loss = 0.688326895237620 / 700 :    loss = 0.688337743282630 / 700 :    loss = 0.688367187977640 / 700 :    loss = 0.688352227211650 / 700 :    loss = 0.688379406929660 / 700 :    loss = 0.688389778137670 / 700 :    loss = 0.688407897949680 / 700 :    loss = 0.688384413719690 / 700 :    loss = 0.688389122486()
Training acc (only root node): 0.58
Valiation acc (only root node): 0.52
[[ 271.   79.]
 [ 215.  135.]]
[[ 39.  11.]
 [ 37.  13.]]
annealed lr to 0.000878
epoch 7
0 / 700 :    loss = 0.68908381462110 / 700 :    loss = 0.68729150295320 / 700 :    loss = 0.68559670448330 / 700 :    loss = 0.68714445829440 / 700 :    loss = 0.68763172626550 / 700 :    loss = 0.68735855817860 / 700 :    loss = 0.68692451715570 / 700 :    loss = 0.68703931570180 / 700 :    loss = 0.68712633848290 / 700 :    loss = 0.687303245068100 / 700 :    loss = 0.687669754028110 / 700 :    loss = 0.68827021122120 / 700 :    loss = 0.687964379787130 / 700 :    loss = 0.688194155693140 / 700 :    loss = 0.688495516777150 / 700 :    loss = 0.688353061676160 / 700 :    loss = 0.688225209713170 / 700 :    loss = 0.688581228256180 / 700 :    loss = 0.688303291798190 / 700 :    loss = 0.688419520855200 / 700 :    loss = 0.688313186169210 / 700 :    loss = 0.688331902027220 / 700 :    loss = 0.688587903976230 / 700 :    loss = 0.688621103764240 / 700 :    loss = 0.688659012318250 / 700 :    loss = 0.688690066338260 / 700 :    loss = 0.688871383667270 / 700 :    loss = 0.68881714344280 / 700 :    loss = 0.688638031483290 / 700 :    loss = 0.688612043858300 / 700 :    loss = 0.68848657608310 / 700 :    loss = 0.688513159752320 / 700 :    loss = 0.688577353954330 / 700 :    loss = 0.688142061234340 / 700 :    loss = 0.688093662262350 / 700 :    loss = 0.688133776188360 / 700 :    loss = 0.688222467899370 / 700 :    loss = 0.688130199909380 / 700 :    loss = 0.688113152981390 / 700 :    loss = 0.688157439232400 / 700 :    loss = 0.688117861748410 / 700 :    loss = 0.688218474388420 / 700 :    loss = 0.688094258308430 / 700 :    loss = 0.688079535961440 / 700 :    loss = 0.688027024269450 / 700 :    loss = 0.688154697418460 / 700 :    loss = 0.688140511513470 / 700 :    loss = 0.688309371471480 / 700 :    loss = 0.688314914703490 / 700 :    loss = 0.688345313072500 / 700 :    loss = 0.688410401344510 / 700 :    loss = 0.688458204269520 / 700 :    loss = 0.688494741917530 / 700 :    loss = 0.688368320465540 / 700 :    loss = 0.688370645046550 / 700 :    loss = 0.688345193863560 / 700 :    loss = 0.688357412815570 / 700 :    loss = 0.68831974268580 / 700 :    loss = 0.688180267811590 / 700 :    loss = 0.688071966171600 / 700 :    loss = 0.688113093376610 / 700 :    loss = 0.688148319721620 / 700 :    loss = 0.688164830208630 / 700 :    loss = 0.688190937042640 / 700 :    loss = 0.688184261322650 / 700 :    loss = 0.688205778599660 / 700 :    loss = 0.688211500645670 / 700 :    loss = 0.688231706619680 / 700 :    loss = 0.688216626644690 / 700 :    loss = 0.688227415085()
Training acc (only root node): 0.598571428571
Valiation acc (only root node): 0.53
[[ 256.   94.]
 [ 187.  163.]]
[[ 37.  13.]
 [ 34.  16.]]
annealed lr to 0.000585
epoch 8
0 / 700 :    loss = 0.68698447942710 / 700 :    loss = 0.68674588203420 / 700 :    loss = 0.68515133857730 / 700 :    loss = 0.68677115440440 / 700 :    loss = 0.68729752302250 / 700 :    loss = 0.6870343089160 / 700 :    loss = 0.6866375207970 / 700 :    loss = 0.68679052591380 / 700 :    loss = 0.68686437606890 / 700 :    loss = 0.687078535557100 / 700 :    loss = 0.687433004379110 / 700 :    loss = 0.688046336174120 / 700 :    loss = 0.687749683857130 / 700 :    loss = 0.687976300716140 / 700 :    loss = 0.688280463219150 / 700 :    loss = 0.688142895699160 / 700 :    loss = 0.688034892082170 / 700 :    loss = 0.688393235207180 / 700 :    loss = 0.688123643398190 / 700 :    loss = 0.688259720802200 / 700 :    loss = 0.688148975372210 / 700 :    loss = 0.688169658184220 / 700 :    loss = 0.68840944767230 / 700 :    loss = 0.688447117805240 / 700 :    loss = 0.688489437103250 / 700 :    loss = 0.68852865696260 / 700 :    loss = 0.688702940941270 / 700 :    loss = 0.688654065132280 / 700 :    loss = 0.688473463058290 / 700 :    loss = 0.688449025154300 / 700 :    loss = 0.688323557377310 / 700 :    loss = 0.688357412815320 / 700 :    loss = 0.688419818878330 / 700 :    loss = 0.688025176525340 / 700 :    loss = 0.687971591949350 / 700 :    loss = 0.688000559807360 / 700 :    loss = 0.688071489334370 / 700 :    loss = 0.687979996204380 / 700 :    loss = 0.687965393066390 / 700 :    loss = 0.68800765276400 / 700 :    loss = 0.687965929508410 / 700 :    loss = 0.688066601753420 / 700 :    loss = 0.68794298172430 / 700 :    loss = 0.687927126884440 / 700 :    loss = 0.687884747982450 / 700 :    loss = 0.688008248806460 / 700 :    loss = 0.687996685505470 / 700 :    loss = 0.68816113472480 / 700 :    loss = 0.688166558743490 / 700 :    loss = 0.688196659088500 / 700 :    loss = 0.688262104988510 / 700 :    loss = 0.688310921192520 / 700 :    loss = 0.688347935677530 / 700 :    loss = 0.688225388527540 / 700 :    loss = 0.688236892223550 / 700 :    loss = 0.688213348389560 / 700 :    loss = 0.688217163086570 / 700 :    loss = 0.688177764416580 / 700 :    loss = 0.688037157059590 / 700 :    loss = 0.687929213047600 / 700 :    loss = 0.687969505787610 / 700 :    loss = 0.688009858131620 / 700 :    loss = 0.688030421734630 / 700 :    loss = 0.688055336475640 / 700 :    loss = 0.688055336475650 / 700 :    loss = 0.688073515892660 / 700 :    loss = 0.688075900078670 / 700 :    loss = 0.688098430634680 / 700 :    loss = 0.688091635704690 / 700 :    loss = 0.688108682632()
Training acc (only root node): 0.612857142857
Valiation acc (only root node): 0.53
[[ 234.  116.]
 [ 155.  195.]]
[[ 32.  18.]
 [ 29.  21.]]
annealed lr to 0.000390
epoch 9
0 / 700 :    loss = 0.68509918451310 / 700 :    loss = 0.68628752231620 / 700 :    loss = 0.68478763103530 / 700 :    loss = 0.68647027015740 / 700 :    loss = 0.68702656030750 / 700 :    loss = 0.68675345182460 / 700 :    loss = 0.68635785579770 / 700 :    loss = 0.68653273582580 / 700 :    loss = 0.68662136793190 / 700 :    loss = 0.686852991581100 / 700 :    loss = 0.687216043472110 / 700 :    loss = 0.68784058094120 / 700 :    loss = 0.687552690506130 / 700 :    loss = 0.687787532806140 / 700 :    loss = 0.68809825182150 / 700 :    loss = 0.687963247299160 / 700 :    loss = 0.687865376472170 / 700 :    loss = 0.688228964806180 / 700 :    loss = 0.687965750694190 / 700 :    loss = 0.688115477562200 / 700 :    loss = 0.688004851341210 / 700 :    loss = 0.688029050827220 / 700 :    loss = 0.688259541988230 / 700 :    loss = 0.688301563263240 / 700 :    loss = 0.688348174095250 / 700 :    loss = 0.688394606113260 / 700 :    loss = 0.688564479351270 / 700 :    loss = 0.68852096796280 / 700 :    loss = 0.688338637352290 / 700 :    loss = 0.688315927982300 / 700 :    loss = 0.688191413879310 / 700 :    loss = 0.688230633736320 / 700 :    loss = 0.688292860985330 / 700 :    loss = 0.68792706728340 / 700 :    loss = 0.687871992588350 / 700 :    loss = 0.687894105911360 / 700 :    loss = 0.687949419022370 / 700 :    loss = 0.687860012054380 / 700 :    loss = 0.68784570694390 / 700 :    loss = 0.687888622284400 / 700 :    loss = 0.687846064568410 / 700 :    loss = 0.687946796417420 / 700 :    loss = 0.687825202942430 / 700 :    loss = 0.687808573246440 / 700 :    loss = 0.687775850296450 / 700 :    loss = 0.687896251678460 / 700 :    loss = 0.68788766861470 / 700 :    loss = 0.688047230244480 / 700 :    loss = 0.68805283308490 / 700 :    loss = 0.688082814217500 / 700 :    loss = 0.688148617744510 / 700 :    loss = 0.688198983669520 / 700 :    loss = 0.68823581934530 / 700 :    loss = 0.68811827898540 / 700 :    loss = 0.688137769699550 / 700 :    loss = 0.688116550446560 / 700 :    loss = 0.68811327219570 / 700 :    loss = 0.688071489334580 / 700 :    loss = 0.687929689884590 / 700 :    loss = 0.687823474407600 / 700 :    loss = 0.687862634659610 / 700 :    loss = 0.687903881073620 / 700 :    loss = 0.687926828861630 / 700 :    loss = 0.687951564789640 / 700 :    loss = 0.687955915928650 / 700 :    loss = 0.68797236681660 / 700 :    loss = 0.687972784042670 / 700 :    loss = 0.687997221947680 / 700 :    loss = 0.687996566296690 / 700 :    loss = 0.688018858433()
Training acc (only root node): 0.614285714286
Valiation acc (only root node): 0.53
[[ 215.  135.]
 [ 135.  215.]]
[[ 31.  19.]
 [ 28.  22.]]
annealed lr to 0.000260
epoch 10
0 / 700 :    loss = 0.68367815017710 / 700 :    loss = 0.68596291542120 / 700 :    loss = 0.68453675508530 / 700 :    loss = 0.6862651109740 / 700 :    loss = 0.68684023618750 / 700 :    loss = 0.68655151128860 / 700 :    loss = 0.6861425638270 / 700 :    loss = 0.68632757663780 / 700 :    loss = 0.68643736839390 / 700 :    loss = 0.686673700809100 / 700 :    loss = 0.687051177025110 / 700 :    loss = 0.687682926655120 / 700 :    loss = 0.68740093708130 / 700 :    loss = 0.687647819519140 / 700 :    loss = 0.687965452671150 / 700 :    loss = 0.687830448151160 / 700 :    loss = 0.687734782696170 / 700 :    loss = 0.688103199005180 / 700 :    loss = 0.687843382359190 / 700 :    loss = 0.688000738621200 / 700 :    loss = 0.68789178133210 / 700 :    loss = 0.687919318676220 / 700 :    loss = 0.688147068024230 / 700 :    loss = 0.68819385767240 / 700 :    loss = 0.688244462013250 / 700 :    loss = 0.688297271729260 / 700 :    loss = 0.688462853432270 / 700 :    loss = 0.688424766064280 / 700 :    loss = 0.688238859177290 / 700 :    loss = 0.688216805458300 / 700 :    loss = 0.688093543053310 / 700 :    loss = 0.688135683537320 / 700 :    loss = 0.688198268414330 / 700 :    loss = 0.687850058079340 / 700 :    loss = 0.687795162201350 / 700 :    loss = 0.687813699245360 / 700 :    loss = 0.687858641148370 / 700 :    loss = 0.68777102232380 / 700 :    loss = 0.687757015228390 / 700 :    loss = 0.687800943851400 / 700 :    loss = 0.687757909298410 / 700 :    loss = 0.687858641148420 / 700 :    loss = 0.687738895416430 / 700 :    loss = 0.687722027302440 / 700 :    loss = 0.687696695328450 / 700 :    loss = 0.687815070152460 / 700 :    loss = 0.687808990479470 / 700 :    loss = 0.687964558601480 / 700 :    loss = 0.687970280647490 / 700 :    loss = 0.688000321388500 / 700 :    loss = 0.688066482544510 / 700 :    loss = 0.68811827898520 / 700 :    loss = 0.688154757023530 / 700 :    loss = 0.688041985035540 / 700 :    loss = 0.688068091869550 / 700 :    loss = 0.688049018383560 / 700 :    loss = 0.688040137291570 / 700 :    loss = 0.687996089458580 / 700 :    loss = 0.68785315752590 / 700 :    loss = 0.687749087811600 / 700 :    loss = 0.687787055969610 / 700 :    loss = 0.687826931477620 / 700 :    loss = 0.687851011753630 / 700 :    loss = 0.68787586689640 / 700 :    loss = 0.687882423401650 / 700 :    loss = 0.687898159027660 / 700 :    loss = 0.68789768219670 / 700 :    loss = 0.687923252583680 / 700 :    loss = 0.687926590443690 / 700 :    loss = 0.687952637672()
Training acc (only root node): 0.614285714286
Valiation acc (only root node): 0.53
[[ 204.  146.]
 [ 124.  226.]]
[[ 29.  21.]
 [ 26.  24.]]
annealed lr to 0.000173
epoch 11
0 / 700 :    loss = 0.68272846937210 / 700 :    loss = 0.68575567007120 / 700 :    loss = 0.68437922000930 / 700 :    loss = 0.68613785505340 / 700 :    loss = 0.68672442436250 / 700 :    loss = 0.68642139434860 / 700 :    loss = 0.68599784374270 / 700 :    loss = 0.6861870884980 / 700 :    loss = 0.68631440401190 / 700 :    loss = 0.686550140381100 / 700 :    loss = 0.686939954758110 / 700 :    loss = 0.687575817108120 / 700 :    loss = 0.687297463417130 / 700 :    loss = 0.687554836273140 / 700 :    loss = 0.687877953053150 / 700 :    loss = 0.687741696835160 / 700 :    loss = 0.687644541264170 / 700 :    loss = 0.688016712666180 / 700 :    loss = 0.687758147717190 / 700 :    loss = 0.687919437885200 / 700 :    loss = 0.687811851501210 / 700 :    loss = 0.687841773033220 / 700 :    loss = 0.688069880009230 / 700 :    loss = 0.688121080399240 / 700 :    loss = 0.688174903393250 / 700 :    loss = 0.688232898712260 / 700 :    loss = 0.688394367695270 / 700 :    loss = 0.688361108303280 / 700 :    loss = 0.688170492649290 / 700 :    loss = 0.688148617744300 / 700 :    loss = 0.688026428223310 / 700 :    loss = 0.688069701195320 / 700 :    loss = 0.688132882118330 / 700 :    loss = 0.687794208527340 / 700 :    loss = 0.687739729881350 / 700 :    loss = 0.687756478786360 / 700 :    loss = 0.687795519829370 / 700 :    loss = 0.687709033489380 / 700 :    loss = 0.687695622444390 / 700 :    loss = 0.687739908695400 / 700 :    loss = 0.687696814537410 / 700 :    loss = 0.687797605991420 / 700 :    loss = 0.687679111958430 / 700 :    loss = 0.687662065029440 / 700 :    loss = 0.687641561031450 / 700 :    loss = 0.687758803368460 / 700 :    loss = 0.687754571438470 / 700 :    loss = 0.687907397747480 / 700 :    loss = 0.687913298607490 / 700 :    loss = 0.687943339348500 / 700 :    loss = 0.688009858131510 / 700 :    loss = 0.688062667847520 / 700 :    loss = 0.688098728657530 / 700 :    loss = 0.687989592552540 / 700 :    loss = 0.688020765781550 / 700 :    loss = 0.688003540039560 / 700 :    loss = 0.687990486622570 / 700 :    loss = 0.68794465065580 / 700 :    loss = 0.687800943851590 / 700 :    loss = 0.687698662281600 / 700 :    loss = 0.687735557556610 / 700 :    loss = 0.68777358532620 / 700 :    loss = 0.687798023224630 / 700 :    loss = 0.687823176384640 / 700 :    loss = 0.687830746174650 / 700 :    loss = 0.687846243382660 / 700 :    loss = 0.687845349312670 / 700 :    loss = 0.687871575356680 / 700 :    loss = 0.687877237797690 / 700 :    loss = 0.687905669212()
Training acc (only root node): 0.607142857143
Valiation acc (only root node): 0.52
[[ 194.  156.]
 [ 119.  231.]]
[[ 26.  24.]
 [ 24.  26.]]
annealed lr to 0.000116
epoch 12
0 / 700 :    loss = 0.68212354183210 / 700 :    loss = 0.6856274604820 / 700 :    loss = 0.68428242206630 / 700 :    loss = 0.68606024980540 / 700 :    loss = 0.68665397167250 / 700 :    loss = 0.68634051084560 / 700 :    loss = 0.68590557575270 / 700 :    loss = 0.68609660863980 / 700 :    loss = 0.6862357258890 / 700 :    loss = 0.686470091343100 / 700 :    loss = 0.686868429184110 / 700 :    loss = 0.687506616116120 / 700 :    loss = 0.687230288982130 / 700 :    loss = 0.687495291233140 / 700 :    loss = 0.687822163105150 / 700 :    loss = 0.68768465519160 / 700 :    loss = 0.687585473061170 / 700 :    loss = 0.687960147858180 / 700 :    loss = 0.687702059746190 / 700 :    loss = 0.687865197659200 / 700 :    loss = 0.687758862972210 / 700 :    loss = 0.68779027462220 / 700 :    loss = 0.688019216061230 / 700 :    loss = 0.688073515892240 / 700 :    loss = 0.688129901886250 / 700 :    loss = 0.688191711903260 / 700 :    loss = 0.688349843025270 / 700 :    loss = 0.688320279121280 / 700 :    loss = 0.688125789165290 / 700 :    loss = 0.688103616238300 / 700 :    loss = 0.687982141972310 / 700 :    loss = 0.688025772572320 / 700 :    loss = 0.688089430332330 / 700 :    loss = 0.687755823135340 / 700 :    loss = 0.687701940536350 / 700 :    loss = 0.687717616558360 / 700 :    loss = 0.687753498554370 / 700 :    loss = 0.687667548656380 / 700 :    loss = 0.687654733658390 / 700 :    loss = 0.687699079514400 / 700 :    loss = 0.687655866146410 / 700 :    loss = 0.687756836414420 / 700 :    loss = 0.687639057636430 / 700 :    loss = 0.687621831894440 / 700 :    loss = 0.687604427338450 / 700 :    loss = 0.687721014023460 / 700 :    loss = 0.687717914581470 / 700 :    loss = 0.68786907196480 / 700 :    loss = 0.68787497282490 / 700 :    loss = 0.687905132771500 / 700 :    loss = 0.687971770763510 / 700 :    loss = 0.688025414944520 / 700 :    loss = 0.688061237335530 / 700 :    loss = 0.687954604626540 / 700 :    loss = 0.687989294529550 / 700 :    loss = 0.687973439693560 / 700 :    loss = 0.687957465649570 / 700 :    loss = 0.687910437584580 / 700 :    loss = 0.68776601553590 / 700 :    loss = 0.687665164471600 / 700 :    loss = 0.687701284885610 / 700 :    loss = 0.687737703323620 / 700 :    loss = 0.687762200832630 / 700 :    loss = 0.687787473202640 / 700 :    loss = 0.687795519829650 / 700 :    loss = 0.687810897827660 / 700 :    loss = 0.687809884548670 / 700 :    loss = 0.68783646822680 / 700 :    loss = 0.687843441963690 / 700 :    loss = 0.687873542309()
Training acc (only root node): 0.607142857143
Valiation acc (only root node): 0.53
[[ 190.  160.]
 [ 115.  235.]]
[[ 26.  24.]
 [ 23.  27.]]
annealed lr to 0.000077
epoch 13
0 / 700 :    loss = 0.68173730373410 / 700 :    loss = 0.68554681539520 / 700 :    loss = 0.68422204256130 / 700 :    loss = 0.68601185083440 / 700 :    loss = 0.68660998344450 / 700 :    loss = 0.68628931045560 / 700 :    loss = 0.68584668636370 / 700 :    loss = 0.68603867292480 / 700 :    loss = 0.68618535995590 / 700 :    loss = 0.68641859293100 / 700 :    loss = 0.686822593212110 / 700 :    loss = 0.687462031841120 / 700 :    loss = 0.687186777592130 / 700 :    loss = 0.687456905842140 / 700 :    loss = 0.687786340714150 / 700 :    loss = 0.687647819519160 / 700 :    loss = 0.687546789646170 / 700 :    loss = 0.687923252583180 / 700 :    loss = 0.68766528368190 / 700 :    loss = 0.687829434872200 / 700 :    loss = 0.687723875046210 / 700 :    loss = 0.687756299973220 / 700 :    loss = 0.687985956669230 / 700 :    loss = 0.688042700291240 / 700 :    loss = 0.688100874424250 / 700 :    loss = 0.688165307045260 / 700 :    loss = 0.688320875168270 / 700 :    loss = 0.688294053078280 / 700 :    loss = 0.688096523285290 / 700 :    loss = 0.688074052334300 / 700 :    loss = 0.68795311451310 / 700 :    loss = 0.68799674511320 / 700 :    loss = 0.688060700893330 / 700 :    loss = 0.687730014324340 / 700 :    loss = 0.687676370144350 / 700 :    loss = 0.687691628933360 / 700 :    loss = 0.68772560358370 / 700 :    loss = 0.687640070915380 / 700 :    loss = 0.687627732754390 / 700 :    loss = 0.687672078609400 / 700 :    loss = 0.687628686428410 / 700 :    loss = 0.68772983551420 / 700 :    loss = 0.687612295151430 / 700 :    loss = 0.687595188618440 / 700 :    loss = 0.687579631805450 / 700 :    loss = 0.687695682049460 / 700 :    loss = 0.687693536282470 / 700 :    loss = 0.687843561172480 / 700 :    loss = 0.687849521637490 / 700 :    loss = 0.687879741192500 / 700 :    loss = 0.687946498394510 / 700 :    loss = 0.688000619411520 / 700 :    loss = 0.688036322594530 / 700 :    loss = 0.68793129921540 / 700 :    loss = 0.687968432903550 / 700 :    loss = 0.687953472137560 / 700 :    loss = 0.687935650349570 / 700 :    loss = 0.68788766861580 / 700 :    loss = 0.687742769718590 / 700 :    loss = 0.687642991543600 / 700 :    loss = 0.68767863512610 / 700 :    loss = 0.687713742256620 / 700 :    loss = 0.687738180161630 / 700 :    loss = 0.687763690948640 / 700 :    loss = 0.687771916389650 / 700 :    loss = 0.687787294388660 / 700 :    loss = 0.687786102295670 / 700 :    loss = 0.687812924385680 / 700 :    loss = 0.687820792198690 / 700 :    loss = 0.687851786613()
Training acc (only root node): 0.611428571429
Valiation acc (only root node): 0.53
[[ 189.  161.]
 [ 111.  239.]]
[[ 26.  24.]
 [ 23.  27.]]
annealed lr to 0.000051
epoch 14
0 / 700 :    loss = 0.68149000406310 / 700 :    loss = 0.68549561500520 / 700 :    loss = 0.68418341875130 / 700 :    loss = 0.68598109483740 / 700 :    loss = 0.68658202886650 / 700 :    loss = 0.6862566471160 / 700 :    loss = 0.685808658670 / 700 :    loss = 0.68600100278980 / 700 :    loss = 0.68615281581990 / 700 :    loss = 0.686385154724100 / 700 :    loss = 0.68679267168110 / 700 :    loss = 0.687433183193120 / 700 :    loss = 0.687158584595130 / 700 :    loss = 0.687432169914140 / 700 :    loss = 0.687763273716150 / 700 :    loss = 0.687623977661160 / 700 :    loss = 0.687521696091170 / 700 :    loss = 0.687899172306180 / 700 :    loss = 0.687641263008190 / 700 :    loss = 0.687806129456200 / 700 :    loss = 0.687700867653210 / 700 :    loss = 0.687734007835220 / 700 :    loss = 0.687964379787230 / 700 :    loss = 0.688022613525240 / 700 :    loss = 0.688081860542250 / 700 :    loss = 0.688148200512260 / 700 :    loss = 0.688302099705270 / 700 :    loss = 0.688277244568280 / 700 :    loss = 0.688077270985290 / 700 :    loss = 0.688054680824300 / 700 :    loss = 0.687934041023310 / 700 :    loss = 0.687977612019320 / 700 :    loss = 0.688041806221330 / 700 :    loss = 0.687712728977340 / 700 :    loss = 0.687659323215350 / 700 :    loss = 0.687674224377360 / 700 :    loss = 0.68770724535370 / 700 :    loss = 0.687621891499380 / 700 :    loss = 0.68760997057390 / 700 :    loss = 0.687654137611400 / 700 :    loss = 0.687610805035410 / 700 :    loss = 0.687711894512420 / 700 :    loss = 0.687594652176430 / 700 :    loss = 0.687577605247440 / 700 :    loss = 0.687563121319450 / 700 :    loss = 0.687678933144460 / 700 :    loss = 0.687677264214470 / 700 :    loss = 0.687826573849480 / 700 :    loss = 0.687832593918490 / 700 :    loss = 0.687862873077W tensorflow/core/framework/op_kernel.cc:993] Invalid argument: Unsuccessful TensorSliceReader constructor: Failed to get matching files on ./weights/rnn_embed=35_l2=0.020000_lr=0.010000.weights.temp: Resource exhausted: ./weights
Traceback (most recent call last):
  File "rnn.py", line 454, in <module>
    test_RNN()
  File "rnn.py", line 436, in test_RNN
    stats = model.train(verbose=True)
  File "rnn.py", line 383, in train
    train_acc, val_acc, loss_history, val_loss = self.run_epoch()
  File "rnn.py", line 313, in run_epoch
    saver.restore(sess, './weights/%s.temp'%self.config.model_name)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 1439, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 767, in run
    run_metadata_ptr)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 965, in _run
    feed_dict_string, options, run_metadata)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1015, in _do_run
    target_list, options, run_metadata)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on ./weights/rnn_embed=35_l2=0.020000_lr=0.010000.weights.temp: Resource exhausted: ./weights
	 [[Node: save/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT], _device="/job:localhost/replica:0/task:0/cpu:0"](_recv_save/Const_0, save/RestoreV2_3/tensor_names, save/RestoreV2_3/shape_and_slices)]]

Caused by op u'save/RestoreV2_3', defined at:
  File "rnn.py", line 454, in <module>
    test_RNN()
  File "rnn.py", line 436, in test_RNN
    stats = model.train(verbose=True)
  File "rnn.py", line 383, in train
    train_acc, val_acc, loss_history, val_loss = self.run_epoch()
  File "rnn.py", line 312, in run_epoch
    saver = tf.train.Saver()
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 1051, in __init__
    self.build()
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 1081, in build
    restore_sequentially=self._restore_sequentially)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 675, in build
    restore_sequentially, reshape)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 402, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 242, in restore_op
    [spec.tensor.dtype])[0])
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py", line 668, in restore_v2
    dtypes=dtypes, name=name)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 763, in apply_op
    op_def=op_def)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on ./weights/rnn_embed=35_l2=0.020000_lr=0.010000.weights.temp: Resource exhausted: ./weights
	 [[Node: save/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT], _device="/job:localhost/replica:0/task:0/cpu:0"](_recv_save/Const_0, save/RestoreV2_3/tensor_names, save/RestoreV2_3/shape_and_slices)]]

