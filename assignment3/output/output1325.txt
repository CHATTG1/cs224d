Loading train trees..
Loading dev trees..
Loading test trees..
1852 1510
5058.0 total words with 1568 uniques
epoch 0
0 / 700 :    loss = 10.254313468910 / 700 :    loss = 3.7065687179620 / 700 :    loss = 2.4152655601530 / 700 :    loss = 1.961811423340 / 700 :    loss = 1.7170479297650 / 700 :    loss = 1.551977276860 / 700 :    loss = 1.4384423494370 / 700 :    loss = 1.3471004962980 / 700 :    loss = 1.2772425413190 / 700 :    loss = 1.2209624052100 / 700 :    loss = 1.19289636612110 / 700 :    loss = 1.29029154778120 / 700 :    loss = 1.30384385586130 / 700 :    loss = 1.30248224735140 / 700 :    loss = 1.29246771336150 / 700 :    loss = 1.27810502052160 / 700 :    loss = 1.25489985943170 / 700 :    loss = 1.24023246765180 / 700 :    loss = 1.21790146828190 / 700 :    loss = 1.1978366375200 / 700 :    loss = 1.17894923687210 / 700 :    loss = 1.17271637917220 / 700 :    loss = 1.15797400475230 / 700 :    loss = 1.14343261719240 / 700 :    loss = 1.12991976738250 / 700 :    loss = 1.11793673038260 / 700 :    loss = 1.10555005074270 / 700 :    loss = 1.09253489971280 / 700 :    loss = 1.08056020737290 / 700 :    loss = 1.06825482845300 / 700 :    loss = 1.05799257755310 / 700 :    loss = 1.04964137077320 / 700 :    loss = 1.03913128376330 / 700 :    loss = 1.0283011198340 / 700 :    loss = 1.01944196224350 / 700 :    loss = 1.01110112667360 / 700 :    loss = 1.00238919258370 / 700 :    loss = 0.994678258896380 / 700 :    loss = 0.987180829048390 / 700 :    loss = 0.979891121387400 / 700 :    loss = 0.977007091045410 / 700 :    loss = 0.979990184307420 / 700 :    loss = 0.979512929916430 / 700 :    loss = 0.977855443954440 / 700 :    loss = 0.974724411964450 / 700 :    loss = 0.970717430115460 / 700 :    loss = 0.966697335243470 / 700 :    loss = 0.962704956532480 / 700 :    loss = 0.957626879215490 / 700 :    loss = 0.953235149384500 / 700 :    loss = 0.955621004105510 / 700 :    loss = 0.992484867573520 / 700 :    loss = 0.993407726288530 / 700 :    loss = 0.990860342979540 / 700 :    loss = 0.987759113312550 / 700 :    loss = 0.984439373016560 / 700 :    loss = 0.981397211552570 / 700 :    loss = 0.977615594864580 / 700 :    loss = 0.973756253719590 / 700 :    loss = 0.969828605652600 / 700 :    loss = 0.97975564003610 / 700 :    loss = 0.99419015646620 / 700 :    loss = 0.994926929474630 / 700 :    loss = 0.99448633194640 / 700 :    loss = 0.99192148447650 / 700 :    loss = 0.989308059216660 / 700 :    loss = 0.986025989056670 / 700 :    loss = 0.983102917671680 / 700 :    loss = 0.979540586472690 / 700 :    loss = 0.976239323616()
Training acc (only root node): 0.54
Valiation acc (only root node): 0.49
[[ 176.  174.]
 [ 148.  202.]]
[[ 26.  24.]
 [ 27.  23.]]
epoch 1
0 / 700 :    loss = 0.74165606498710 / 700 :    loss = 0.73654818534920 / 700 :    loss = 0.72753554582630 / 700 :    loss = 0.72379666566840 / 700 :    loss = 0.72184234857650 / 700 :    loss = 0.71972537040760 / 700 :    loss = 0.7186074256970 / 700 :    loss = 0.71706086397280 / 700 :    loss = 0.71586507558890 / 700 :    loss = 0.714767515659100 / 700 :    loss = 0.714315652847110 / 700 :    loss = 0.714784026146120 / 700 :    loss = 0.714412212372130 / 700 :    loss = 0.71360129118140 / 700 :    loss = 0.713081300259150 / 700 :    loss = 0.712670445442160 / 700 :    loss = 0.711803674698170 / 700 :    loss = 0.711359500885180 / 700 :    loss = 0.710674405098190 / 700 :    loss = 0.709974110126200 / 700 :    loss = 0.709450960159210 / 700 :    loss = 0.70903801918220 / 700 :    loss = 0.708776950836230 / 700 :    loss = 0.708297014236240 / 700 :    loss = 0.708096086979250 / 700 :    loss = 0.707937598228260 / 700 :    loss = 0.7077216506270 / 700 :    loss = 0.707552015781280 / 700 :    loss = 0.707267045975290 / 700 :    loss = 0.706837952137300 / 700 :    loss = 0.706771731377310 / 700 :    loss = 0.706732273102320 / 700 :    loss = 0.706523656845330 / 700 :    loss = 0.705416798592340 / 700 :    loss = 0.705485701561350 / 700 :    loss = 0.70536762476360 / 700 :    loss = 0.704833447933370 / 700 :    loss = 0.704801380634380 / 700 :    loss = 0.70464861393390 / 700 :    loss = 0.704630613327400 / 700 :    loss = 0.70453864336410 / 700 :    loss = 0.704457700253420 / 700 :    loss = 0.70422744751430 / 700 :    loss = 0.70427185297440 / 700 :    loss = 0.703764140606450 / 700 :    loss = 0.703883290291460 / 700 :    loss = 0.703622162342470 / 700 :    loss = 0.703526675701480 / 700 :    loss = 0.703335106373490 / 700 :    loss = 0.703404784203500 / 700 :    loss = 0.703321576118510 / 700 :    loss = 0.703209638596520 / 700 :    loss = 0.703281402588530 / 700 :    loss = 0.703049063683540 / 700 :    loss = 0.702949047089550 / 700 :    loss = 0.7027977705560 / 700 :    loss = 0.702755331993570 / 700 :    loss = 0.702714622021580 / 700 :    loss = 0.702719390392590 / 700 :    loss = 0.702583193779600 / 700 :    loss = 0.702571034431610 / 700 :    loss = 0.702241718769620 / 700 :    loss = 0.702223062515630 / 700 :    loss = 0.702192246914640 / 700 :    loss = 0.702147006989650 / 700 :    loss = 0.702089846134660 / 700 :    loss = 0.701949119568670 / 700 :    loss = 0.701975345612680 / 700 :    loss = 0.701806902885690 / 700 :    loss = 0.701724529266()
Training acc (only root node): 0.562857142857
Valiation acc (only root node): 0.52
[[ 303.   47.]
 [ 259.   91.]]
[[ 44.   6.]
 [ 42.   8.]]
epoch 2
0 / 700 :    loss = 0.69288492202810 / 700 :    loss = 0.69687420129820 / 700 :    loss = 0.69685983657830 / 700 :    loss = 0.69642174243940 / 700 :    loss = 0.69650763273250 / 700 :    loss = 0.69713914394460 / 700 :    loss = 0.69678670167970 / 700 :    loss = 0.69636768102680 / 700 :    loss = 0.69661134481490 / 700 :    loss = 0.696414649487100 / 700 :    loss = 0.696713149548110 / 700 :    loss = 0.697391569614120 / 700 :    loss = 0.697821676731130 / 700 :    loss = 0.69785284996140 / 700 :    loss = 0.697861254215150 / 700 :    loss = 0.697996139526160 / 700 :    loss = 0.697624325752170 / 700 :    loss = 0.697707772255180 / 700 :    loss = 0.697514057159190 / 700 :    loss = 0.697246611118200 / 700 :    loss = 0.697239279747210 / 700 :    loss = 0.697165131569220 / 700 :    loss = 0.697129070759230 / 700 :    loss = 0.696984767914240 / 700 :    loss = 0.697095036507250 / 700 :    loss = 0.697266161442260 / 700 :    loss = 0.697405099869270 / 700 :    loss = 0.697496056557280 / 700 :    loss = 0.697471320629290 / 700 :    loss = 0.697286188602300 / 700 :    loss = 0.697405815125310 / 700 :    loss = 0.697512626648320 / 700 :    loss = 0.697514533997330 / 700 :    loss = 0.696618497372340 / 700 :    loss = 0.696852087975350 / 700 :    loss = 0.696891605854360 / 700 :    loss = 0.696531116962370 / 700 :    loss = 0.69667339325380 / 700 :    loss = 0.696636199951390 / 700 :    loss = 0.696732521057400 / 700 :    loss = 0.696761965752410 / 700 :    loss = 0.696776628494420 / 700 :    loss = 0.696680188179430 / 700 :    loss = 0.696801304817440 / 700 :    loss = 0.696429669857450 / 700 :    loss = 0.696640133858460 / 700 :    loss = 0.696507036686470 / 700 :    loss = 0.696496069431480 / 700 :    loss = 0.696425914764490 / 700 :    loss = 0.696577370167500 / 700 :    loss = 0.696568787098510 / 700 :    loss = 0.696529269218520 / 700 :    loss = 0.696658074856530 / 700 :    loss = 0.696508765221540 / 700 :    loss = 0.696478545666550 / 700 :    loss = 0.696397066116560 / 700 :    loss = 0.696428537369570 / 700 :    loss = 0.696461498737580 / 700 :    loss = 0.696521520615590 / 700 :    loss = 0.696462571621600 / 700 :    loss = 0.696511685848610 / 700 :    loss = 0.696259438992620 / 700 :    loss = 0.6963108778630 / 700 :    loss = 0.696349918842640 / 700 :    loss = 0.696345627308650 / 700 :    loss = 0.69635373354660 / 700 :    loss = 0.696286320686670 / 700 :    loss = 0.696354389191680 / 700 :    loss = 0.696249425411690 / 700 :    loss = 0.696223676205()
Training acc (only root node): 0.561428571429
Valiation acc (only root node): 0.51
[[ 309.   41.]
 [ 266.   84.]]
[[ 44.   6.]
 [ 43.   7.]]
annealed lr to 0.006667
epoch 3
0 / 700 :    loss = 0.69081592559810 / 700 :    loss = 0.69455009698920 / 700 :    loss = 0.69440811872530 / 700 :    loss = 0.69388800859540 / 700 :    loss = 0.69368135929150 / 700 :    loss = 0.69433677196560 / 700 :    loss = 0.69404941797370 / 700 :    loss = 0.6935448050580 / 700 :    loss = 0.69375765323690 / 700 :    loss = 0.69350373745100 / 700 :    loss = 0.693867683411110 / 700 :    loss = 0.69447940588120 / 700 :    loss = 0.694848179817130 / 700 :    loss = 0.694897651672140 / 700 :    loss = 0.694863379002150 / 700 :    loss = 0.695059597492160 / 700 :    loss = 0.694823026657170 / 700 :    loss = 0.69491559267180 / 700 :    loss = 0.694663882256190 / 700 :    loss = 0.69442898035200 / 700 :    loss = 0.694450855255210 / 700 :    loss = 0.694356918335220 / 700 :    loss = 0.694390177727230 / 700 :    loss = 0.69429653883240 / 700 :    loss = 0.694430112839250 / 700 :    loss = 0.694560647011260 / 700 :    loss = 0.694729506969270 / 700 :    loss = 0.694780528545280 / 700 :    loss = 0.694846570492290 / 700 :    loss = 0.694659233093300 / 700 :    loss = 0.694750607014310 / 700 :    loss = 0.694851100445320 / 700 :    loss = 0.694840788841330 / 700 :    loss = 0.694076240063340 / 700 :    loss = 0.694262087345350 / 700 :    loss = 0.694347441196360 / 700 :    loss = 0.694123983383370 / 700 :    loss = 0.694242298603380 / 700 :    loss = 0.694184482098390 / 700 :    loss = 0.694310963154400 / 700 :    loss = 0.694326758385410 / 700 :    loss = 0.694331765175420 / 700 :    loss = 0.694229543209430 / 700 :    loss = 0.694334566593440 / 700 :    loss = 0.694042503834450 / 700 :    loss = 0.69424790144460 / 700 :    loss = 0.694120764732470 / 700 :    loss = 0.694146096706480 / 700 :    loss = 0.694086849689490 / 700 :    loss = 0.694207012653500 / 700 :    loss = 0.694225788116510 / 700 :    loss = 0.694175899029520 / 700 :    loss = 0.69428986311530 / 700 :    loss = 0.694154798985540 / 700 :    loss = 0.694117963314550 / 700 :    loss = 0.694036543369560 / 700 :    loss = 0.694089949131570 / 700 :    loss = 0.694135844707580 / 700 :    loss = 0.69417822361590 / 700 :    loss = 0.694133341312600 / 700 :    loss = 0.694173872471610 / 700 :    loss = 0.693973481655620 / 700 :    loss = 0.694007754326630 / 700 :    loss = 0.694075286388640 / 700 :    loss = 0.694048643112650 / 700 :    loss = 0.694060921669660 / 700 :    loss = 0.693999290466670 / 700 :    loss = 0.694065272808680 / 700 :    loss = 0.69398266077690 / 700 :    loss = 0.693955481052()
Training acc (only root node): 0.567142857143
Valiation acc (only root node): 0.51
[[ 311.   39.]
 [ 264.   86.]]
[[ 44.   6.]
 [ 43.   7.]]
annealed lr to 0.004444
epoch 4
0 / 700 :    loss = 0.68946635723110 / 700 :    loss = 0.69321066141120 / 700 :    loss = 0.69299489259730 / 700 :    loss = 0.69252783060140 / 700 :    loss = 0.69216197729150 / 700 :    loss = 0.69277787208660 / 700 :    loss = 0.69256442785370 / 700 :    loss = 0.69206792116280 / 700 :    loss = 0.69224506616690 / 700 :    loss = 0.691944897175100 / 700 :    loss = 0.692351698875110 / 700 :    loss = 0.692939341068120 / 700 :    loss = 0.693242609501130 / 700 :    loss = 0.69332587719140 / 700 :    loss = 0.693261325359150 / 700 :    loss = 0.693460762501160 / 700 :    loss = 0.693327963352170 / 700 :    loss = 0.693395853043180 / 700 :    loss = 0.693116605282190 / 700 :    loss = 0.692908346653200 / 700 :    loss = 0.692934632301210 / 700 :    loss = 0.69283002615220 / 700 :    loss = 0.692943334579230 / 700 :    loss = 0.692893922329240 / 700 :    loss = 0.693030118942250 / 700 :    loss = 0.693143546581260 / 700 :    loss = 0.69332075119270 / 700 :    loss = 0.693335950375280 / 700 :    loss = 0.693463802338290 / 700 :    loss = 0.693278729916300 / 700 :    loss = 0.693340480328310 / 700 :    loss = 0.693440437317320 / 700 :    loss = 0.693420410156330 / 700 :    loss = 0.692758440971340 / 700 :    loss = 0.692890405655350 / 700 :    loss = 0.693002104759360 / 700 :    loss = 0.692919373512370 / 700 :    loss = 0.692982494831380 / 700 :    loss = 0.692913889885390 / 700 :    loss = 0.693041324615400 / 700 :    loss = 0.693043529987410 / 700 :    loss = 0.693038463593420 / 700 :    loss = 0.692932903767430 / 700 :    loss = 0.693022429943440 / 700 :    loss = 0.692783594131450 / 700 :    loss = 0.692963778973460 / 700 :    loss = 0.692836284637470 / 700 :    loss = 0.692887961864480 / 700 :    loss = 0.69283425808490 / 700 :    loss = 0.692937254906500 / 700 :    loss = 0.692964196205510 / 700 :    loss = 0.692916512489520 / 700 :    loss = 0.693013310432530 / 700 :    loss = 0.692892670631540 / 700 :    loss = 0.692856609821550 / 700 :    loss = 0.692767500877560 / 700 :    loss = 0.692831575871570 / 700 :    loss = 0.692892253399580 / 700 :    loss = 0.692924380302590 / 700 :    loss = 0.692874610424600 / 700 :    loss = 0.692908108234610 / 700 :    loss = 0.692747354507620 / 700 :    loss = 0.692770183086630 / 700 :    loss = 0.692846119404640 / 700 :    loss = 0.692805230618650 / 700 :    loss = 0.692818701267660 / 700 :    loss = 0.692760646343670 / 700 :    loss = 0.692819178104680 / 700 :    loss = 0.692748010159690 / 700 :    loss = 0.692718744278()
Training acc (only root node): 0.557142857143
Valiation acc (only root node): 0.51
[[ 322.   28.]
 [ 282.   68.]]
[[ 44.   6.]
 [ 43.   7.]]
annealed lr to 0.002963
epoch 5
0 / 700 :    loss = 0.68894463777510 / 700 :    loss = 0.69264888763420 / 700 :    loss = 0.69220399856630 / 700 :    loss = 0.69176602363640 / 700 :    loss = 0.69130212068650 / 700 :    loss = 0.69191098213260 / 700 :    loss = 0.69177657365870 / 700 :    loss = 0.69131022691780 / 700 :    loss = 0.69141060113990 / 700 :    loss = 0.691090762615100 / 700 :    loss = 0.691502213478110 / 700 :    loss = 0.692071914673120 / 700 :    loss = 0.692325472832130 / 700 :    loss = 0.692445933819140 / 700 :    loss = 0.692367017269150 / 700 :    loss = 0.692541897297160 / 700 :    loss = 0.692462444305170 / 700 :    loss = 0.692505538464180 / 700 :    loss = 0.692215979099190 / 700 :    loss = 0.692035794258200 / 700 :    loss = 0.692042589188210 / 700 :    loss = 0.691929876804220 / 700 :    loss = 0.692100763321230 / 700 :    loss = 0.692096948624240 / 700 :    loss = 0.692230403423250 / 700 :    loss = 0.692344605923260 / 700 :    loss = 0.69250524044270 / 700 :    loss = 0.69250535965280 / 700 :    loss = 0.692669391632290 / 700 :    loss = 0.692492485046300 / 700 :    loss = 0.692528963089310 / 700 :    loss = 0.692632138729320 / 700 :    loss = 0.69259750843330 / 700 :    loss = 0.692029595375340 / 700 :    loss = 0.692109405994350 / 700 :    loss = 0.692223906517360 / 700 :    loss = 0.692254126072370 / 700 :    loss = 0.692266523838380 / 700 :    loss = 0.692202091217390 / 700 :    loss = 0.692306816578400 / 700 :    loss = 0.692299187183410 / 700 :    loss = 0.692288219929420 / 700 :    loss = 0.692180931568430 / 700 :    loss = 0.6922570467440 / 700 :    loss = 0.692057847977450 / 700 :    loss = 0.692211091518460 / 700 :    loss = 0.692082762718470 / 700 :    loss = 0.692150294781480 / 700 :    loss = 0.692101478577490 / 700 :    loss = 0.692193746567500 / 700 :    loss = 0.692219793797510 / 700 :    loss = 0.69217467308520 / 700 :    loss = 0.692259311676530 / 700 :    loss = 0.692152678967540 / 700 :    loss = 0.69212436676550 / 700 :    loss = 0.692026376724560 / 700 :    loss = 0.692086577415570 / 700 :    loss = 0.692159116268580 / 700 :    loss = 0.692186653614590 / 700 :    loss = 0.692124307156600 / 700 :    loss = 0.692155182362610 / 700 :    loss = 0.692030012608620 / 700 :    loss = 0.692049443722630 / 700 :    loss = 0.692119419575640 / 700 :    loss = 0.692073106766650 / 700 :    loss = 0.692084312439660 / 700 :    loss = 0.692027628422670 / 700 :    loss = 0.692080080509680 / 700 :    loss = 0.692014753819690 / 700 :    loss = 0.691985070705()
Training acc (only root node): 0.555714285714
Valiation acc (only root node): 0.52
[[ 325.   25.]
 [ 286.   64.]]
[[ 45.   5.]
 [ 43.   7.]]
annealed lr to 0.001975
epoch 6
0 / 700 :    loss = 0.68833374977110 / 700 :    loss = 0.69228196144120 / 700 :    loss = 0.69170463085230 / 700 :    loss = 0.69130676984840 / 700 :    loss = 0.69079327583350 / 700 :    loss = 0.69142830371960 / 700 :    loss = 0.69139242172270 / 700 :    loss = 0.69097208976780 / 700 :    loss = 0.69095683097890 / 700 :    loss = 0.690649330616100 / 700 :    loss = 0.691019654274110 / 700 :    loss = 0.691576838493120 / 700 :    loss = 0.691796302795130 / 700 :    loss = 0.691935062408140 / 700 :    loss = 0.691851139069150 / 700 :    loss = 0.691995918751160 / 700 :    loss = 0.691937983036170 / 700 :    loss = 0.691964030266180 / 700 :    loss = 0.691671252251190 / 700 :    loss = 0.691518068314200 / 700 :    loss = 0.691500544548210 / 700 :    loss = 0.691380083561220 / 700 :    loss = 0.691577076912230 / 700 :    loss = 0.69161272049240 / 700 :    loss = 0.691748261452250 / 700 :    loss = 0.691875398159260 / 700 :    loss = 0.692003905773270 / 700 :    loss = 0.692006826401280 / 700 :    loss = 0.692175626755290 / 700 :    loss = 0.692004919052300 / 700 :    loss = 0.692025542259310 / 700 :    loss = 0.692135334015320 / 700 :    loss = 0.692088544369330 / 700 :    loss = 0.691607058048340 / 700 :    loss = 0.691646814346350 / 700 :    loss = 0.691747009754360 / 700 :    loss = 0.691841006279370 / 700 :    loss = 0.691823780537380 / 700 :    loss = 0.691771090031390 / 700 :    loss = 0.691845118999400 / 700 :    loss = 0.69183164835410 / 700 :    loss = 0.691816866398420 / 700 :    loss = 0.691706418991430 / 700 :    loss = 0.691774487495440 / 700 :    loss = 0.691601514816450 / 700 :    loss = 0.691733598709460 / 700 :    loss = 0.691606223583470 / 700 :    loss = 0.691679418087480 / 700 :    loss = 0.691635191441490 / 700 :    loss = 0.691722214222500 / 700 :    loss = 0.691745758057510 / 700 :    loss = 0.69170242548520 / 700 :    loss = 0.691779494286530 / 700 :    loss = 0.691684186459540 / 700 :    loss = 0.69166713953550 / 700 :    loss = 0.691563785076560 / 700 :    loss = 0.691611111164570 / 700 :    loss = 0.691689431667580 / 700 :    loss = 0.69171589613590 / 700 :    loss = 0.691642642021600 / 700 :    loss = 0.69167381525610 / 700 :    loss = 0.69157987833620 / 700 :    loss = 0.691603243351630 / 700 :    loss = 0.691662311554640 / 700 :    loss = 0.691617786884650 / 700 :    loss = 0.691624403660 / 700 :    loss = 0.691564977169670 / 700 :    loss = 0.691614627838680 / 700 :    loss = 0.691554248333690 / 700 :    loss = 0.691525518894()
Training acc (only root node): 0.547142857143
Valiation acc (only root node): 0.52
[[ 318.   32.]
 [ 285.   65.]]
[[ 45.   5.]
 [ 43.   7.]]
annealed lr to 0.001317
epoch 7
0 / 700 :    loss = 0.68732696771610 / 700 :    loss = 0.69188350439120 / 700 :    loss = 0.69131904840530 / 700 :    loss = 0.69097226858140 / 700 :    loss = 0.69044500589450 / 700 :    loss = 0.69112181663560 / 700 :    loss = 0.69119495153470 / 700 :    loss = 0.69083726406180 / 700 :    loss = 0.69070410728590 / 700 :    loss = 0.690441191196100 / 700 :    loss = 0.690742135048110 / 700 :    loss = 0.691288173199120 / 700 :    loss = 0.691489338875130 / 700 :    loss = 0.69161683321140 / 700 :    loss = 0.691529691219150 / 700 :    loss = 0.691655874252160 / 700 :    loss = 0.691613137722170 / 700 :    loss = 0.691628694534180 / 700 :    loss = 0.69133991003190 / 700 :    loss = 0.691216111183200 / 700 :    loss = 0.691175937653210 / 700 :    loss = 0.691048741341220 / 700 :    loss = 0.691243886948230 / 700 :    loss = 0.691303730011240 / 700 :    loss = 0.691444516182250 / 700 :    loss = 0.691587984562260 / 700 :    loss = 0.691686868668270 / 700 :    loss = 0.69170153141280 / 700 :    loss = 0.691854476929290 / 700 :    loss = 0.691685259342300 / 700 :    loss = 0.691698253155310 / 700 :    loss = 0.69181406498320 / 700 :    loss = 0.691759169102330 / 700 :    loss = 0.691348552704340 / 700 :    loss = 0.691362798214350 / 700 :    loss = 0.691442489624360 / 700 :    loss = 0.691558599472370 / 700 :    loss = 0.691527247429380 / 700 :    loss = 0.691486120224390 / 700 :    loss = 0.691536128521400 / 700 :    loss = 0.691519498825410 / 700 :    loss = 0.691503226757420 / 700 :    loss = 0.691387832165430 / 700 :    loss = 0.691451966763440 / 700 :    loss = 0.691295087337450 / 700 :    loss = 0.691413462162460 / 700 :    loss = 0.691287875175470 / 700 :    loss = 0.691360771656480 / 700 :    loss = 0.691319465637490 / 700 :    loss = 0.691404402256500 / 700 :    loss = 0.6914255023510 / 700 :    loss = 0.691382467747520 / 700 :    loss = 0.691455960274530 / 700 :    loss = 0.691368103027540 / 700 :    loss = 0.691362380981550 / 700 :    loss = 0.691257715225560 / 700 :    loss = 0.691290438175570 / 700 :    loss = 0.691368937492580 / 700 :    loss = 0.691394984722590 / 700 :    loss = 0.691315591335600 / 700 :    loss = 0.691347301006610 / 700 :    loss = 0.691277384758620 / 700 :    loss = 0.691308736801630 / 700 :    loss = 0.691358566284640 / 700 :    loss = 0.69132065773650 / 700 :    loss = 0.691322565079660 / 700 :    loss = 0.691256940365670 / 700 :    loss = 0.691306948662680 / 700 :    loss = 0.691254675388690 / 700 :    loss = 0.691228508949()
Training acc (only root node): 0.558571428571
Valiation acc (only root node): 0.5
[[ 305.   45.]
 [ 264.   86.]]
[[ 43.   7.]
 [ 43.   7.]]
annealed lr to 0.000878
epoch 8
0 / 700 :    loss = 0.68578463792810 / 700 :    loss = 0.69134593009920 / 700 :    loss = 0.6908802390130 / 700 :    loss = 0.69060927629540 / 700 :    loss = 0.69010478258150 / 700 :    loss = 0.69083017110860 / 700 :    loss = 0.69099611043970 / 700 :    loss = 0.69070786237780 / 700 :    loss = 0.69049787521490 / 700 :    loss = 0.690293550491100 / 700 :    loss = 0.690535366535110 / 700 :    loss = 0.691078066826120 / 700 :    loss = 0.691275477409130 / 700 :    loss = 0.691376566887140 / 700 :    loss = 0.691284954548150 / 700 :    loss = 0.691407382488160 / 700 :    loss = 0.691387653351170 / 700 :    loss = 0.691397488117180 / 700 :    loss = 0.691119849682190 / 700 :    loss = 0.691028475761200 / 700 :    loss = 0.690968453884210 / 700 :    loss = 0.690838158131220 / 700 :    loss = 0.691012561321230 / 700 :    loss = 0.691080749035240 / 700 :    loss = 0.691226243973250 / 700 :    loss = 0.691382825375260 / 700 :    loss = 0.691463470459270 / 700 :    loss = 0.691490471363280 / 700 :    loss = 0.691626250744290 / 700 :    loss = 0.691456913948300 / 700 :    loss = 0.691466331482310 / 700 :    loss = 0.691586971283320 / 700 :    loss = 0.691527664661330 / 700 :    loss = 0.691170573235340 / 700 :    loss = 0.691170990467350 / 700 :    loss = 0.691232979298360 / 700 :    loss = 0.691347777843370 / 700 :    loss = 0.691311001778380 / 700 :    loss = 0.691277742386390 / 700 :    loss = 0.69131320715400 / 700 :    loss = 0.691295146942410 / 700 :    loss = 0.69127869606420 / 700 :    loss = 0.691159784794430 / 700 :    loss = 0.691222250462440 / 700 :    loss = 0.691075325012450 / 700 :    loss = 0.691185235977460 / 700 :    loss = 0.691061913967470 / 700 :    loss = 0.691132545471480 / 700 :    loss = 0.691093325615490 / 700 :    loss = 0.691177666187500 / 700 :    loss = 0.691197216511510 / 700 :    loss = 0.691154360771520 / 700 :    loss = 0.691226184368530 / 700 :    loss = 0.691143393517540 / 700 :    loss = 0.691147089005550 / 700 :    loss = 0.691043615341560 / 700 :    loss = 0.69106388092570 / 700 :    loss = 0.691139876842580 / 700 :    loss = 0.691165149212590 / 700 :    loss = 0.691083192825600 / 700 :    loss = 0.691114962101610 / 700 :    loss = 0.691061377525620 / 700 :    loss = 0.691100955009630 / 700 :    loss = 0.691145062447640 / 700 :    loss = 0.691115617752650 / 700 :    loss = 0.691114127636660 / 700 :    loss = 0.691042006016670 / 700 :    loss = 0.691094815731680 / 700 :    loss = 0.691053330898690 / 700 :    loss = 0.691031396389()
Training acc (only root node): 0.565714285714
Valiation acc (only root node): 0.51
[[ 290.   60.]
 [ 244.  106.]]
[[ 42.   8.]
 [ 41.   9.]]
annealed lr to 0.000585
epoch 9
0 / 700 :    loss = 0.68377220630610 / 700 :    loss = 0.69067478179920 / 700 :    loss = 0.69038581848130 / 700 :    loss = 0.69019544124640 / 700 :    loss = 0.68973129987750 / 700 :    loss = 0.6904875636160 / 700 :    loss = 0.69070929288970 / 700 :    loss = 0.69047951698380 / 700 :    loss = 0.69024670124190 / 700 :    loss = 0.690091490746100 / 700 :    loss = 0.690308094025110 / 700 :    loss = 0.690858244896120 / 700 :    loss = 0.691062510014130 / 700 :    loss = 0.6911470294140 / 700 :    loss = 0.691055595875150 / 700 :    loss = 0.691182851791160 / 700 :    loss = 0.691188633442170 / 700 :    loss = 0.691198885441180 / 700 :    loss = 0.690936326981190 / 700 :    loss = 0.69087767601200 / 700 :    loss = 0.690804541111210 / 700 :    loss = 0.690675973892220 / 700 :    loss = 0.690824627876230 / 700 :    loss = 0.69089192152240 / 700 :    loss = 0.69104039669250 / 700 :    loss = 0.691204607487260 / 700 :    loss = 0.691277980804270 / 700 :    loss = 0.691313147545280 / 700 :    loss = 0.691440522671290 / 700 :    loss = 0.691272497177300 / 700 :    loss = 0.691280603409310 / 700 :    loss = 0.691406667233320 / 700 :    loss = 0.691345095634330 / 700 :    loss = 0.691031336784340 / 700 :    loss = 0.691025316715350 / 700 :    loss = 0.691073358059360 / 700 :    loss = 0.691175043583370 / 700 :    loss = 0.691137611866380 / 700 :    loss = 0.691107809544390 / 700 :    loss = 0.691137135029400 / 700 :    loss = 0.691118478775410 / 700 :    loss = 0.691102087498420 / 700 :    loss = 0.690982937813430 / 700 :    loss = 0.691044032574440 / 700 :    loss = 0.690906643867450 / 700 :    loss = 0.691010832787460 / 700 :    loss = 0.690890014172470 / 700 :    loss = 0.690957546234480 / 700 :    loss = 0.690919756889490 / 700 :    loss = 0.691004097462500 / 700 :    loss = 0.691023111343510 / 700 :    loss = 0.690981149673520 / 700 :    loss = 0.69105219841530 / 700 :    loss = 0.690973818302540 / 700 :    loss = 0.690985679626550 / 700 :    loss = 0.690884530544560 / 700 :    loss = 0.690894842148570 / 700 :    loss = 0.690967440605580 / 700 :    loss = 0.690991580486590 / 700 :    loss = 0.690909922123600 / 700 :    loss = 0.690940916538610 / 700 :    loss = 0.690896391869620 / 700 :    loss = 0.690942525864630 / 700 :    loss = 0.690983951092640 / 700 :    loss = 0.690961956978650 / 700 :    loss = 0.690959095955660 / 700 :    loss = 0.690882205963670 / 700 :    loss = 0.690938711166680 / 700 :    loss = 0.690908133984690 / 700 :    loss = 0.690891623497()
Training acc (only root node): 0.597142857143
Valiation acc (only root node): 0.5
[[ 279.   71.]
 [ 211.  139.]]
[[ 39.  11.]
 [ 39.  11.]]
annealed lr to 0.000390
epoch 10
0 / 700 :    loss = 0.68178236484510 / 700 :    loss = 0.69003558158920 / 700 :    loss = 0.68993645906430 / 700 :    loss = 0.68981486558940 / 700 :    loss = 0.68938982486750 / 700 :    loss = 0.69015508890260 / 700 :    loss = 0.69039267301670 / 700 :    loss = 0.69019889831580 / 700 :    loss = 0.68997901678190 / 700 :    loss = 0.689850449562100 / 700 :    loss = 0.69007062912110 / 700 :    loss = 0.690632283688120 / 700 :    loss = 0.690845727921130 / 700 :    loss = 0.690930783749140 / 700 :    loss = 0.690845668316150 / 700 :    loss = 0.690977215767160 / 700 :    loss = 0.690999388695170 / 700 :    loss = 0.691013813019180 / 700 :    loss = 0.690764307976190 / 700 :    loss = 0.69073164463200 / 700 :    loss = 0.690653085709210 / 700 :    loss = 0.690529346466220 / 700 :    loss = 0.690659165382230 / 700 :    loss = 0.69072496891240 / 700 :    loss = 0.690876066685250 / 700 :    loss = 0.691045761108260 / 700 :    loss = 0.691117286682270 / 700 :    loss = 0.691158175468280 / 700 :    loss = 0.691282451153290 / 700 :    loss = 0.69111686945300 / 700 :    loss = 0.691125273705310 / 700 :    loss = 0.691256821156320 / 700 :    loss = 0.691194415092330 / 700 :    loss = 0.690914273262340 / 700 :    loss = 0.69090628624350 / 700 :    loss = 0.690944731236360 / 700 :    loss = 0.691030502319370 / 700 :    loss = 0.690994858742380 / 700 :    loss = 0.690965533257390 / 700 :    loss = 0.690994203091400 / 700 :    loss = 0.690975248814410 / 700 :    loss = 0.690958976746420 / 700 :    loss = 0.690841853619430 / 700 :    loss = 0.690901696682440 / 700 :    loss = 0.690774083138450 / 700 :    loss = 0.690874397755460 / 700 :    loss = 0.690756738186470 / 700 :    loss = 0.690819978714480 / 700 :    loss = 0.690783023834490 / 700 :    loss = 0.690867602825500 / 700 :    loss = 0.690886795521510 / 700 :    loss = 0.690846502781520 / 700 :    loss = 0.690916895866530 / 700 :    loss = 0.690843939781540 / 700 :    loss = 0.690863132477550 / 700 :    loss = 0.690765023232560 / 700 :    loss = 0.690767526627570 / 700 :    loss = 0.690836310387580 / 700 :    loss = 0.690859138966590 / 700 :    loss = 0.690779685974600 / 700 :    loss = 0.690809309483610 / 700 :    loss = 0.690767943859620 / 700 :    loss = 0.690818190575630 / 700 :    loss = 0.690858781338640 / 700 :    loss = 0.690842092037650 / 700 :    loss = 0.6908390522660 / 700 :    loss = 0.690759241581670 / 700 :    loss = 0.69081902504680 / 700 :    loss = 0.690797448158690 / 700 :    loss = 0.690786361694()
Training acc (only root node): 0.598571428571
Valiation acc (only root node): 0.53
[[ 263.   87.]
 [ 194.  156.]]
[[ 38.  12.]
 [ 35.  15.]]
annealed lr to 0.000260
epoch 11
0 / 700 :    loss = 0.68018263578410 / 700 :    loss = 0.68953984975820 / 700 :    loss = 0.68959796428730 / 700 :    loss = 0.68952894210840 / 700 :    loss = 0.68913412094150 / 700 :    loss = 0.68989545106960 / 700 :    loss = 0.69012480974270 / 700 :    loss = 0.68994772434280 / 700 :    loss = 0.68975389003890 / 700 :    loss = 0.689632773399100 / 700 :    loss = 0.689868867397110 / 700 :    loss = 0.690440714359120 / 700 :    loss = 0.690659821033130 / 700 :    loss = 0.690756201744140 / 700 :    loss = 0.690680325031150 / 700 :    loss = 0.690812110901160 / 700 :    loss = 0.690838575363170 / 700 :    loss = 0.690857887268180 / 700 :    loss = 0.690616130829190 / 700 :    loss = 0.690599381924200 / 700 :    loss = 0.690520226955210 / 700 :    loss = 0.690401315689220 / 700 :    loss = 0.690522372723230 / 700 :    loss = 0.690589427948240 / 700 :    loss = 0.690743744373250 / 700 :    loss = 0.690919041634260 / 700 :    loss = 0.690989375114270 / 700 :    loss = 0.69103628397280 / 700 :    loss = 0.691157102585290 / 700 :    loss = 0.690992951393300 / 700 :    loss = 0.691002428532310 / 700 :    loss = 0.691137850285320 / 700 :    loss = 0.691075742245330 / 700 :    loss = 0.690818011761340 / 700 :    loss = 0.690810143948350 / 700 :    loss = 0.690842926502360 / 700 :    loss = 0.690916538239370 / 700 :    loss = 0.690882921219380 / 700 :    loss = 0.690853714943390 / 700 :    loss = 0.690883159637400 / 700 :    loss = 0.690864384174410 / 700 :    loss = 0.69084829092420 / 700 :    loss = 0.690733611584430 / 700 :    loss = 0.690792500973440 / 700 :    loss = 0.690673351288450 / 700 :    loss = 0.690771043301460 / 700 :    loss = 0.690656125546470 / 700 :    loss = 0.690715670586480 / 700 :    loss = 0.690679311752490 / 700 :    loss = 0.690764069557500 / 700 :    loss = 0.690783977509510 / 700 :    loss = 0.690745413303520 / 700 :    loss = 0.690815210342530 / 700 :    loss = 0.690747857094540 / 700 :    loss = 0.690773546696550 / 700 :    loss = 0.690678298473560 / 700 :    loss = 0.690674483776570 / 700 :    loss = 0.690739691257580 / 700 :    loss = 0.690761208534590 / 700 :    loss = 0.690684616566600 / 700 :    loss = 0.690712511539610 / 700 :    loss = 0.690670907497620 / 700 :    loss = 0.690723240376630 / 700 :    loss = 0.690763890743640 / 700 :    loss = 0.69075012207650 / 700 :    loss = 0.690747320652660 / 700 :    loss = 0.69066619873670 / 700 :    loss = 0.69072830677680 / 700 :    loss = 0.690712809563690 / 700 :    loss = 0.690706074238()
Training acc (only root node): 0.601428571429
Valiation acc (only root node): 0.56
[[ 249.  101.]
 [ 178.  172.]]
[[ 38.  12.]
 [ 32.  18.]]
annealed lr to 0.000173
epoch 12
0 / 700 :    loss = 0.6790708899510 / 700 :    loss = 0.68920630216620 / 700 :    loss = 0.68937522172930 / 700 :    loss = 0.68934106826840 / 700 :    loss = 0.68896704912250 / 700 :    loss = 0.68972045183260 / 700 :    loss = 0.68993633985570 / 700 :    loss = 0.68976622819980 / 700 :    loss = 0.68959516286890 / 700 :    loss = 0.689473390579100 / 700 :    loss = 0.689725220203110 / 700 :    loss = 0.690303862095120 / 700 :    loss = 0.690525949001130 / 700 :    loss = 0.690634846687140 / 700 :    loss = 0.690567314625150 / 700 :    loss = 0.690697073936160 / 700 :    loss = 0.690721869469170 / 700 :    loss = 0.690744876862180 / 700 :    loss = 0.690506756306190 / 700 :    loss = 0.690498173237200 / 700 :    loss = 0.690420150757210 / 700 :    loss = 0.690304875374220 / 700 :    loss = 0.690422773361230 / 700 :    loss = 0.690492331982240 / 700 :    loss = 0.690649569035250 / 700 :    loss = 0.690830051899260 / 700 :    loss = 0.69089871645270 / 700 :    loss = 0.690951287746280 / 700 :    loss = 0.691067695618290 / 700 :    loss = 0.690904140472300 / 700 :    loss = 0.69091463089310 / 700 :    loss = 0.691052019596320 / 700 :    loss = 0.690990626812330 / 700 :    loss = 0.690745770931340 / 700 :    loss = 0.690738677979350 / 700 :    loss = 0.690768480301360 / 700 :    loss = 0.690834820271370 / 700 :    loss = 0.690802633762380 / 700 :    loss = 0.690773785114390 / 700 :    loss = 0.690803706646400 / 700 :    loss = 0.690785169601410 / 700 :    loss = 0.690769314766420 / 700 :    loss = 0.690656423569430 / 700 :    loss = 0.690714895725440 / 700 :    loss = 0.690601706505450 / 700 :    loss = 0.690697789192460 / 700 :    loss = 0.690585017204470 / 700 :    loss = 0.69064193964480 / 700 :    loss = 0.69060587883490 / 700 :    loss = 0.690690934658500 / 700 :    loss = 0.690711319447510 / 700 :    loss = 0.690674304962520 / 700 :    loss = 0.690743625164530 / 700 :    loss = 0.690680623055540 / 700 :    loss = 0.690711379051550 / 700 :    loss = 0.690618634224560 / 700 :    loss = 0.690610170364570 / 700 :    loss = 0.690672457218580 / 700 :    loss = 0.690692842007590 / 700 :    loss = 0.690618872643600 / 700 :    loss = 0.690645217896610 / 700 :    loss = 0.690601885319620 / 700 :    loss = 0.690655231476630 / 700 :    loss = 0.690696060658640 / 700 :    loss = 0.690683841705650 / 700 :    loss = 0.690681278706660 / 700 :    loss = 0.690599560738670 / 700 :    loss = 0.690663158894680 / 700 :    loss = 0.690651416779690 / 700 :    loss = 0.690647661686()
Training acc (only root node): 0.611428571429
Valiation acc (only root node): 0.56
[[ 238.  112.]
 [ 160.  190.]]
[[ 36.  14.]
 [ 30.  20.]]
annealed lr to 0.000116
epoch 13
0 / 700 :    loss = 0.67834794521310 / 700 :    loss = 0.68899464607220 / 700 :    loss = 0.68923580646530 / 700 :    loss = 0.68922382593240 / 700 :    loss = 0.68886327743550 / 700 :    loss = 0.68960928916960 / 700 :    loss = 0.68981355428770 / 700 :    loss = 0.6896460056380 / 700 :    loss = 0.68949151039190 / 700 :    loss = 0.689367175102100 / 700 :    loss = 0.689630866051110 / 700 :    loss = 0.690213918686120 / 700 :    loss = 0.690437316895130 / 700 :    loss = 0.690556049347140 / 700 :    loss = 0.690494596958150 / 700 :    loss = 0.690622091293160 / 700 :    loss = 0.690643787384170 / 700 :    loss = 0.690669178963180 / 700 :    loss = 0.690432548523190 / 700 :    loss = 0.690428197384200 / 700 :    loss = 0.690351366997210 / 700 :    loss = 0.690238416195220 / 700 :    loss = 0.690355420113230 / 700 :    loss = 0.690427303314240 / 700 :    loss = 0.690586686134250 / 700 :    loss = 0.690771222115260 / 700 :    loss = 0.69083827734270 / 700 :    loss = 0.690895438194280 / 700 :    loss = 0.69100767374290 / 700 :    loss = 0.690844237804300 / 700 :    loss = 0.690855383873310 / 700 :    loss = 0.690993726254320 / 700 :    loss = 0.690932869911330 / 700 :    loss = 0.690695285797340 / 700 :    loss = 0.690688848495350 / 700 :    loss = 0.690716981888360 / 700 :    loss = 0.690779328346370 / 700 :    loss = 0.690748035908380 / 700 :    loss = 0.690719544888390 / 700 :    loss = 0.690749704838400 / 700 :    loss = 0.690731346607410 / 700 :    loss = 0.6907158494420 / 700 :    loss = 0.690603971481430 / 700 :    loss = 0.690662205219440 / 700 :    loss = 0.690552711487450 / 700 :    loss = 0.690647900105460 / 700 :    loss = 0.690536618233470 / 700 :    loss = 0.690591812134480 / 700 :    loss = 0.690556049347490 / 700 :    loss = 0.690641283989500 / 700 :    loss = 0.690662026405510 / 700 :    loss = 0.690626144409520 / 700 :    loss = 0.690695166588530 / 700 :    loss = 0.690635204315540 / 700 :    loss = 0.690669775009550 / 700 :    loss = 0.690578758717560 / 700 :    loss = 0.690567076206570 / 700 :    loss = 0.690627098083580 / 700 :    loss = 0.690646648407590 / 700 :    loss = 0.690574884415600 / 700 :    loss = 0.69059997797610 / 700 :    loss = 0.690554976463620 / 700 :    loss = 0.690608561039630 / 700 :    loss = 0.690649628639640 / 700 :    loss = 0.690638005733650 / 700 :    loss = 0.690635740757660 / 700 :    loss = 0.690553843975670 / 700 :    loss = 0.690618157387680 / 700 :    loss = 0.690608859062690 / 700 :    loss = 0.690607070923()
Training acc (only root node): 0.611428571429
Valiation acc (only root node): 0.56
[[ 229.  121.]
 [ 151.  199.]]
[[ 36.  14.]
 [ 30.  20.]]
annealed lr to 0.000077
epoch 14
0 / 700 :    loss = 0.67788696289110 / 700 :    loss = 0.68886196613320 / 700 :    loss = 0.68914902210230 / 700 :    loss = 0.68915086984640 / 700 :    loss = 0.68879902362850 / 700 :    loss = 0.68953943252660 / 700 :    loss = 0.68973529338870 / 700 :    loss = 0.68956881761680 / 700 :    loss = 0.68942523002690 / 700 :    loss = 0.689298510551100 / 700 :    loss = 0.689570128918110 / 700 :    loss = 0.69015622139120 / 700 :    loss = 0.690379917622130 / 700 :    loss = 0.690505743027140 / 700 :    loss = 0.690448582172150 / 700 :    loss = 0.69057404995160 / 700 :    loss = 0.690592885017170 / 700 :    loss = 0.690619766712180 / 700 :    loss = 0.690383851528190 / 700 :    loss = 0.690381646156200 / 700 :    loss = 0.690305769444210 / 700 :    loss = 0.690194249153220 / 700 :    loss = 0.690310955048230 / 700 :    loss = 0.690384626389240 / 700 :    loss = 0.690545678139250 / 700 :    loss = 0.690733015537260 / 700 :    loss = 0.69079887867270 / 700 :    loss = 0.690859496593280 / 700 :    loss = 0.690968334675290 / 700 :    loss = 0.690804719925300 / 700 :    loss = 0.690816283226310 / 700 :    loss = 0.690954983234320 / 700 :    loss = 0.690894782543330 / 700 :    loss = 0.690661132336340 / 700 :    loss = 0.69065529108350 / 700 :    loss = 0.690682470798360 / 700 :    loss = 0.69074255228370 / 700 :    loss = 0.690711677074380 / 700 :    loss = 0.690683782101390 / 700 :    loss = 0.690713763237400 / 700 :    loss = 0.69069558382410 / 700 :    loss = 0.690680265427420 / 700 :    loss = 0.690568983555430 / 700 :    loss = 0.690627098083440 / 700 :    loss = 0.690520048141450 / 700 :    loss = 0.690614700317460 / 700 :    loss = 0.690504252911470 / 700 :    loss = 0.690558373928480 / 700 :    loss = 0.69052284956490 / 700 :    loss = 0.690608084202