I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.7.5 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
NVIDIA: no NVIDIA devices found
E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_UNKNOWN
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:145] kernel driver does not appear to be running on this host (ip-172-30-1-62): /proc/driver/nvidia/version does not exist
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcupti.so.7.5 locally
Loading train trees..
Loading dev trees..
Loading test trees..
1852 1510
5058.0 total words with 1568 uniques
epoch 0
0 / 700 :    loss = 0.68096482753810 / 700 :    loss = 0.68587851524420 / 700 :    loss = 0.68545120954530 / 700 :    loss = 0.68809592723840 / 700 :    loss = 0.6889001727150 / 700 :    loss = 0.68850159645160 / 700 :    loss = 0.68746960163170 / 700 :    loss = 0.68747055530580 / 700 :    loss = 0.68789750337690 / 700 :    loss = 0.688107013702100 / 700 :    loss = 0.688774943352110 / 700 :    loss = 0.689524114132120 / 700 :    loss = 0.68926602602130 / 700 :    loss = 0.689357638359140 / 700 :    loss = 0.689786374569150 / 700 :    loss = 0.689692139626160 / 700 :    loss = 0.689332842827170 / 700 :    loss = 0.689776837826180 / 700 :    loss = 0.689566791058190 / 700 :    loss = 0.689637601376200 / 700 :    loss = 0.689609646797210 / 700 :    loss = 0.689617156982220 / 700 :    loss = 0.689650893211230 / 700 :    loss = 0.689563214779240 / 700 :    loss = 0.689608931541250 / 700 :    loss = 0.689655780792260 / 700 :    loss = 0.689869403839270 / 700 :    loss = 0.68989700079280 / 700 :    loss = 0.689554810524290 / 700 :    loss = 0.689564585686300 / 700 :    loss = 0.68952280283310 / 700 :    loss = 0.689536392689320 / 700 :    loss = 0.689690649509330 / 700 :    loss = 0.688769817352340 / 700 :    loss = 0.688970029354350 / 700 :    loss = 0.689005732536360 / 700 :    loss = 0.688688516617370 / 700 :    loss = 0.688798427582380 / 700 :    loss = 0.688736319542390 / 700 :    loss = 0.688853681087400 / 700 :    loss = 0.688855111599410 / 700 :    loss = 0.688999176025420 / 700 :    loss = 0.688912093639430 / 700 :    loss = 0.688933670521440 / 700 :    loss = 0.688737154007450 / 700 :    loss = 0.688939392567460 / 700 :    loss = 0.688945114613470 / 700 :    loss = 0.689058125019480 / 700 :    loss = 0.68910241127490 / 700 :    loss = 0.689173817635500 / 700 :    loss = 0.689235210419510 / 700 :    loss = 0.689294457436520 / 700 :    loss = 0.689357757568530 / 700 :    loss = 0.68917119503540 / 700 :    loss = 0.689127087593550 / 700 :    loss = 0.689134299755560 / 700 :    loss = 0.689166903496570 / 700 :    loss = 0.689114153385580 / 700 :    loss = 0.688972175121590 / 700 :    loss = 0.688897848129600 / 700 :    loss = 0.688963115215610 / 700 :    loss = 0.688854932785620 / 700 :    loss = 0.688930749893630 / 700 :    loss = 0.688987851143640 / 700 :    loss = 0.688980102539650 / 700 :    loss = 0.689011096954660 / 700 :    loss = 0.68902015686670 / 700 :    loss = 0.689039051533680 / 700 :    loss = 0.688980102539690 / 700 :    loss = 0.688988804817()
Training acc (only root node): 0.57
Valiation acc (only root node): 0.54
[[ 299.   51.]
 [ 250.  100.]]
[[ 45.   5.]
 [ 41.   9.]]
epoch 1
0 / 700 :    loss = 0.69174993038210 / 700 :    loss = 0.68618428707120 / 700 :    loss = 0.68524640798630 / 700 :    loss = 0.68790507316640 / 700 :    loss = 0.6884388327650 / 700 :    loss = 0.68798643350660 / 700 :    loss = 0.68693786859570 / 700 :    loss = 0.68694472312980 / 700 :    loss = 0.68732529878690 / 700 :    loss = 0.687495529652100 / 700 :    loss = 0.688129901886110 / 700 :    loss = 0.688850224018120 / 700 :    loss = 0.688541769981130 / 700 :    loss = 0.688606500626140 / 700 :    loss = 0.689019501209150 / 700 :    loss = 0.688920199871160 / 700 :    loss = 0.688549757004170 / 700 :    loss = 0.688993155956180 / 700 :    loss = 0.688767910004190 / 700 :    loss = 0.688845813274200 / 700 :    loss = 0.688806951046210 / 700 :    loss = 0.688816308975220 / 700 :    loss = 0.688846349716230 / 700 :    loss = 0.688781201839240 / 700 :    loss = 0.688827812672250 / 700 :    loss = 0.688876330853260 / 700 :    loss = 0.689100384712270 / 700 :    loss = 0.689121723175280 / 700 :    loss = 0.688774466515290 / 700 :    loss = 0.688781440258300 / 700 :    loss = 0.688739001751310 / 700 :    loss = 0.6887575984320 / 700 :    loss = 0.688909888268330 / 700 :    loss = 0.687992334366340 / 700 :    loss = 0.688190281391350 / 700 :    loss = 0.688229560852360 / 700 :    loss = 0.687902152538370 / 700 :    loss = 0.688021540642380 / 700 :    loss = 0.687964260578390 / 700 :    loss = 0.688082337379400 / 700 :    loss = 0.688062429428410 / 700 :    loss = 0.688210606575420 / 700 :    loss = 0.688116371632430 / 700 :    loss = 0.688136398792440 / 700 :    loss = 0.687939882278450 / 700 :    loss = 0.688144743443460 / 700 :    loss = 0.68815100193470 / 700 :    loss = 0.688265323639480 / 700 :    loss = 0.688312768936490 / 700 :    loss = 0.688380300999500 / 700 :    loss = 0.688447952271510 / 700 :    loss = 0.688511788845520 / 700 :    loss = 0.688577115536530 / 700 :    loss = 0.688384473324540 / 700 :    loss = 0.688347518444550 / 700 :    loss = 0.688354194164560 / 700 :    loss = 0.688385248184570 / 700 :    loss = 0.688333451748580 / 700 :    loss = 0.688174843788590 / 700 :    loss = 0.688096821308600 / 700 :    loss = 0.68815779686610 / 700 :    loss = 0.688045680523620 / 700 :    loss = 0.688121259212630 / 700 :    loss = 0.688182175159640 / 700 :    loss = 0.688169121742650 / 700 :    loss = 0.688196957111660 / 700 :    loss = 0.688201606274670 / 700 :    loss = 0.68822312355680 / 700 :    loss = 0.688161075115690 / 700 :    loss = 0.688172161579()
Training acc (only root node): 0.585714285714
Valiation acc (only root node): 0.54
[[ 297.   53.]
 [ 237.  113.]]
[[ 44.   6.]
 [ 40.  10.]]
annealed lr to 0.006667
epoch 2
0 / 700 :    loss = 0.69046646356610 / 700 :    loss = 0.68482893705420 / 700 :    loss = 0.68369507789630 / 700 :    loss = 0.68643891811440 / 700 :    loss = 0.6868438720750 / 700 :    loss = 0.68637615442360 / 700 :    loss = 0.68547397851970 / 700 :    loss = 0.68548995256480 / 700 :    loss = 0.68587100505890 / 700 :    loss = 0.685994565487100 / 700 :    loss = 0.686632573605110 / 700 :    loss = 0.687336266041120 / 700 :    loss = 0.687016069889130 / 700 :    loss = 0.687090933323140 / 700 :    loss = 0.687439918518150 / 700 :    loss = 0.687356710434160 / 700 :    loss = 0.687072277069170 / 700 :    loss = 0.687524557114180 / 700 :    loss = 0.687245249748190 / 700 :    loss = 0.687336206436200 / 700 :    loss = 0.687285602093210 / 700 :    loss = 0.687295913696220 / 700 :    loss = 0.687401294708230 / 700 :    loss = 0.687370955944240 / 700 :    loss = 0.687418341637250 / 700 :    loss = 0.687456607819260 / 700 :    loss = 0.687731385231270 / 700 :    loss = 0.687712609768280 / 700 :    loss = 0.68742364645290 / 700 :    loss = 0.687397778034300 / 700 :    loss = 0.687347769737310 / 700 :    loss = 0.687381565571320 / 700 :    loss = 0.687513411045330 / 700 :    loss = 0.68669462204340 / 700 :    loss = 0.686844348907350 / 700 :    loss = 0.686917185783360 / 700 :    loss = 0.686692237854370 / 700 :    loss = 0.686784088612380 / 700 :    loss = 0.686724245548390 / 700 :    loss = 0.686857938766400 / 700 :    loss = 0.68680036068410 / 700 :    loss = 0.686941623688420 / 700 :    loss = 0.686828315258430 / 700 :    loss = 0.68683797121440 / 700 :    loss = 0.686685085297450 / 700 :    loss = 0.686894953251460 / 700 :    loss = 0.686890184879470 / 700 :    loss = 0.687032341957480 / 700 :    loss = 0.687060296535490 / 700 :    loss = 0.687108039856500 / 700 :    loss = 0.687193214893510 / 700 :    loss = 0.687257289886520 / 700 :    loss = 0.687317609787530 / 700 :    loss = 0.68713414669540 / 700 :    loss = 0.687099575996550 / 700 :    loss = 0.687101721764560 / 700 :    loss = 0.687152326107570 / 700 :    loss = 0.687101721764580 / 700 :    loss = 0.686923980713590 / 700 :    loss = 0.686843693256600 / 700 :    loss = 0.686888813972610 / 700 :    loss = 0.686806440353620 / 700 :    loss = 0.686864852905630 / 700 :    loss = 0.686936318874640 / 700 :    loss = 0.686901688576650 / 700 :    loss = 0.686932682991660 / 700 :    loss = 0.686933159828670 / 700 :    loss = 0.686956167221680 / 700 :    loss = 0.686904609203690 / 700 :    loss = 0.686911642551()
Training acc (only root node): 0.597142857143
Valiation acc (only root node): 0.53
[[ 299.   51.]
 [ 231.  119.]]
[[ 43.   7.]
 [ 40.  10.]]
annealed lr to 0.004444
epoch 3
0 / 700 :    loss = 0.6891242265710 / 700 :    loss = 0.68394249677720 / 700 :    loss = 0.68259131908430 / 700 :    loss = 0.68536782264740 / 700 :    loss = 0.68576210737250 / 700 :    loss = 0.68529444932960 / 700 :    loss = 0.68450307846170 / 700 :    loss = 0.6845225691880 / 700 :    loss = 0.68487250804990 / 700 :    loss = 0.684968113899100 / 700 :    loss = 0.685603141785110 / 700 :    loss = 0.686296463013120 / 700 :    loss = 0.685958206654130 / 700 :    loss = 0.686057388783140 / 700 :    loss = 0.686373531818150 / 700 :    loss = 0.686287522316160 / 700 :    loss = 0.686064898968170 / 700 :    loss = 0.686512708664180 / 700 :    loss = 0.68620967865190 / 700 :    loss = 0.68631541729200 / 700 :    loss = 0.686247110367210 / 700 :    loss = 0.686258912086220 / 700 :    loss = 0.686439752579230 / 700 :    loss = 0.686447679996240 / 700 :    loss = 0.686487019062250 / 700 :    loss = 0.68652099371260 / 700 :    loss = 0.686829388142270 / 700 :    loss = 0.686776518822280 / 700 :    loss = 0.686540842056290 / 700 :    loss = 0.686492264271300 / 700 :    loss = 0.686429262161310 / 700 :    loss = 0.686470866203320 / 700 :    loss = 0.686585307121330 / 700 :    loss = 0.685850083828340 / 700 :    loss = 0.685953080654350 / 700 :    loss = 0.686048030853360 / 700 :    loss = 0.685939729214370 / 700 :    loss = 0.685982584953380 / 700 :    loss = 0.68592685461390 / 700 :    loss = 0.686057031155400 / 700 :    loss = 0.685974121094410 / 700 :    loss = 0.686105072498420 / 700 :    loss = 0.685983717442430 / 700 :    loss = 0.685984373093440 / 700 :    loss = 0.685866057873450 / 700 :    loss = 0.686064481735460 / 700 :    loss = 0.686051905155470 / 700 :    loss = 0.686217486858480 / 700 :    loss = 0.68623316288490 / 700 :    loss = 0.686266541481500 / 700 :    loss = 0.686358690262510 / 700 :    loss = 0.686426103115520 / 700 :    loss = 0.686481297016530 / 700 :    loss = 0.686310231686540 / 700 :    loss = 0.686286151409550 / 700 :    loss = 0.686279714108560 / 700 :    loss = 0.68634223938570 / 700 :    loss = 0.68629783392580 / 700 :    loss = 0.686107933521590 / 700 :    loss = 0.686018824577600 / 700 :    loss = 0.686055064201610 / 700 :    loss = 0.685997903347620 / 700 :    loss = 0.6860460639630 / 700 :    loss = 0.686118066311640 / 700 :    loss = 0.686071336269650 / 700 :    loss = 0.68610483408660 / 700 :    loss = 0.686105012894670 / 700 :    loss = 0.686125457287680 / 700 :    loss = 0.686081588268690 / 700 :    loss = 0.686087369919()
Training acc (only root node): 0.597142857143
Valiation acc (only root node): 0.53
[[ 299.   51.]
 [ 231.  119.]]
[[ 43.   7.]
 [ 40.  10.]]
annealed lr to 0.002963
epoch 4
0 / 700 :    loss = 0.68826478719710 / 700 :    loss = 0.68342298269320 / 700 :    loss = 0.68183284997930 / 700 :    loss = 0.68463337421440 / 700 :    loss = 0.6850301027350 / 700 :    loss = 0.68457150459360 / 700 :    loss = 0.68389171361970 / 700 :    loss = 0.68391728401280 / 700 :    loss = 0.68418848514690 / 700 :    loss = 0.684278905392100 / 700 :    loss = 0.684899568558110 / 700 :    loss = 0.685586988926120 / 700 :    loss = 0.685238420963130 / 700 :    loss = 0.685363590717140 / 700 :    loss = 0.685666322708150 / 700 :    loss = 0.685562252998160 / 700 :    loss = 0.685375809669170 / 700 :    loss = 0.685818791389180 / 700 :    loss = 0.685505330563190 / 700 :    loss = 0.685627937317200 / 700 :    loss = 0.685536324978210 / 700 :    loss = 0.68555021286220 / 700 :    loss = 0.685786545277230 / 700 :    loss = 0.685836851597240 / 700 :    loss = 0.685870945454250 / 700 :    loss = 0.685909211636260 / 700 :    loss = 0.686220109463270 / 700 :    loss = 0.686148643494280 / 700 :    loss = 0.685954034328290 / 700 :    loss = 0.685896933079300 / 700 :    loss = 0.685818612576310 / 700 :    loss = 0.685866951942320 / 700 :    loss = 0.685966491699330 / 700 :    loss = 0.685306668282340 / 700 :    loss = 0.685368835926350 / 700 :    loss = 0.685466885567360 / 700 :    loss = 0.685449302197370 / 700 :    loss = 0.685448765755380 / 700 :    loss = 0.685406625271390 / 700 :    loss = 0.685516655445400 / 700 :    loss = 0.685418128967410 / 700 :    loss = 0.685540616512420 / 700 :    loss = 0.685414731503430 / 700 :    loss = 0.685408473015440 / 700 :    loss = 0.685317873955450 / 700 :    loss = 0.685500979424460 / 700 :    loss = 0.685483694077470 / 700 :    loss = 0.685662090778480 / 700 :    loss = 0.685668170452490 / 700 :    loss = 0.685692727566500 / 700 :    loss = 0.685786902905510 / 700 :    loss = 0.68585640192520 / 700 :    loss = 0.685908079147530 / 700 :    loss = 0.685750067234540 / 700 :    loss = 0.685741961002550 / 700 :    loss = 0.685728311539560 / 700 :    loss = 0.685789585114570 / 700 :    loss = 0.685752093792580 / 700 :    loss = 0.685555756092590 / 700 :    loss = 0.685456693172600 / 700 :    loss = 0.685488402843610 / 700 :    loss = 0.685453951359620 / 700 :    loss = 0.685498356819630 / 700 :    loss = 0.685565173626640 / 700 :    loss = 0.685513615608650 / 700 :    loss = 0.685546159744660 / 700 :    loss = 0.685546576977670 / 700 :    loss = 0.685564219952680 / 700 :    loss = 0.685523867607690 / 700 :    loss = 0.685531139374()
Training acc (only root node): 0.611428571429
Valiation acc (only root node): 0.52
[[ 295.   55.]
 [ 217.  133.]]
[[ 42.   8.]
 [ 40.  10.]]
annealed lr to 0.001975
epoch 5
0 / 700 :    loss = 0.68703734874710 / 700 :    loss = 0.68297952413620 / 700 :    loss = 0.68128722906130 / 700 :    loss = 0.68410652875940 / 700 :    loss = 0.68453294038850 / 700 :    loss = 0.68410110473660 / 700 :    loss = 0.68353730440170 / 700 :    loss = 0.6835828423580 / 700 :    loss = 0.68374127149690 / 700 :    loss = 0.683854997158100 / 700 :    loss = 0.68443363905110 / 700 :    loss = 0.685116887093120 / 700 :    loss = 0.68475908041130 / 700 :    loss = 0.684898078442140 / 700 :    loss = 0.685196042061150 / 700 :    loss = 0.685075104237160 / 700 :    loss = 0.684906005859170 / 700 :    loss = 0.685348749161180 / 700 :    loss = 0.685032784939190 / 700 :    loss = 0.685170769691200 / 700 :    loss = 0.68506270647210 / 700 :    loss = 0.685074627399220 / 700 :    loss = 0.685340046883230 / 700 :    loss = 0.685426652431240 / 700 :    loss = 0.685465157032250 / 700 :    loss = 0.685513198376260 / 700 :    loss = 0.6858035326270 / 700 :    loss = 0.68572884798280 / 700 :    loss = 0.685553431511290 / 700 :    loss = 0.685492575169300 / 700 :    loss = 0.68540418148310 / 700 :    loss = 0.685464918613320 / 700 :    loss = 0.685551941395330 / 700 :    loss = 0.684964179993340 / 700 :    loss = 0.684993863106350 / 700 :    loss = 0.685079813004360 / 700 :    loss = 0.685110807419370 / 700 :    loss = 0.685083448887380 / 700 :    loss = 0.685058832169390 / 700 :    loss = 0.685144424438400 / 700 :    loss = 0.685034751892410 / 700 :    loss = 0.685153365135420 / 700 :    loss = 0.68502175808430 / 700 :    loss = 0.685011506081440 / 700 :    loss = 0.684940457344450 / 700 :    loss = 0.685111045837460 / 700 :    loss = 0.685091614723470 / 700 :    loss = 0.685274362564480 / 700 :    loss = 0.685276508331490 / 700 :    loss = 0.685296416283500 / 700 :    loss = 0.685391187668510 / 700 :    loss = 0.685461223125520 / 700 :    loss = 0.68551158905530 / 700 :    loss = 0.685362815857540 / 700 :    loss = 0.685371398926550 / 700 :    loss = 0.685354113579560 / 700 :    loss = 0.685405373573570 / 700 :    loss = 0.68537157774580 / 700 :    loss = 0.685171961784590 / 700 :    loss = 0.685065567493600 / 700 :    loss = 0.685095071793610 / 700 :    loss = 0.685079455376620 / 700 :    loss = 0.685126364231630 / 700 :    loss = 0.685186386108640 / 700 :    loss = 0.685136079788650 / 700 :    loss = 0.685163676739660 / 700 :    loss = 0.685161650181670 / 700 :    loss = 0.685178160667680 / 700 :    loss = 0.685141742229690 / 700 :    loss = 0.68515175581()
Training acc (only root node): 0.618571428571
Valiation acc (only root node): 0.53
[[ 287.   63.]
 [ 204.  146.]]
[[ 42.   8.]
 [ 39.  11.]]
annealed lr to 0.001317
epoch 6
0 / 700 :    loss = 0.68555694818510 / 700 :    loss = 0.68252712488220 / 700 :    loss = 0.68082201480930 / 700 :    loss = 0.6836708784140 / 700 :    loss = 0.68414407968550 / 700 :    loss = 0.68374961614660 / 700 :    loss = 0.68329280614970 / 700 :    loss = 0.68337345123380 / 700 :    loss = 0.68342798948390 / 700 :    loss = 0.683584928513100 / 700 :    loss = 0.68411052227110 / 700 :    loss = 0.684796094894120 / 700 :    loss = 0.684435665607130 / 700 :    loss = 0.684569597244140 / 700 :    loss = 0.684866845608150 / 700 :    loss = 0.684738337994160 / 700 :    loss = 0.684583604336170 / 700 :    loss = 0.685027182102180 / 700 :    loss = 0.684713065624190 / 700 :    loss = 0.684866726398200 / 700 :    loss = 0.684745371342210 / 700 :    loss = 0.684755861759220 / 700 :    loss = 0.685027480125230 / 700 :    loss = 0.68513906002240 / 700 :    loss = 0.685185730457250 / 700 :    loss = 0.685247302055260 / 700 :    loss = 0.685513496399270 / 700 :    loss = 0.685442566872280 / 700 :    loss = 0.685269832611290 / 700 :    loss = 0.685206055641300 / 700 :    loss = 0.685112655163310 / 700 :    loss = 0.685183942318320 / 700 :    loss = 0.6852632761330 / 700 :    loss = 0.684736192226340 / 700 :    loss = 0.684744894505350 / 700 :    loss = 0.684814810753360 / 700 :    loss = 0.684858202934370 / 700 :    loss = 0.684818387032380 / 700 :    loss = 0.684807956219390 / 700 :    loss = 0.684875667095400 / 700 :    loss = 0.684758901596410 / 700 :    loss = 0.684875905514420 / 700 :    loss = 0.684739589691430 / 700 :    loss = 0.684727489948440 / 700 :    loss = 0.684668719769450 / 700 :    loss = 0.684831082821460 / 700 :    loss = 0.684811472893470 / 700 :    loss = 0.684994339943480 / 700 :    loss = 0.684993982315490 / 700 :    loss = 0.685011684895500 / 700 :    loss = 0.685106933117510 / 700 :    loss = 0.685176610947520 / 700 :    loss = 0.685227572918530 / 700 :    loss = 0.685083806515540 / 700 :    loss = 0.68510645628550 / 700 :    loss = 0.68508887291560 / 700 :    loss = 0.685128867626570 / 700 :    loss = 0.685095667839580 / 700 :    loss = 0.68489408493590 / 700 :    loss = 0.684784054756600 / 700 :    loss = 0.684811711311610 / 700 :    loss = 0.684810161591620 / 700 :    loss = 0.684861600399630 / 700 :    loss = 0.684916615486640 / 700 :    loss = 0.684871315956650 / 700 :    loss = 0.684892654419660 / 700 :    loss = 0.684886574745670 / 700 :    loss = 0.684903621674680 / 700 :    loss = 0.68487393856690 / 700 :    loss = 0.684887886047()
Training acc (only root node): 0.632857142857
Valiation acc (only root node): 0.54
[[ 280.   70.]
 [ 187.  163.]]
[[ 41.   9.]
 [ 37.  13.]]
annealed lr to 0.000878
epoch 7
0 / 700 :    loss = 0.68380570411710 / 700 :    loss = 0.68203186988820 / 700 :    loss = 0.68037843704230 / 700 :    loss = 0.68328070640640 / 700 :    loss = 0.68379604816450 / 700 :    loss = 0.68343329429660 / 700 :    loss = 0.68305367231470 / 700 :    loss = 0.68317610025480 / 700 :    loss = 0.68316775560490 / 700 :    loss = 0.683373689651100 / 700 :    loss = 0.683859169483110 / 700 :    loss = 0.68455183506120 / 700 :    loss = 0.684193491936130 / 700 :    loss = 0.684314072132140 / 700 :    loss = 0.6846113801150 / 700 :    loss = 0.684483170509160 / 700 :    loss = 0.684346735477170 / 700 :    loss = 0.684791505337180 / 700 :    loss = 0.684483885765190 / 700 :    loss = 0.684654593468200 / 700 :    loss = 0.684524476528210 / 700 :    loss = 0.684534430504220 / 700 :    loss = 0.684796750546230 / 700 :    loss = 0.684921622276240 / 700 :    loss = 0.684975624084250 / 700 :    loss = 0.685049414635260 / 700 :    loss = 0.685298264027270 / 700 :    loss = 0.685232937336280 / 700 :    loss = 0.685057461262290 / 700 :    loss = 0.684992134571300 / 700 :    loss = 0.684897184372310 / 700 :    loss = 0.684976637363320 / 700 :    loss = 0.685051679611330 / 700 :    loss = 0.684571802616340 / 700 :    loss = 0.684569478035350 / 700 :    loss = 0.684625983238360 / 700 :    loss = 0.68466258049370 / 700 :    loss = 0.684618413448380 / 700 :    loss = 0.684616446495390 / 700 :    loss = 0.684674203396400 / 700 :    loss = 0.684553444386410 / 700 :    loss = 0.684670031071420 / 700 :    loss = 0.684531033039430 / 700 :    loss = 0.684517920017440 / 700 :    loss = 0.684468328953450 / 700 :    loss = 0.684625327587460 / 700 :    loss = 0.684606075287470 / 700 :    loss = 0.684786736965480 / 700 :    loss = 0.684785962105490 / 700 :    loss = 0.684802532196500 / 700 :    loss = 0.684898138046510 / 700 :    loss = 0.684968054295520 / 700 :    loss = 0.685020029545530 / 700 :    loss = 0.684879601002540 / 700 :    loss = 0.684913635254550 / 700 :    loss = 0.684897005558560 / 700 :    loss = 0.684926748276570 / 700 :    loss = 0.684892475605580 / 700 :    loss = 0.684689223766590 / 700 :    loss = 0.684578180313600 / 700 :    loss = 0.684604763985610 / 700 :    loss = 0.684611737728620 / 700 :    loss = 0.684668183327630 / 700 :    loss = 0.684720575809640 / 700 :    loss = 0.684681534767650 / 700 :    loss = 0.684697628021660 / 700 :    loss = 0.684687316418670 / 700 :    loss = 0.684705972672680 / 700 :    loss = 0.684684872627690 / 700 :    loss = 0.684703707695()
Training acc (only root node): 0.631428571429
Valiation acc (only root node): 0.51
[[ 261.   89.]
 [ 169.  181.]]
[[ 36.  14.]
 [ 35.  15.]]
annealed lr to 0.000585
epoch 8
0 / 700 :    loss = 0.68187308311510 / 700 :    loss = 0.68152415752420 / 700 :    loss = 0.6799542903930 / 700 :    loss = 0.68292117118840 / 700 :    loss = 0.68347442150150 / 700 :    loss = 0.68312406539960 / 700 :    loss = 0.68277996778570 / 700 :    loss = 0.68293583393180 / 700 :    loss = 0.6829065680590 / 700 :    loss = 0.683149039745100 / 700 :    loss = 0.683619260788110 / 700 :    loss = 0.68432277441120 / 700 :    loss = 0.683972358704130 / 700 :    loss = 0.68408703804140 / 700 :    loss = 0.684388279915150 / 700 :    loss = 0.684264719486160 / 700 :    loss = 0.68414491415170 / 700 :    loss = 0.684593617916180 / 700 :    loss = 0.684294879436190 / 700 :    loss = 0.684482097626200 / 700 :    loss = 0.684347987175210 / 700 :    loss = 0.684359848499220 / 700 :    loss = 0.684608578682230 / 700 :    loss = 0.684739530087240 / 700 :    loss = 0.684799432755250 / 700 :    loss = 0.684882700443260 / 700 :    loss = 0.685122787952270 / 700 :    loss = 0.685062348843280 / 700 :    loss = 0.684885621071290 / 700 :    loss = 0.684820115566300 / 700 :    loss = 0.684724986553310 / 700 :    loss = 0.684811234474320 / 700 :    loss = 0.684884309769330 / 700 :    loss = 0.684441626072340 / 700 :    loss = 0.684434592724350 / 700 :    loss = 0.684481441975360 / 700 :    loss = 0.684503436089370 / 700 :    loss = 0.684459507465380 / 700 :    loss = 0.684461236390 / 700 :    loss = 0.684515237808400 / 700 :    loss = 0.684392094612410 / 700 :    loss = 0.684508681297420 / 700 :    loss = 0.684369504452430 / 700 :    loss = 0.68435561657440 / 700 :    loss = 0.684314846992450 / 700 :    loss = 0.684468209743460 / 700 :    loss = 0.684450209141470 / 700 :    loss = 0.684627592564480 / 700 :    loss = 0.684626758099490 / 700 :    loss = 0.684643030167500 / 700 :    loss = 0.684739053249510 / 700 :    loss = 0.684809744358520 / 700 :    loss = 0.684862494469530 / 700 :    loss = 0.684725165367540 / 700 :    loss = 0.684768140316550 / 700 :    loss = 0.684753298759560 / 700 :    loss = 0.684774935246570 / 700 :    loss = 0.684739172459580 / 700 :    loss = 0.684534728527590 / 700 :    loss = 0.684424221516600 / 700 :    loss = 0.684449791908610 / 700 :    loss = 0.684460818768620 / 700 :    loss = 0.684521019459630 / 700 :    loss = 0.684572458267640 / 700 :    loss = 0.684538781643650 / 700 :    loss = 0.684551477432660 / 700 :    loss = 0.684538125992670 / 700 :    loss = 0.684558868408680 / 700 :    loss = 0.6845459342690 / 700 :    loss = 0.684570014477()
Training acc (only root node): 0.635714285714
Valiation acc (only root node): 0.5
[[ 241.  109.]
 [ 146.  204.]]
[[ 32.  18.]
 [ 32.  18.]]
annealed lr to 0.000390
epoch 9
0 / 700 :    loss = 0.68014496564910 / 700 :    loss = 0.68109637498920 / 700 :    loss = 0.67960429191630 / 700 :    loss = 0.68263447284740 / 700 :    loss = 0.68321537971550 / 700 :    loss = 0.68285834789360 / 700 :    loss = 0.68251544237170 / 700 :    loss = 0.6826909780580 / 700 :    loss = 0.68267142772790 / 700 :    loss = 0.682930529118100 / 700 :    loss = 0.68340575695110 / 700 :    loss = 0.684119224548120 / 700 :    loss = 0.683777034283130 / 700 :    loss = 0.683896839619140 / 700 :    loss = 0.684204995632150 / 700 :    loss = 0.684084355831160 / 700 :    loss = 0.683973371983170 / 700 :    loss = 0.684427261353180 / 700 :    loss = 0.684135437012190 / 700 :    loss = 0.68433535099200 / 700 :    loss = 0.684200763702210 / 700 :    loss = 0.684216082096220 / 700 :    loss = 0.684455811977230 / 700 :    loss = 0.684591710567240 / 700 :    loss = 0.684656441212250 / 700 :    loss = 0.684747517109260 / 700 :    loss = 0.684982299805270 / 700 :    loss = 0.684926569462280 / 700 :    loss = 0.684748351574290 / 700 :    loss = 0.684683442116300 / 700 :    loss = 0.684589266777310 / 700 :    loss = 0.684680223465320 / 700 :    loss = 0.684752702713330 / 700 :    loss = 0.684336304665340 / 700 :    loss = 0.684328138828350 / 700 :    loss = 0.684369027615360 / 700 :    loss = 0.684376657009370 / 700 :    loss = 0.68433457613380 / 700 :    loss = 0.684337377548390 / 700 :    loss = 0.684391260147400 / 700 :    loss = 0.684266984463410 / 700 :    loss = 0.684383332729420 / 700 :    loss = 0.684245884418430 / 700 :    loss = 0.684231460094440 / 700 :    loss = 0.684199094772450 / 700 :    loss = 0.68434971571460 / 700 :    loss = 0.68433368206470 / 700 :    loss = 0.684506893158480 / 700 :    loss = 0.684505999088490 / 700 :    loss = 0.684522211552500 / 700 :    loss = 0.684618532658510 / 700 :    loss = 0.684690535069520 / 700 :    loss = 0.684743523598530 / 700 :    loss = 0.684610307217540 / 700 :    loss = 0.684661269188550 / 700 :    loss = 0.684648633003560 / 700 :    loss = 0.684663534164570 / 700 :    loss = 0.684625804424580 / 700 :    loss = 0.684420108795590 / 700 :    loss = 0.684311330318600 / 700 :    loss = 0.684335649014610 / 700 :    loss = 0.684347271919620 / 700 :    loss = 0.684409439564630 / 700 :    loss = 0.684460878372640 / 700 :    loss = 0.684430599213650 / 700 :    loss = 0.684441566467660 / 700 :    loss = 0.684426426888670 / 700 :    loss = 0.684448778629680 / 700 :    loss = 0.684442222118690 / 700 :    loss = 0.684470772743()
Training acc (only root node): 0.64
Valiation acc (only root node): 0.53
[[ 229.  121.]
 [ 131.  219.]]
[[ 32.  18.]
 [ 29.  21.]]
annealed lr to 0.000260
epoch 10
0 / 700 :    loss = 0.67883467674310 / 700 :    loss = 0.68078941106820 / 700 :    loss = 0.6793611645730 / 700 :    loss = 0.68244040012440 / 700 :    loss = 0.68303871154850 / 700 :    loss = 0.68266892433260 / 700 :    loss = 0.68231296539370 / 700 :    loss = 0.68249732255980 / 700 :    loss = 0.6824964284990 / 700 :    loss = 0.682759404182100 / 700 :    loss = 0.683245360851110 / 700 :    loss = 0.683964967728120 / 700 :    loss = 0.683628022671130 / 700 :    loss = 0.683757483959140 / 700 :    loss = 0.684072375298150 / 700 :    loss = 0.683952093124160 / 700 :    loss = 0.683842837811170 / 700 :    loss = 0.684301376343180 / 700 :    loss = 0.684013307095190 / 700 :    loss = 0.684220612049200 / 700 :    loss = 0.684087216854210 / 700 :    loss = 0.684105753899220 / 700 :    loss = 0.684342324734230 / 700 :    loss = 0.684482872486240 / 700 :    loss = 0.684552013874250 / 700 :    loss = 0.684649705887260 / 700 :    loss = 0.684879660606270 / 700 :    loss = 0.68482875824280 / 700 :    loss = 0.684647083282290 / 700 :    loss = 0.684582352638300 / 700 :    loss = 0.684489309788310 / 700 :    loss = 0.68458288908320 / 700 :    loss = 0.684655487537330 / 700 :    loss = 0.684255182743340 / 700 :    loss = 0.684247255325350 / 700 :    loss = 0.684285163879360 / 700 :    loss = 0.684283077717370 / 700 :    loss = 0.684242665768380 / 700 :    loss = 0.684246480465390 / 700 :    loss = 0.684300541878400 / 700 :    loss = 0.684175729752410 / 700 :    loss = 0.684292018414420 / 700 :    loss = 0.68415594101430 / 700 :    loss = 0.684141218662440 / 700 :    loss = 0.68411552906450 / 700 :    loss = 0.684264302254460 / 700 :    loss = 0.684250175953470 / 700 :    loss = 0.684419870377480 / 700 :    loss = 0.684419095516490 / 700 :    loss = 0.68443530798500 / 700 :    loss = 0.684532046318510 / 700 :    loss = 0.684605181217520 / 700 :    loss = 0.684658050537530 / 700 :    loss = 0.684528768063540 / 700 :    loss = 0.684586107731550 / 700 :    loss = 0.684575498104560 / 700 :    loss = 0.684585094452570 / 700 :    loss = 0.684545516968580 / 700 :    loss = 0.684338748455590 / 700 :    loss = 0.684232115746600 / 700 :    loss = 0.684255242348610 / 700 :    loss = 0.684265494347620 / 700 :    loss = 0.684328556061630 / 700 :    loss = 0.684380233288640 / 700 :    loss = 0.684351801872650 / 700 :    loss = 0.684361875057660 / 700 :    loss = 0.684345960617670 / 700 :    loss = 0.684369266033680 / 700 :    loss = 0.684366703033690 / 700 :    loss = 0.684398531914()
Training acc (only root node): 0.637142857143
Valiation acc (only root node): 0.53
[[ 223.  127.]
 [ 127.  223.]]
[[ 31.  19.]
 [ 28.  22.]]
annealed lr to 0.000173
epoch 11
0 / 700 :    loss = 0.6779549717910 / 700 :    loss = 0.68059164285720 / 700 :    loss = 0.6792051196130 / 700 :    loss = 0.68231838941640 / 700 :    loss = 0.68292772769950 / 700 :    loss = 0.68254637718260 / 700 :    loss = 0.68217599391970 / 700 :    loss = 0.68236392736480 / 700 :    loss = 0.68237918615390 / 700 :    loss = 0.682641208172100 / 700 :    loss = 0.683137178421110 / 700 :    loss = 0.68385976553120 / 700 :    loss = 0.6835257411130 / 700 :    loss = 0.683664143085140 / 700 :    loss = 0.683984398842150 / 700 :    loss = 0.68386310339160 / 700 :    loss = 0.683752298355170 / 700 :    loss = 0.684214293957180 / 700 :    loss = 0.683927834034190 / 700 :    loss = 0.684139370918200 / 700 :    loss = 0.684007227421210 / 700 :    loss = 0.684027791023220 / 700 :    loss = 0.68426400423230 / 700 :    loss = 0.684408843517240 / 700 :    loss = 0.684481441975250 / 700 :    loss = 0.684584438801260 / 700 :    loss = 0.684809982777270 / 700 :    loss = 0.68476319313280 / 700 :    loss = 0.684577703476290 / 700 :    loss = 0.684512853622300 / 700 :    loss = 0.684420645237310 / 700 :    loss = 0.684515476227320 / 700 :    loss = 0.684588432312330 / 700 :    loss = 0.684197127819340 / 700 :    loss = 0.684189677238350 / 700 :    loss = 0.684226036072360 / 700 :    loss = 0.684218049049370 / 700 :    loss = 0.684178769588380 / 700 :    loss = 0.684183359146390 / 700 :    loss = 0.684237480164400 / 700 :    loss = 0.68411231041410 / 700 :    loss = 0.684228599072420 / 700 :    loss = 0.684093654156430 / 700 :    loss = 0.684078752995440 / 700 :    loss = 0.684057533741450 / 700 :    loss = 0.684205234051460 / 700 :    loss = 0.684192538261470 / 700 :    loss = 0.68435972929480 / 700 :    loss = 0.684359014034490 / 700 :    loss = 0.684375345707500 / 700 :    loss = 0.684472203255510 / 700 :    loss = 0.684546351433520 / 700 :    loss = 0.684599101543530 / 700 :    loss = 0.684472858906540 / 700 :    loss = 0.684535086155550 / 700 :    loss = 0.684526264668560 / 700 :    loss = 0.684531986713570 / 700 :    loss = 0.684490919113580 / 700 :    loss = 0.684283316135590 / 700 :    loss = 0.684178411961600 / 700 :    loss = 0.684200525284610 / 700 :    loss = 0.684208929539620 / 700 :    loss = 0.684272289276630 / 700 :    loss = 0.684324324131640 / 700 :    loss = 0.684296607971650 / 700 :    loss = 0.684306383133660 / 700 :    loss = 0.68429017067670 / 700 :    loss = 0.684314191341680 / 700 :    loss = 0.684313774109690 / 700 :    loss = 0.684347748756()
Training acc (only root node): 0.637142857143
Valiation acc (only root node): 0.53
[[ 214.  136.]
 [ 118.  232.]]
[[ 31.  19.]
 [ 28.  22.]]
annealed lr to 0.000116
epoch 12
0 / 700 :    loss = 0.67739111185110 / 700 :    loss = 0.6804686188720 / 700 :    loss = 0.67910730838830 / 700 :    loss = 0.68224251270340 / 700 :    loss = 0.68285924196250 / 700 :    loss = 0.68246918916760 / 700 :    loss = 0.68208765983670 / 700 :    loss = 0.68227684497880 / 700 :    loss = 0.68230336904590 / 700 :    loss = 0.682563602924100 / 700 :    loss = 0.683066725731110 / 700 :    loss = 0.683791339397120 / 700 :    loss = 0.683458566666130 / 700 :    loss = 0.68360376358140 / 700 :    loss = 0.683927834034150 / 700 :    loss = 0.683805346489160 / 700 :    loss = 0.683692514896170 / 700 :    loss = 0.684156835079180 / 700 :    loss = 0.683871090412190 / 700 :    loss = 0.68408459425200 / 700 :    loss = 0.683953404427210 / 700 :    loss = 0.68397551775220 / 700 :    loss = 0.684212386608230 / 700 :    loss = 0.684360265732240 / 700 :    loss = 0.684435427189250 / 700 :    loss = 0.684542357922260 / 700 :    loss = 0.684764385223270 / 700 :    loss = 0.684720993042280 / 700 :    loss = 0.684531867504290 / 700 :    loss = 0.684466719627300 / 700 :    loss = 0.684375107288310 / 700 :    loss = 0.684470236301320 / 700 :    loss = 0.684543550014330 / 700 :    loss = 0.684157073498340 / 700 :    loss = 0.68415004015350 / 700 :    loss = 0.684185564518360 / 700 :    loss = 0.684174537659370 / 700 :    loss = 0.684135854244380 / 700 :    loss = 0.684141218662390 / 700 :    loss = 0.684195041656400 / 700 :    loss = 0.684069693089410 / 700 :    loss = 0.684186160564420 / 700 :    loss = 0.684051573277430 / 700 :    loss = 0.684036791325440 / 700 :    loss = 0.684018373489450 / 700 :    loss = 0.684165298939460 / 700 :    loss = 0.684153497219470 / 700 :    loss = 0.684319198132480 / 700 :    loss = 0.68431854248490 / 700 :    loss = 0.684334874153500 / 700 :    loss = 0.684431910515510 / 700 :    loss = 0.684506773949520 / 700 :    loss = 0.684559285641530 / 700 :    loss = 0.684435248375540 / 700 :    loss = 0.684500932693550 / 700 :    loss = 0.684493422508560 / 700 :    loss = 0.684496462345570 / 700 :    loss = 0.684454202652580 / 700 :    loss = 0.684246122837590 / 700 :    loss = 0.684142589569600 / 700 :    loss = 0.684164106846610 / 700 :    loss = 0.684170722961620 / 700 :    loss = 0.684234201908630 / 700 :    loss = 0.684286415577640 / 700 :    loss = 0.684259057045650 / 700 :    loss = 0.684268712997660 / 700 :    loss = 0.68425232172670 / 700 :    loss = 0.68427670002680 / 700 :    loss = 0.684277653694690 / 700 :    loss = 0.684312999249()
Training acc (only root node): 0.642857142857
Valiation acc (only root node): 0.52
[[ 211.  139.]
 [ 111.  239.]]
[[ 29.  21.]
 [ 27.  23.]]
annealed lr to 0.000077
epoch 13
0 / 700 :    loss = 0.67703378200510 / 700 :    loss = 0.68039155006420 / 700 :    loss = 0.67904591560430 / 700 :    loss = 0.68219482898740 / 700 :    loss = 0.68281656503750 / 700 :    loss = 0.68242031335860 / 700 :    loss = 0.68203121423770 / 700 :    loss = 0.68222099542680 / 700 :    loss = 0.6822547912690 / 700 :    loss = 0.682513535023100 / 700 :    loss = 0.683021485806110 / 700 :    loss = 0.683746874332120 / 700 :    loss = 0.68341511488130 / 700 :    loss = 0.683564960957140 / 700 :    loss = 0.68389159441150 / 700 :    loss = 0.683768153191160 / 700 :    loss = 0.683653533459170 / 700 :    loss = 0.684119522572180 / 700 :    loss = 0.683833837509190 / 700 :    loss = 0.68404853344200 / 700 :    loss = 0.683918118477210 / 700 :    loss = 0.68394100666220 / 700 :    loss = 0.68417853117230 / 700 :    loss = 0.684328794479240 / 700 :    loss = 0.684405744076250 / 700 :    loss = 0.684515357018260 / 700 :    loss = 0.68473482132270 / 700 :    loss = 0.684693753719280 / 700 :    loss = 0.684501767159290 / 700 :    loss = 0.684436321259300 / 700 :    loss = 0.684345245361310 / 700 :    loss = 0.68444031477320 / 700 :    loss = 0.684513926506330 / 700 :    loss = 0.684130072594340 / 700 :    loss = 0.684123396873350 / 700 :    loss = 0.684158504009360 / 700 :    loss = 0.684145689011370 / 700 :    loss = 0.684107303619380 / 700 :    loss = 0.684113264084390 / 700 :    loss = 0.684166967869400 / 700 :    loss = 0.684041440487410 / 700 :    loss = 0.684157907963420 / 700 :    loss = 0.684023618698430 / 700 :    loss = 0.684008836746440 / 700 :    loss = 0.683992087841450 / 700 :    loss = 0.684138715267460 / 700 :    loss = 0.684127569199470 / 700 :    loss = 0.684292197227480 / 700 :    loss = 0.684291541576490 / 700 :    loss = 0.684307932854500 / 700 :    loss = 0.68440502882510 / 700 :    loss = 0.684480369091520 / 700 :    loss = 0.684532940388530 / 700 :    loss = 0.684410333633540 / 700 :    loss = 0.684478223324550 / 700 :    loss = 0.684471726418560 / 700 :    loss = 0.68447291851570 / 700 :    loss = 0.684429943562580 / 700 :    loss = 0.684221446514590 / 700 :    loss = 0.684118926525600 / 700 :    loss = 0.68413990736610 / 700 :    loss = 0.684145271778620 / 700 :    loss = 0.684208750725630 / 700 :    loss = 0.684261143208640 / 700 :    loss = 0.684233784676650 / 700 :    loss = 0.684243500233660 / 700 :    loss = 0.684227108955670 / 700 :    loss = 0.684251606464680 / 700 :    loss = 0.684253334999690 / 700 :    loss = 0.684289574623()
Training acc (only root node): 0.641428571429
Valiation acc (only root node): 0.51
[[ 207.  143.]
 [ 108.  242.]]
[[ 28.  22.]
 [ 27.  23.]]
annealed lr to 0.000051
epoch 14
0 / 700 :    loss = 0.6768056750310 / 700 :    loss = 0.6803427338620 / 700 :    loss = 0.67900657653830 / 700 :    loss = 0.68216443061840 / 700 :    loss = 0.68278938531950 / 700 :    loss = 0.68238919973460 / 700 :    loss = 0.68199491500970 / 700 :    loss = 0.68218499422180 / 700 :    loss = 0.68222349882190 / 700 :    loss = 0.682481110096100 / 700 :    loss = 0.682992100716110 / 700 :    loss = 0.683718144894120 / 700 :    loss = 0.683386921883130 / 700 :    loss = 0.683539867401140 / 700 :    loss = 0.68386811018150 / 700 :    loss = 0.683744132519160 / 700 :    loss = 0.68362814188170 / 700 :    loss = 0.684095084667180 / 700 :    loss = 0.683809518814190 / 700 :    loss = 0.684024989605200 / 700 :    loss = 0.683894991875210 / 700 :    loss = 0.683918297291220 / 700 :    loss = 0.684156477451230 / 700 :    loss = 0.684308230877240 / 700 :    loss = 0.684386432171250 / 700 :    loss = 0.684497892857260 / 700 :    loss = 0.68471544981270 / 700 :    loss = 0.68467605114280 / 700 :    loss = 0.684482038021290 / 700 :    loss = 0.684416353703300 / 700 :    loss = 0.684325516224310 / 700 :    loss = 0.684420585632320 / 700 :    loss = 0.684494376183330 / 700 :    loss = 0.684111952782340 / 700 :    loss = 0.68410551548350 / 700 :    loss = 0.684140384197360 / 700 :    loss = 0.68412655592370 / 700 :    loss = 0.684088349342380 / 700 :    loss = 0.684094905853390 / 700 :    loss = 0.684148311615400 / 700 :    loss = 0.684022665024410 / 700 :    loss = 0.684139251709420 / 700 :    loss = 0.684005022049430 / 700 :    loss = 0.683990240097440 / 700 :    loss = 0.68397462368450 / 700 :    loss = 0.684120953083460 / 700 :    loss = 0.684110224247470 / 700 :    loss = 0.684274196625480 / 700 :    loss = 0.684273660183490 / 700 :    loss = 0.684290111065W tensorflow/core/framework/op_kernel.cc:993] Invalid argument: Unsuccessful TensorSliceReader constructor: Failed to get matching files on ./weights/rnn_embed=35_l2=0.020000_lr=0.010000.weights.temp: Resource exhausted: ./weights
Traceback (most recent call last):
  File "rnn.py", line 454, in <module>
    test_RNN()
  File "rnn.py", line 436, in test_RNN
    stats = model.train(verbose=True)
  File "rnn.py", line 383, in train
    train_acc, val_acc, loss_history, val_loss = self.run_epoch()
  File "rnn.py", line 313, in run_epoch
    saver.restore(sess, './weights/%s.temp'%self.config.model_name)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 1439, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 767, in run
    run_metadata_ptr)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 965, in _run
    feed_dict_string, options, run_metadata)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1015, in _do_run
    target_list, options, run_metadata)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on ./weights/rnn_embed=35_l2=0.020000_lr=0.010000.weights.temp: Resource exhausted: ./weights
	 [[Node: save/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device="/job:localhost/replica:0/task:0/cpu:0"](_recv_save/Const_0, save/RestoreV2_2/tensor_names, save/RestoreV2_2/shape_and_slices)]]

Caused by op u'save/RestoreV2_2', defined at:
  File "rnn.py", line 454, in <module>
    test_RNN()
  File "rnn.py", line 436, in test_RNN
    stats = model.train(verbose=True)
  File "rnn.py", line 383, in train
    train_acc, val_acc, loss_history, val_loss = self.run_epoch()
  File "rnn.py", line 312, in run_epoch
    saver = tf.train.Saver()
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 1051, in __init__
    self.build()
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 1081, in build
    restore_sequentially=self._restore_sequentially)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 675, in build
    restore_sequentially, reshape)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 402, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/training/saver.py", line 242, in restore_op
    [spec.tensor.dtype])[0])
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py", line 668, in restore_v2
    dtypes=dtypes, name=name)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 763, in apply_op
    op_def=op_def)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/usr/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on ./weights/rnn_embed=35_l2=0.020000_lr=0.010000.weights.temp: Resource exhausted: ./weights
	 [[Node: save/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device="/job:localhost/replica:0/task:0/cpu:0"](_recv_save/Const_0, save/RestoreV2_2/tensor_names, save/RestoreV2_2/shape_and_slices)]]

